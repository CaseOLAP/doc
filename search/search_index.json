{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CaseOLAP Platform We are greatly excited with the new enthusiasm to introduce the Context-aware Semantic Online Analytical Processing pipeline (CaseOLAP), developed in 2016. The rapidly accumulating quantity of biomedical textual data has far exceeded the human capacity of manual curation and analysis, necessitating novel text-mining tools to extract biological insights from large volumes of scientific reports.CaseOLAP successfully quantifies user-defined phrase-category relationships through analysis of textual data. This online documents has been prepared for caseOLAP platform for coding tools development and implementation. We have developed a protocol for the complete CaseOLAP platform, including data preprocessing (i.e., downloading and parsing text documents), indexing and searching with Elasticsearch, creating a functional document structure called Text-Cube and quantifying phrase-category relationships using the core CaseOLAP algorithm. image source: www.jove.com Figure : Counting number of decendent nodes in each categories of ICD 11 code trees. Prepared By: PingLab (UCLA)","title":"Home"},{"location":"#caseolap-platform","text":"We are greatly excited with the new enthusiasm to introduce the Context-aware Semantic Online Analytical Processing pipeline (CaseOLAP), developed in 2016. The rapidly accumulating quantity of biomedical textual data has far exceeded the human capacity of manual curation and analysis, necessitating novel text-mining tools to extract biological insights from large volumes of scientific reports.CaseOLAP successfully quantifies user-defined phrase-category relationships through analysis of textual data. This online documents has been prepared for caseOLAP platform for coding tools development and implementation. We have developed a protocol for the complete CaseOLAP platform, including data preprocessing (i.e., downloading and parsing text documents), indexing and searching with Elasticsearch, creating a functional document structure called Text-Cube and quantifying phrase-category relationships using the core CaseOLAP algorithm. image source: www.jove.com Figure : Counting number of decendent nodes in each categories of ICD 11 code trees. Prepared By: PingLab (UCLA)","title":"CaseOLAP Platform"},{"location":"affinity/Affinity/","text":"Clustering: Affinity Propagation from sklearn.cluster import AffinityPropagation import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns % matplotlib inline from itertools import cycle Get Data data = pd . read_csv( ../datareader/score/old-score.csv ) data . index = data[ phrase ] ndf = data . drop( phrase ,axis = 1 ) ndf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 CVA IHD CM ARR VD CHD phrase 1,25-dihydroxyvitamin_d(3)_24-hydroxylase,_mitochondrial 0 0.000000 0.446767 0.000000 0.000000 0.0 0.000000 1,4-alpha-glucan-branching_enzyme 1 0.290414 0.000000 1.035505 0.091431 0.0 0.372223 1,5-anhydro-d-fructose_reductase 2 0.000000 0.000000 0.000000 0.000000 0.0 2.220267 1-phosphatidylinositol_3-phosphate_5-kinase 3 0.000000 0.000000 0.000000 0.308680 0.0 0.317226 1-phosphatidylinositol_4,5-bisphosphate_phosphodiesterase_beta-1 4 0.000000 0.000000 0.472116 0.000000 0.0 0.000000 ndf . shape (3175, 7) Split data tdata = ndf . copy(deep = True ) cva_ihd = tdata . drop([ CM , ARR , VD , CHD ],axis = 1 ) cva_cm = tdata . drop([ IHD , ARR , VD , CHD ],axis = 1 ) cva_arr = tdata . drop([ IHD , CM , VD , CHD ],axis = 1 ) cva_vd = tdata . drop([ IHD , CM , ARR , CHD ],axis = 1 ) cva_chd = tdata . drop([ IHD , CM , ARR , VD ],axis = 1 ) ihd_cm = tdata . drop([ CVA , ARR , VD , CHD ],axis = 1 ) ihd_arr = tdata . drop([ CVA , CM , VD , CHD ],axis = 1 ) ihd_vd = tdata . drop([ CVA , CM , ARR , CHD ],axis = 1 ) ihd_chd = tdata . drop([ CVA , CM , ARR , VD ],axis = 1 ) cm_arr = tdata . drop([ CVA , IHD , VD , CHD ],axis = 1 ) cm_vd = tdata . drop([ CVA , IHD , ARR , CHD ],axis = 1 ) cm_chd = tdata . drop([ CVA , IHD , ARR , VD ],axis = 1 ) arr_vd = tdata . drop([ CVA , IHD , CM , CHD ],axis = 1 ) arr_chd = tdata . drop([ CVA , IHD , CM , VD ],axis = 1 ) vd_chd = tdata . drop([ CVA , IHD , CM , ARR ],axis = 1 ) def remover (df,cvd1,cvd2): protein = df . index d1 = df[cvd1] d2 = df[cvd2] DT = [] c = 0.5 cutoff = list (df . mean() + (c * df . std())) for p,a,b in zip (protein,d1,d2): if a cutoff[ 0 ] or b cutoff[ 1 ] : DT . append({ protein : p, cvd1: a, cvd2: b},) dfr = pd . DataFrame(DT) dfr . index = dfr[ protein ] dfr = dfr . drop( protein ,axis = 1 ) return dfr Select Pair cvd1 = ARR # cvd2 = CVA df = cva_arr pref =- 1.0 cvd1 = IHD # cvd2 = CVA df = cva_ihd pref =- 1.0 cvd1 = CM # cvd2 = CVA df = cva_cm pref =- 0.1 cvd1 = VD # cvd2 = CVA df = cva_vd pref =- 0.1 cvd1 = CHD # cvd2 = CVA df = cva_chd pref =- 0.1 cvd1 = IHD # cvd2 = CM df = ihd_cm pref =- 0.1 cvd1 = IHD # cvd2 = ARR df = ihd_arr pref =- 0.1 cvd1 = IHD # cvd2 = VD df = ihd_vd pref =- 0.1 cvd1 = IHD # cvd2 = CHD df = ihd_chd pref =- 0.1 cvd1 = CM # cvd2 = ARR df = cm_arr pref =- 0.1 cvd1 = CM # cvd2 = VD df = cm_vd pref =- 0.1 cvd1 = CM # cvd2 = CHD df = cm_chd pref =- 0.1 cvd1 = ARR # cvd2 = VD df = arr_vd pref =- 0.1 cvd1 = ARR # cvd2 = CHD df = arr_chd pref =- 0.1 cvd1 = VD # cvd2 = CHD df = vd_chd pref =- 0.1 rdf = remover(df,cvd1,cvd2) rdf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD protein 1-phosphatidylinositol_4,5-bisphosphate_phosphodiesterase_delta-1 0.484608 0.000000 2',3'-cyclic-nucleotide_3'-phosphodiesterase 2.739150 0.000000 3-hydroxy-3-methylglutaryl-coenzyme_a_reductase 1.006448 3.095639 43_kda_receptor-associated_protein_of_the_synapse 1.283404 0.169499 5-hydroxytryptamine_receptor_1a 0.846001 0.484862 rdf . shape (628, 2) X = rdf . copy(deep = True ) p = np . array( list (X . index)) Affinity Propagation # Compute Affinity Propagation af = AffinityPropagation(preference = pref) . fit(X) indices = af . cluster_centers_indices_ labels = af . labels_ n_clusters = len (indices) n_clusters 23 Data and Plot sns . set() plt . figure(figsize = [ 15 , 15 ]) DTT = [] X = np . array(X) colors = cycle( bgrcmykbgrcmykbgrcmykbgrcmyk ) for k, col in zip ( range (n_clusters), colors): class_members = labels == k cluster_center = X[indices[k]] ln = len (X[class_members, 0 ]) if ln 1 : for i in range (ln): DTT . append({ label : k,\\ protein : p[class_members][i],\\ cvd1: X[class_members, 0 ][i],\\ cvd2: X[class_members, 1 ][i],\\ color :col}) else : DTT . append({ label : k,\\ protein : p[class_members][ 0 ],\\ cvd1: X[class_members, 0 ][ 0 ],\\ cvd2: X[class_members, 1 ][ 0 ],\\ color :col}) # plot data plt . plot(X[class_members, 0 ], X[class_members, 1 ], col + . ) # plot center plt . plot(cluster_center[ 0 ], cluster_center[ 1 ], . , markerfacecolor = col, markeredgecolor = k , markersize = 14 ) # draw lines for x in X[class_members]: plt . plot([cluster_center[ 0 ], x[ 0 ]], [cluster_center[ 1 ], x[ 1 ]], col) plt . title( Estimated number of clusters: %d % n_clusters) plt . xlabel(cvd1, fontsize = 25 ) plt . ylabel(cvd2, fontsize = 25 ) plt . savefig( plot/ + cvd1 + - + cvd2 + .pdf ) plt . show() listdf = pd . DataFrame(DTT) listdf . index = listdf[ protein ] listdf = listdf . drop( protein ,axis = 1 ) listdf . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CVA color label protein 1-phosphatidylinositol_4,5-bisphosphate_phosphodiesterase_delta-1 0.000000 0.484608 b 0 5-hydroxytryptamine_receptor_2a 0.174764 0.555104 b 0 78_kda_glucose-regulated_protein 0.065296 0.529532 b 0 achaete-scute_homolog_1 0.000000 0.449759 b 0 adenylyl_cyclase-associated_protein_2 0.000000 0.449759 b 0 Add Uniprot data = pd . read_csv( uniprot.csv ) data . index = data[ protein ] udf = data . drop( protein ,axis = 1 ) udf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uniprot_id gene_names has_multiple_IDs protein natriuretic_peptides_b P16860 NPPB NaN angiotensin-converting_enzyme P12821 ACE DCP DCP1 NaN potassium_voltage-gated_channel_subfamily_h_member_2 Q12809 KCNH2 ERG ERG1 HERG NaN c-reactive_protein P02741 CRP PTX1 NaN apolipoprotein_e P02649 APOE NaN idx_cvd = list (listdf . index) idx_uprt = list (udf . index) prot_dict = [] for item in idx_cvd: data = listdf . loc[item,:] if item in idx_uprt: prot_dict . append({ Protein :item,\\ cvd1:data[ 0 ],\\ cvd2:data[ 1 ],\\ color :data[ 2 ],\\ label : data[ 3 ],\\ uprot :udf . loc[item,:][ 0 ]}) else : print (item, Match not Found ) prot_dict . append({ Protein :item,\\ cvd1:data[ 0 ],\\ cvd2:data[ 1 ],\\ color :data[ 2 ],\\ label : data[ 3 ],\\ uprot : NAN }) resultdf = pd . DataFrame(prot_dict) resultdf . index = resultdf[ Protein ] resultdf = resultdf . drop( Protein ,axis = 1 ) resultdf . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD color label uprot Protein brain-derived_neurotrophic_factor 0.492909 0.026065 b 0 P23560 alpha-galactosidase_a 0.488128 0.003105 b 0 P06280 neurotrophin-3 0.510094 0.025944 b 0 P20783 tetraspanin-33 0.686945 0.003340 b 0 Q86UF1 glutamate_receptor_ionotropic,_nmda_2c 0.479266 0.051119 b 0 Q14957 nestin 0.404329 0.040345 b 0 P48681 glutamate_receptor_ionotropic,_nmda_2b 0.480174 0.050999 b 0 Q13224 glutamate_receptor_ionotropic,_nmda_1 0.479685 0.051063 b 0 Q05586 microtubule-associated_protein_2 0.520331 0.000000 b 0 P11137 glial_fibrillary_acidic_protein 0.568521 0.003560 b 0 P14136 resultdf . to_csv( data/ + cvd1 + - + cvd2 + .csv )","title":"Affinity Propagation"},{"location":"affinity/Affinity/#clustering-affinity-propagation","text":"from sklearn.cluster import AffinityPropagation import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns % matplotlib inline from itertools import cycle","title":"Clustering: Affinity Propagation"},{"location":"affinity/Affinity/#get-data","text":"data = pd . read_csv( ../datareader/score/old-score.csv ) data . index = data[ phrase ] ndf = data . drop( phrase ,axis = 1 ) ndf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 CVA IHD CM ARR VD CHD phrase 1,25-dihydroxyvitamin_d(3)_24-hydroxylase,_mitochondrial 0 0.000000 0.446767 0.000000 0.000000 0.0 0.000000 1,4-alpha-glucan-branching_enzyme 1 0.290414 0.000000 1.035505 0.091431 0.0 0.372223 1,5-anhydro-d-fructose_reductase 2 0.000000 0.000000 0.000000 0.000000 0.0 2.220267 1-phosphatidylinositol_3-phosphate_5-kinase 3 0.000000 0.000000 0.000000 0.308680 0.0 0.317226 1-phosphatidylinositol_4,5-bisphosphate_phosphodiesterase_beta-1 4 0.000000 0.000000 0.472116 0.000000 0.0 0.000000 ndf . shape (3175, 7)","title":"Get Data"},{"location":"affinity/Affinity/#split-data","text":"tdata = ndf . copy(deep = True ) cva_ihd = tdata . drop([ CM , ARR , VD , CHD ],axis = 1 ) cva_cm = tdata . drop([ IHD , ARR , VD , CHD ],axis = 1 ) cva_arr = tdata . drop([ IHD , CM , VD , CHD ],axis = 1 ) cva_vd = tdata . drop([ IHD , CM , ARR , CHD ],axis = 1 ) cva_chd = tdata . drop([ IHD , CM , ARR , VD ],axis = 1 ) ihd_cm = tdata . drop([ CVA , ARR , VD , CHD ],axis = 1 ) ihd_arr = tdata . drop([ CVA , CM , VD , CHD ],axis = 1 ) ihd_vd = tdata . drop([ CVA , CM , ARR , CHD ],axis = 1 ) ihd_chd = tdata . drop([ CVA , CM , ARR , VD ],axis = 1 ) cm_arr = tdata . drop([ CVA , IHD , VD , CHD ],axis = 1 ) cm_vd = tdata . drop([ CVA , IHD , ARR , CHD ],axis = 1 ) cm_chd = tdata . drop([ CVA , IHD , ARR , VD ],axis = 1 ) arr_vd = tdata . drop([ CVA , IHD , CM , CHD ],axis = 1 ) arr_chd = tdata . drop([ CVA , IHD , CM , VD ],axis = 1 ) vd_chd = tdata . drop([ CVA , IHD , CM , ARR ],axis = 1 ) def remover (df,cvd1,cvd2): protein = df . index d1 = df[cvd1] d2 = df[cvd2] DT = [] c = 0.5 cutoff = list (df . mean() + (c * df . std())) for p,a,b in zip (protein,d1,d2): if a cutoff[ 0 ] or b cutoff[ 1 ] : DT . append({ protein : p, cvd1: a, cvd2: b},) dfr = pd . DataFrame(DT) dfr . index = dfr[ protein ] dfr = dfr . drop( protein ,axis = 1 ) return dfr","title":"Split data"},{"location":"affinity/Affinity/#select-pair","text":"cvd1 = ARR # cvd2 = CVA df = cva_arr pref =- 1.0 cvd1 = IHD # cvd2 = CVA df = cva_ihd pref =- 1.0 cvd1 = CM # cvd2 = CVA df = cva_cm pref =- 0.1 cvd1 = VD # cvd2 = CVA df = cva_vd pref =- 0.1 cvd1 = CHD # cvd2 = CVA df = cva_chd pref =- 0.1 cvd1 = IHD # cvd2 = CM df = ihd_cm pref =- 0.1 cvd1 = IHD # cvd2 = ARR df = ihd_arr pref =- 0.1 cvd1 = IHD # cvd2 = VD df = ihd_vd pref =- 0.1 cvd1 = IHD # cvd2 = CHD df = ihd_chd pref =- 0.1 cvd1 = CM # cvd2 = ARR df = cm_arr pref =- 0.1 cvd1 = CM # cvd2 = VD df = cm_vd pref =- 0.1 cvd1 = CM # cvd2 = CHD df = cm_chd pref =- 0.1 cvd1 = ARR # cvd2 = VD df = arr_vd pref =- 0.1 cvd1 = ARR # cvd2 = CHD df = arr_chd pref =- 0.1 cvd1 = VD # cvd2 = CHD df = vd_chd pref =- 0.1 rdf = remover(df,cvd1,cvd2) rdf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD protein 1-phosphatidylinositol_4,5-bisphosphate_phosphodiesterase_delta-1 0.484608 0.000000 2',3'-cyclic-nucleotide_3'-phosphodiesterase 2.739150 0.000000 3-hydroxy-3-methylglutaryl-coenzyme_a_reductase 1.006448 3.095639 43_kda_receptor-associated_protein_of_the_synapse 1.283404 0.169499 5-hydroxytryptamine_receptor_1a 0.846001 0.484862 rdf . shape (628, 2) X = rdf . copy(deep = True ) p = np . array( list (X . index))","title":"Select Pair"},{"location":"affinity/Affinity/#affinity-propagation","text":"# Compute Affinity Propagation af = AffinityPropagation(preference = pref) . fit(X) indices = af . cluster_centers_indices_ labels = af . labels_ n_clusters = len (indices) n_clusters 23","title":"Affinity Propagation"},{"location":"affinity/Affinity/#data-and-plot","text":"sns . set() plt . figure(figsize = [ 15 , 15 ]) DTT = [] X = np . array(X) colors = cycle( bgrcmykbgrcmykbgrcmykbgrcmyk ) for k, col in zip ( range (n_clusters), colors): class_members = labels == k cluster_center = X[indices[k]] ln = len (X[class_members, 0 ]) if ln 1 : for i in range (ln): DTT . append({ label : k,\\ protein : p[class_members][i],\\ cvd1: X[class_members, 0 ][i],\\ cvd2: X[class_members, 1 ][i],\\ color :col}) else : DTT . append({ label : k,\\ protein : p[class_members][ 0 ],\\ cvd1: X[class_members, 0 ][ 0 ],\\ cvd2: X[class_members, 1 ][ 0 ],\\ color :col}) # plot data plt . plot(X[class_members, 0 ], X[class_members, 1 ], col + . ) # plot center plt . plot(cluster_center[ 0 ], cluster_center[ 1 ], . , markerfacecolor = col, markeredgecolor = k , markersize = 14 ) # draw lines for x in X[class_members]: plt . plot([cluster_center[ 0 ], x[ 0 ]], [cluster_center[ 1 ], x[ 1 ]], col) plt . title( Estimated number of clusters: %d % n_clusters) plt . xlabel(cvd1, fontsize = 25 ) plt . ylabel(cvd2, fontsize = 25 ) plt . savefig( plot/ + cvd1 + - + cvd2 + .pdf ) plt . show() listdf = pd . DataFrame(DTT) listdf . index = listdf[ protein ] listdf = listdf . drop( protein ,axis = 1 ) listdf . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CVA color label protein 1-phosphatidylinositol_4,5-bisphosphate_phosphodiesterase_delta-1 0.000000 0.484608 b 0 5-hydroxytryptamine_receptor_2a 0.174764 0.555104 b 0 78_kda_glucose-regulated_protein 0.065296 0.529532 b 0 achaete-scute_homolog_1 0.000000 0.449759 b 0 adenylyl_cyclase-associated_protein_2 0.000000 0.449759 b 0","title":"Data and Plot"},{"location":"affinity/Affinity/#add-uniprot","text":"data = pd . read_csv( uniprot.csv ) data . index = data[ protein ] udf = data . drop( protein ,axis = 1 ) udf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uniprot_id gene_names has_multiple_IDs protein natriuretic_peptides_b P16860 NPPB NaN angiotensin-converting_enzyme P12821 ACE DCP DCP1 NaN potassium_voltage-gated_channel_subfamily_h_member_2 Q12809 KCNH2 ERG ERG1 HERG NaN c-reactive_protein P02741 CRP PTX1 NaN apolipoprotein_e P02649 APOE NaN idx_cvd = list (listdf . index) idx_uprt = list (udf . index) prot_dict = [] for item in idx_cvd: data = listdf . loc[item,:] if item in idx_uprt: prot_dict . append({ Protein :item,\\ cvd1:data[ 0 ],\\ cvd2:data[ 1 ],\\ color :data[ 2 ],\\ label : data[ 3 ],\\ uprot :udf . loc[item,:][ 0 ]}) else : print (item, Match not Found ) prot_dict . append({ Protein :item,\\ cvd1:data[ 0 ],\\ cvd2:data[ 1 ],\\ color :data[ 2 ],\\ label : data[ 3 ],\\ uprot : NAN }) resultdf = pd . DataFrame(prot_dict) resultdf . index = resultdf[ Protein ] resultdf = resultdf . drop( Protein ,axis = 1 ) resultdf . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD color label uprot Protein brain-derived_neurotrophic_factor 0.492909 0.026065 b 0 P23560 alpha-galactosidase_a 0.488128 0.003105 b 0 P06280 neurotrophin-3 0.510094 0.025944 b 0 P20783 tetraspanin-33 0.686945 0.003340 b 0 Q86UF1 glutamate_receptor_ionotropic,_nmda_2c 0.479266 0.051119 b 0 Q14957 nestin 0.404329 0.040345 b 0 P48681 glutamate_receptor_ionotropic,_nmda_2b 0.480174 0.050999 b 0 Q13224 glutamate_receptor_ionotropic,_nmda_1 0.479685 0.051063 b 0 Q05586 microtubule-associated_protein_2 0.520331 0.000000 b 0 P11137 glial_fibrillary_acidic_protein 0.568521 0.003560 b 0 P14136 resultdf . to_csv( data/ + cvd1 + - + cvd2 + .csv )","title":"Add Uniprot"},{"location":"caseolap/CaseOLAP/","text":"CaseOLAP score calculation CaseOLAP score calculation : CaseOLAP score are the quantification of user defined entity-category association. It start with the text-cube document structure and finds the entity in each document in each cell of the text-cube and by implementing updated text-cube metadata, it calculates the CaseOLAP score with following steps: Integrity : Integrity of user defined phrase is taken to be 1. (Autophrase, Segphrase) Popularity : It depends on how frequently a protein name is mentioned within one category, and it is calculated only using the statistics from the cells of documents pertaining to that individual category. Rare protein names in a cell are ranked low, while an increase in their frequency of mention has a diminishing return. - In each of the cell c in text-cube, term frequency tf(p,c) of each entity is calculated. - Using individual term frequency for each entity(protein), total sum of the term frequency cntP(c) is calculated. - A normalized popularity of phrase p in cell c , pop(p,c) is calculated by using tf(p,c) and ntfP(c) calculated in 6.2.1 and 6.2.2. [eq ref]. Distinctiveness : It is based on the relevance of a entity name to a specific category by comparing the occurrence of the protein name in the target data set, i.e., the cell documents describing one cell, to the contrastive data set, i.e., the cells of documents describing the remaining cells. - Normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] at 5.4.3 are used to calculated the relevance score rel(p,c) [eq. ref] of protein p in cell c . - Normalized distinctiveness disti(p,c) [eq ref] is calculated by using relevance score of protein p in cell c and and relevance across neighbouring cells c\u2019 (c\u2019 K(p,c) : neighbourhoods of cell c ). CaseOLAP score : It is the product of Integrity, Popularity and Distinctiveness calculated in 6.1,6.2 and 6.3. Import required libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline import json Data with open ( input/cat2pmids.json , r ) as f: cvd2pmids = json . load(f) print ( total cat2pmids: , len (cat2pmids)) with open ( input/pmid2pcount.json , r ) as f: pmid2pcount = json . load(f) print ( total pmid2pcount: , len (pmid2pcount)) Caseolap Score Calculation class Caseolap ( object ): def __init__ ( self ,cvd2pmids,pmid2pcount): self . cellnames = [] self . cvd2pmids = cvd2pmids self . pmid2pcount = pmid2pcount self . cell_pmids = {} self . cell_pmid2pcount = {} self . all_proteins = [] self . cell_uniqp = {} self . cell_p2tf = {} self . cell_tf = {} self . cell_cntp = {} self . cell_pop = {} self . cell_p2pmid = {} self . cell_ntf = {} self . cell_ndf = {} self . cell_rel = {} self . cell_dist = {} self . cell_caseolap = {} def df_builder ( self ,cell_quant,fname): flatdata = [] for p in self . all_proteins: d = { protein :p} for name in self . cellnames: d . update({name:cell_quant[name][p]}) flatdata . append(d) df = pd . DataFrame(flatdata) df = df . set_index( protein ) df . to_csv( data/ + fname + .csv ) return df def dump_json ( self ,data,fname): with open ( data/ + fname + .json , w ) as dl: json . dump(data, dl) def cell_pmids_collector ( self , dump = False ,verbose = False ): for key,value in self . cvd2pmids . items(): cell_name = key cell_pmids = value self . cellnames . append(cell_name) self . cell_pmids . update({cell_name:cell_pmids}) if verbose: print ( total pmids collected for cell - ,cell_name, len (cell_pmids)) if dump: self . dump_json( self . cell_pmids,fname = cellpmids ) def cell_pmid2pcount_collector ( self ): for key,value in self . cell_pmids . items(): cell_name = key cell_pmids = value ipmid2pcount = {} for pmid in cell_pmids: pmid_pcount = self . pmid2pcount[pmid] ipmid2pcount . update({pmid:pmid_pcount}) self . cell_pmid2pcount . update({cell_name:ipmid2pcount}) def all_protein_finder ( self ,dump = False ,verbose = False ): allproteins = [] for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value cellproteins = [] for key, value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key,value in pmid_pcount . items(): allproteins . append(key) cellproteins . append(key) uprotein = list ( set (cellproteins)) self . cell_uniqp . update({cell_name:uprotein}) if verbose: print ( total proteins collected for cell - ,cell_name, len (uprotein)) self . all_proteins = list ( set (allproteins)) if verbose: print ( total proteins collected: , len ( self . all_proteins)) if dump: self . dump_json( self . all_proteins,fname = allproteins ) self . dump_json( self . cell_uniqp,fname = unique_proteins ) def cell_map ( self ,cellpmid2pcount,select): map_dict = [] for key,value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key, value in pmid_pcount . items(): if select == tf : map_dict . append({ protein : key, tf : int (value)}) elif select == pmid : map_dict . append({ protein : key, pmid :pmid}) return map_dict def cell_reduce ( self ,Dict,col,operation): df = pd . DataFrame(Dict) df = df . set_index(col[ 0 ]) if operation == sum : gdf = df . groupby(col[ 0 ]) . sum() elif operation == count : gdf = df . groupby(col[ 0 ]) . count() index_name = list (gdf . index) csum = list (gdf[col[ 1 ]]) ucount = {} for x,y in zip (index_name,csum): ucount . update({x:y}) return ucount def cell_p2tf_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value map-reduce CellP2tf = self . cell_map(cellpmid2pcount,select = tf ) cellp2tf = self . cell_reduce(CellP2tf,[ protein , tf ],operation = sum ) self . cell_p2tf . update({cell_name:cellp2tf}) def cell_tf_finder ( self ): for key, value in self . cell_p2tf . items(): cell_name = key cellp2tf = value celltf = {} for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: celltf . update({p:cellp2tf[p]}) else : celltf . update({p: 0 }) self . cell_tf . update({cell_name:celltf}) def cell_pop_finder ( self ,dump = False ): for key,value in self . cell_tf . items(): cell_name = key cell_tf = value cellpop = {} cntp = 0 #---------------------------- for key,value in cell_tf . items(): cntp = cntp + int (value) self . cell_cntp . update({cell_name:cntp}) #------------------------------ for key,value in cell_tf . items(): pop = np . log(value + 1 ) / np . log(cntp) cellpop . update({key:pop}) self . cell_pop . update({cell_name:cellpop}) if dump: self . df_builder( self . cell_pop,fname = pop ) def cell_p2pmid_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value map-reduce CellP2pmid = self . cell_map(cellpmid2pcount,select = pmid ) cellp2pmid = self . cell_reduce(CellP2pmid,[ protein , pmid ],operation = count ) self . cell_p2pmid . update({cell_name:cellp2pmid}) def cell_ntf_finder ( self ): k1 = 1.2 b = 0.75 for key,value in self . cell_tf . items(): cell_name = key celltf = value #---------------------------- nonzero_celltf = [] for key,value in celltf . items(): if int (value) 0 : nonzero_celltf . append( int (value)) #------------------------------------------- av_cntp = self . cell_cntp[cell_name] / float ( len (nonzero_celltf)) cellntf = {} for key,value in celltf . items(): p = key tf = value ntf = (tf * (k1 + 1 )) / float (tf + (k1 * ( 1 - b + (b * ( self . cell_cntp[cell_name] / float (av_cntp)))))) cellntf . update({p:ntf}) self . cell_ntf . update({cell_name:cellntf}) def cell_ndf_finder ( self ): for key,value in self . cell_p2pmid . items(): cell_name = key cellp2pmid = value all_pmid_counts = [] cellndf = {} #-------------------------------------------- for key,value in cellp2pmid . items(): all_pmid_counts . append(value) maxv = max (all_pmid_counts) #----------------------------------------- for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: c = cellp2pmid[p] ndf = np . log( 1 + c) / np . log( 1 + maxv) else : ndf = 0 cellndf . update({p:ndf}) self . cell_ndf . update({cell_name:cellndf}) def cell_rel_finder ( self ): for key,value in self . cell_ntf . items(): cell_name = key cellntf = value cellrel = {} for p in self . all_proteins: rel = cellntf[p] * self . cell_ndf[cell_name][p] cellrel . update({p:rel}) self . cell_rel . update({cell_name:cellrel}) def cell_dist_finder ( self ,dump = False ): cell_exprel = {} for key,value in self . cell_rel . items(): cell_name = key cellrel = value cellexprel = {} for key,value in cellrel . items(): cellexprel . update({key:np . exp(value)}) cell_exprel . update({cell_name:cellexprel}) #----------------------------------------------------- p2din = {} for p in self . all_proteins: din = 1.0 for cellname in self . cellnames: din = din + cell_exprel[cellname][p] p2din . update({p:din}) #-------------------------------------------------------- for key,value in cell_exprel . items(): cell_name = key cellexprel = value celldist = {} for key,value in cellexprel . items(): celldist . update({key:value / p2din[key]}) self . cell_dist . update({cell_name:celldist}) if dump: self . df_builder( self . cell_dist,fname = dist ) def cell_cseolap_finder ( self ,dump = False ): for key,value in self . cell_dist . items(): cell_name = key celldist = value cellcaseolap = {} for key,value in celldist . items(): cellcaseolap . update({key:(value * self . cell_pop[cell_name][key])}) self . cell_caseolap . update({cell_name:cellcaseolap}) if dump: self . df_builder( self . cell_caseolap,fname = caseolap ) self . dump_json( self . cell_caseolap,fname = caseolap ) Test Run C = Caseolap(cvd2pmids,pmid2pcount) C . cell_pmids_collector(dump = True ,verbose = True ) #C.cell_pmids C . cell_pmid2pcount_collector() #C.cell_pmid2pcount C . all_protein_finder(dump = True ,verbose = True ) #C.all_proteins C . all_protein_finder() #C.all_proteins #C.cell_uniqp C . cell_p2tf_finder() #C.cell_p2tf C . cell_tf_finder() #C.cell_tf C . cell_pop_finder(dump = True ) #C.cell_pop C . cell_p2pmid_finder() #C.cell_p2pmid C . cell_ntf_finder() #C.cell_ntf C . cell_ndf_finder() #C.cell_ndf C . cell_rel_finder() #C.cell_rel C . cell_dist_finder(dump = True ) #C.cell_dist C . cell_cseolap_finder(dump = True ) #C.cell_caseolap","title":"CaseOLAP Score"},{"location":"caseolap/CaseOLAP/#caseolap-score-calculation","text":"CaseOLAP score calculation : CaseOLAP score are the quantification of user defined entity-category association. It start with the text-cube document structure and finds the entity in each document in each cell of the text-cube and by implementing updated text-cube metadata, it calculates the CaseOLAP score with following steps: Integrity : Integrity of user defined phrase is taken to be 1. (Autophrase, Segphrase) Popularity : It depends on how frequently a protein name is mentioned within one category, and it is calculated only using the statistics from the cells of documents pertaining to that individual category. Rare protein names in a cell are ranked low, while an increase in their frequency of mention has a diminishing return. - In each of the cell c in text-cube, term frequency tf(p,c) of each entity is calculated. - Using individual term frequency for each entity(protein), total sum of the term frequency cntP(c) is calculated. - A normalized popularity of phrase p in cell c , pop(p,c) is calculated by using tf(p,c) and ntfP(c) calculated in 6.2.1 and 6.2.2. [eq ref]. Distinctiveness : It is based on the relevance of a entity name to a specific category by comparing the occurrence of the protein name in the target data set, i.e., the cell documents describing one cell, to the contrastive data set, i.e., the cells of documents describing the remaining cells. - Normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] at 5.4.3 are used to calculated the relevance score rel(p,c) [eq. ref] of protein p in cell c . - Normalized distinctiveness disti(p,c) [eq ref] is calculated by using relevance score of protein p in cell c and and relevance across neighbouring cells c\u2019 (c\u2019 K(p,c) : neighbourhoods of cell c ). CaseOLAP score : It is the product of Integrity, Popularity and Distinctiveness calculated in 6.1,6.2 and 6.3.","title":"CaseOLAP score calculation"},{"location":"caseolap/CaseOLAP/#import-required-libraries","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline import json","title":"Import required libraries"},{"location":"caseolap/CaseOLAP/#data","text":"with open ( input/cat2pmids.json , r ) as f: cvd2pmids = json . load(f) print ( total cat2pmids: , len (cat2pmids)) with open ( input/pmid2pcount.json , r ) as f: pmid2pcount = json . load(f) print ( total pmid2pcount: , len (pmid2pcount))","title":"Data"},{"location":"caseolap/CaseOLAP/#caseolap-score-calculation_1","text":"class Caseolap ( object ): def __init__ ( self ,cvd2pmids,pmid2pcount): self . cellnames = [] self . cvd2pmids = cvd2pmids self . pmid2pcount = pmid2pcount self . cell_pmids = {} self . cell_pmid2pcount = {} self . all_proteins = [] self . cell_uniqp = {} self . cell_p2tf = {} self . cell_tf = {} self . cell_cntp = {} self . cell_pop = {} self . cell_p2pmid = {} self . cell_ntf = {} self . cell_ndf = {} self . cell_rel = {} self . cell_dist = {} self . cell_caseolap = {} def df_builder ( self ,cell_quant,fname): flatdata = [] for p in self . all_proteins: d = { protein :p} for name in self . cellnames: d . update({name:cell_quant[name][p]}) flatdata . append(d) df = pd . DataFrame(flatdata) df = df . set_index( protein ) df . to_csv( data/ + fname + .csv ) return df def dump_json ( self ,data,fname): with open ( data/ + fname + .json , w ) as dl: json . dump(data, dl) def cell_pmids_collector ( self , dump = False ,verbose = False ): for key,value in self . cvd2pmids . items(): cell_name = key cell_pmids = value self . cellnames . append(cell_name) self . cell_pmids . update({cell_name:cell_pmids}) if verbose: print ( total pmids collected for cell - ,cell_name, len (cell_pmids)) if dump: self . dump_json( self . cell_pmids,fname = cellpmids ) def cell_pmid2pcount_collector ( self ): for key,value in self . cell_pmids . items(): cell_name = key cell_pmids = value ipmid2pcount = {} for pmid in cell_pmids: pmid_pcount = self . pmid2pcount[pmid] ipmid2pcount . update({pmid:pmid_pcount}) self . cell_pmid2pcount . update({cell_name:ipmid2pcount}) def all_protein_finder ( self ,dump = False ,verbose = False ): allproteins = [] for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value cellproteins = [] for key, value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key,value in pmid_pcount . items(): allproteins . append(key) cellproteins . append(key) uprotein = list ( set (cellproteins)) self . cell_uniqp . update({cell_name:uprotein}) if verbose: print ( total proteins collected for cell - ,cell_name, len (uprotein)) self . all_proteins = list ( set (allproteins)) if verbose: print ( total proteins collected: , len ( self . all_proteins)) if dump: self . dump_json( self . all_proteins,fname = allproteins ) self . dump_json( self . cell_uniqp,fname = unique_proteins ) def cell_map ( self ,cellpmid2pcount,select): map_dict = [] for key,value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key, value in pmid_pcount . items(): if select == tf : map_dict . append({ protein : key, tf : int (value)}) elif select == pmid : map_dict . append({ protein : key, pmid :pmid}) return map_dict def cell_reduce ( self ,Dict,col,operation): df = pd . DataFrame(Dict) df = df . set_index(col[ 0 ]) if operation == sum : gdf = df . groupby(col[ 0 ]) . sum() elif operation == count : gdf = df . groupby(col[ 0 ]) . count() index_name = list (gdf . index) csum = list (gdf[col[ 1 ]]) ucount = {} for x,y in zip (index_name,csum): ucount . update({x:y}) return ucount def cell_p2tf_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value map-reduce CellP2tf = self . cell_map(cellpmid2pcount,select = tf ) cellp2tf = self . cell_reduce(CellP2tf,[ protein , tf ],operation = sum ) self . cell_p2tf . update({cell_name:cellp2tf}) def cell_tf_finder ( self ): for key, value in self . cell_p2tf . items(): cell_name = key cellp2tf = value celltf = {} for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: celltf . update({p:cellp2tf[p]}) else : celltf . update({p: 0 }) self . cell_tf . update({cell_name:celltf}) def cell_pop_finder ( self ,dump = False ): for key,value in self . cell_tf . items(): cell_name = key cell_tf = value cellpop = {} cntp = 0 #---------------------------- for key,value in cell_tf . items(): cntp = cntp + int (value) self . cell_cntp . update({cell_name:cntp}) #------------------------------ for key,value in cell_tf . items(): pop = np . log(value + 1 ) / np . log(cntp) cellpop . update({key:pop}) self . cell_pop . update({cell_name:cellpop}) if dump: self . df_builder( self . cell_pop,fname = pop ) def cell_p2pmid_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value map-reduce CellP2pmid = self . cell_map(cellpmid2pcount,select = pmid ) cellp2pmid = self . cell_reduce(CellP2pmid,[ protein , pmid ],operation = count ) self . cell_p2pmid . update({cell_name:cellp2pmid}) def cell_ntf_finder ( self ): k1 = 1.2 b = 0.75 for key,value in self . cell_tf . items(): cell_name = key celltf = value #---------------------------- nonzero_celltf = [] for key,value in celltf . items(): if int (value) 0 : nonzero_celltf . append( int (value)) #------------------------------------------- av_cntp = self . cell_cntp[cell_name] / float ( len (nonzero_celltf)) cellntf = {} for key,value in celltf . items(): p = key tf = value ntf = (tf * (k1 + 1 )) / float (tf + (k1 * ( 1 - b + (b * ( self . cell_cntp[cell_name] / float (av_cntp)))))) cellntf . update({p:ntf}) self . cell_ntf . update({cell_name:cellntf}) def cell_ndf_finder ( self ): for key,value in self . cell_p2pmid . items(): cell_name = key cellp2pmid = value all_pmid_counts = [] cellndf = {} #-------------------------------------------- for key,value in cellp2pmid . items(): all_pmid_counts . append(value) maxv = max (all_pmid_counts) #----------------------------------------- for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: c = cellp2pmid[p] ndf = np . log( 1 + c) / np . log( 1 + maxv) else : ndf = 0 cellndf . update({p:ndf}) self . cell_ndf . update({cell_name:cellndf}) def cell_rel_finder ( self ): for key,value in self . cell_ntf . items(): cell_name = key cellntf = value cellrel = {} for p in self . all_proteins: rel = cellntf[p] * self . cell_ndf[cell_name][p] cellrel . update({p:rel}) self . cell_rel . update({cell_name:cellrel}) def cell_dist_finder ( self ,dump = False ): cell_exprel = {} for key,value in self . cell_rel . items(): cell_name = key cellrel = value cellexprel = {} for key,value in cellrel . items(): cellexprel . update({key:np . exp(value)}) cell_exprel . update({cell_name:cellexprel}) #----------------------------------------------------- p2din = {} for p in self . all_proteins: din = 1.0 for cellname in self . cellnames: din = din + cell_exprel[cellname][p] p2din . update({p:din}) #-------------------------------------------------------- for key,value in cell_exprel . items(): cell_name = key cellexprel = value celldist = {} for key,value in cellexprel . items(): celldist . update({key:value / p2din[key]}) self . cell_dist . update({cell_name:celldist}) if dump: self . df_builder( self . cell_dist,fname = dist ) def cell_cseolap_finder ( self ,dump = False ): for key,value in self . cell_dist . items(): cell_name = key celldist = value cellcaseolap = {} for key,value in celldist . items(): cellcaseolap . update({key:(value * self . cell_pop[cell_name][key])}) self . cell_caseolap . update({cell_name:cellcaseolap}) if dump: self . df_builder( self . cell_caseolap,fname = caseolap ) self . dump_json( self . cell_caseolap,fname = caseolap )","title":"Caseolap Score Calculation"},{"location":"caseolap/CaseOLAP/#test-run","text":"C = Caseolap(cvd2pmids,pmid2pcount) C . cell_pmids_collector(dump = True ,verbose = True ) #C.cell_pmids C . cell_pmid2pcount_collector() #C.cell_pmid2pcount C . all_protein_finder(dump = True ,verbose = True ) #C.all_proteins C . all_protein_finder() #C.all_proteins #C.cell_uniqp C . cell_p2tf_finder() #C.cell_p2tf C . cell_tf_finder() #C.cell_tf C . cell_pop_finder(dump = True ) #C.cell_pop C . cell_p2pmid_finder() #C.cell_p2pmid C . cell_ntf_finder() #C.cell_ntf C . cell_ndf_finder() #C.cell_ndf C . cell_rel_finder() #C.cell_rel C . cell_dist_finder(dump = True ) #C.cell_dist C . cell_cseolap_finder(dump = True ) #C.cell_caseolap","title":"Test Run"},{"location":"caseolap/Download/","text":"Download Pipeline This pipeline downloads the data from the source server. It checks the integrity of the downloaded data and extracts it to the target directory. Following are the outline the download of the pipeline. 1. Import Required Python packages import os import sys import re import time import subprocess 2. Specify source and target data directory DATA_DIR = ./ FTP address for baseline directory of Pubmed BASELINE_DIR = os . path . join(DATA_DIR,\\ ftp.ncbi.nlm.nih.gov/pubmed/baseline/ ) FTP address for baseline directory of Pubmed UPDATE_FILES_DIR = os . path . join(DATA_DIR,\\ ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ ) 3. Controlling download with os package and wget command. os.path.join() functionality is implemented to join one or more path components intelligently. os.system (wget command) functionality is implemented to execute the wget command in a subshell. The following are optional syntax modifiers for the wget command to control the download: - flag -q turns off wget output, - flag -r turns on recursive retrieving, - directory prefix --directory-prefix=%s sets the prefix of all other files and subdirectories, - directory based limit --no-parent guarantees that you will never leave the existing hierarchy of the directory. def download_pubmed_baseline (): Baseline print ( Start downloading pubmed baseline files. ,\\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/ ) t1 = time . time() rc = os . system( wget -q -r --directory-prefix= %s --no-parent \\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/ % DATA_DIR) if rc != 0 : print ( Return code of downloading pubmed \\ baseline files via wget is %d , not zero. % rc) print ( Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline ) exit(rc) t2 = time . time() print ( Finish downloading pubmed baseline files. %f s % (t2 - t1)) def download_pubmed_update (): Update print ( Start downloading pubmed updatefiles. , \\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ ) rc = os . system( wget -q -r --directory-prefix= %s --no-parent \\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ % DATA_DIR) if rc != 0 : print ( Return code of downloading pubmed update \\ files via wget is %d , not zero. % rc) print ( Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles ) exit(rc) t3 = time . time() print ( Finish downloading pubmed updatefiles. %f s % (t3 - t2)) def download_bioconcepts2pubtator_offsets (): bioconcepts2pubtator rc = os . system( wget -q --directory-prefix= %s \\ ftp://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator/ \\ bioconcepts2pubtator_offsets.gz % DATA_DIR) if rc != 0 : print ( Return code of downloading pubmed update \\ files via wget is %d , not zero. % rc) print ( Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles ) exit(rc) 4. Md5-checksum An MD5-checksum is a 32-character hexadecimal number that is computed on a file. This number helps to verify the integrity of the file download. Implementing the re and subprocess packages adds MD5-checksum functionality. def check_all_md5_in_dir ( dir ): if os . system( which md5sum 1 /dev/null ) != 0 : print ( md5sum not found ) # Continue executing return count = 0 print ( ==== Start checking md5 in %s ==== % dir ) if os . path . isdir( dir ): for file in os . listdir( dir ): if re . search( ^medline17n\\d\\d\\d\\d.xml.gz$ , file): count += 1 check_md5(os . path . join( dir , file)) if count % 100 == 0 : print ( %d files checked % count) print ( ==== All md5 check succeeded ( %d files) ==== % count) else : print ( Directory not found: %s (for md5 check) % dir ) def check_md5 (file): if os . path . isfile(file) and os . path . isfile(file + .md5 ): # Work only on Linux, user md5 for Mac stdout = subprocess . check_output( md5sum %s % \\ file, shell = True ) . decode( utf-8 ) md5_calculated = re . search( [0-9a-f] {32} , stdout) . group( 0 ) md5 = re . search( [0-9a-f] {32} , open (file + .md5 ,\\ r ) . readline()) . group( 0 ) if md5 != md5_calculated: print ( Error: md5 check failed for file %s % file) exit( 1 ) 5. Data extraction : Downloaded data files are in a compressed \u2018.gz\u2019 format, which need to be extracted. A data extraction pipeline can be created with the following steps: - import regular expression (re) , and subprocess modules, - list all data files using os.listdir functionality, - with os and wget command, gunzip -fqk , extract all files in a loop. # Assume filename is *.gz def extract (file): rc = os . system( gunzip -fqk %s % file) if rc != 0 : print ( gunzip return code for file %s is %d , \\ not zero % (file, rc)) exit(rc) return rc def extract_all_gz_in_dir ( dir ): if os . path . isdir( dir ): count = 0 print ( ==== Start extracting in %s ==== % dir ) t1 = time . time() for file in os . listdir( dir ): if re . search( .*\\.gz$ , file): extract(os . path . join( dir , file)) count += 1 if count % 50 == 0 : print ( %d files extracted, %f s taken so far % (count, time . time() - t1)) print ( ==== All files extracted ( %d files). \\ Total time: %f s ==== % (count, time . time() - t1)) else : print ( Directory not found: %s (for extraction) % dir ) Run the pipeline BASELINE_DIR = os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/baseline/ ) UPDATE_FILES_DIR = os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ ) Download download_pubmed_baseline() download_pubmed_update() MD5 Checksum check_all_md5_in_dir(BASELINE_DIR) check_all_md5_in_dir(UPDATE_FILES_DIR) Extraction extract_all_gz_in_dir(BASELINE_DIR) extract_all_gz_in_dir(UPDATE_FILES_DIR) bioconcepts2pubtator download_bioconcepts2pubtator_offsets() extract(os . path . join(DATA_DIR, bioconcepts2pubtator_offsets.gz ))","title":"Download Pipeline"},{"location":"caseolap/Download/#download-pipeline","text":"This pipeline downloads the data from the source server. It checks the integrity of the downloaded data and extracts it to the target directory. Following are the outline the download of the pipeline. 1. Import Required Python packages import os import sys import re import time import subprocess 2. Specify source and target data directory DATA_DIR = ./ FTP address for baseline directory of Pubmed BASELINE_DIR = os . path . join(DATA_DIR,\\ ftp.ncbi.nlm.nih.gov/pubmed/baseline/ ) FTP address for baseline directory of Pubmed UPDATE_FILES_DIR = os . path . join(DATA_DIR,\\ ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ ) 3. Controlling download with os package and wget command. os.path.join() functionality is implemented to join one or more path components intelligently. os.system (wget command) functionality is implemented to execute the wget command in a subshell. The following are optional syntax modifiers for the wget command to control the download: - flag -q turns off wget output, - flag -r turns on recursive retrieving, - directory prefix --directory-prefix=%s sets the prefix of all other files and subdirectories, - directory based limit --no-parent guarantees that you will never leave the existing hierarchy of the directory. def download_pubmed_baseline (): Baseline print ( Start downloading pubmed baseline files. ,\\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/ ) t1 = time . time() rc = os . system( wget -q -r --directory-prefix= %s --no-parent \\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/ % DATA_DIR) if rc != 0 : print ( Return code of downloading pubmed \\ baseline files via wget is %d , not zero. % rc) print ( Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline ) exit(rc) t2 = time . time() print ( Finish downloading pubmed baseline files. %f s % (t2 - t1)) def download_pubmed_update (): Update print ( Start downloading pubmed updatefiles. , \\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ ) rc = os . system( wget -q -r --directory-prefix= %s --no-parent \\ ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ % DATA_DIR) if rc != 0 : print ( Return code of downloading pubmed update \\ files via wget is %d , not zero. % rc) print ( Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles ) exit(rc) t3 = time . time() print ( Finish downloading pubmed updatefiles. %f s % (t3 - t2)) def download_bioconcepts2pubtator_offsets (): bioconcepts2pubtator rc = os . system( wget -q --directory-prefix= %s \\ ftp://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator/ \\ bioconcepts2pubtator_offsets.gz % DATA_DIR) if rc != 0 : print ( Return code of downloading pubmed update \\ files via wget is %d , not zero. % rc) print ( Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles ) exit(rc) 4. Md5-checksum An MD5-checksum is a 32-character hexadecimal number that is computed on a file. This number helps to verify the integrity of the file download. Implementing the re and subprocess packages adds MD5-checksum functionality. def check_all_md5_in_dir ( dir ): if os . system( which md5sum 1 /dev/null ) != 0 : print ( md5sum not found ) # Continue executing return count = 0 print ( ==== Start checking md5 in %s ==== % dir ) if os . path . isdir( dir ): for file in os . listdir( dir ): if re . search( ^medline17n\\d\\d\\d\\d.xml.gz$ , file): count += 1 check_md5(os . path . join( dir , file)) if count % 100 == 0 : print ( %d files checked % count) print ( ==== All md5 check succeeded ( %d files) ==== % count) else : print ( Directory not found: %s (for md5 check) % dir ) def check_md5 (file): if os . path . isfile(file) and os . path . isfile(file + .md5 ): # Work only on Linux, user md5 for Mac stdout = subprocess . check_output( md5sum %s % \\ file, shell = True ) . decode( utf-8 ) md5_calculated = re . search( [0-9a-f] {32} , stdout) . group( 0 ) md5 = re . search( [0-9a-f] {32} , open (file + .md5 ,\\ r ) . readline()) . group( 0 ) if md5 != md5_calculated: print ( Error: md5 check failed for file %s % file) exit( 1 ) 5. Data extraction : Downloaded data files are in a compressed \u2018.gz\u2019 format, which need to be extracted. A data extraction pipeline can be created with the following steps: - import regular expression (re) , and subprocess modules, - list all data files using os.listdir functionality, - with os and wget command, gunzip -fqk , extract all files in a loop. # Assume filename is *.gz def extract (file): rc = os . system( gunzip -fqk %s % file) if rc != 0 : print ( gunzip return code for file %s is %d , \\ not zero % (file, rc)) exit(rc) return rc def extract_all_gz_in_dir ( dir ): if os . path . isdir( dir ): count = 0 print ( ==== Start extracting in %s ==== % dir ) t1 = time . time() for file in os . listdir( dir ): if re . search( .*\\.gz$ , file): extract(os . path . join( dir , file)) count += 1 if count % 50 == 0 : print ( %d files extracted, %f s taken so far % (count, time . time() - t1)) print ( ==== All files extracted ( %d files). \\ Total time: %f s ==== % (count, time . time() - t1)) else : print ( Directory not found: %s (for extraction) % dir ) Run the pipeline BASELINE_DIR = os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/baseline/ ) UPDATE_FILES_DIR = os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ ) Download download_pubmed_baseline() download_pubmed_update() MD5 Checksum check_all_md5_in_dir(BASELINE_DIR) check_all_md5_in_dir(UPDATE_FILES_DIR) Extraction extract_all_gz_in_dir(BASELINE_DIR) extract_all_gz_in_dir(UPDATE_FILES_DIR) bioconcepts2pubtator download_bioconcepts2pubtator_offsets() extract(os . path . join(DATA_DIR, bioconcepts2pubtator_offsets.gz ))","title":"Download Pipeline"},{"location":"caseolap/EntityCount/","text":"Entity Count CaseOLAP score calculation requires the entity count per document. This step provides the data which connects metadata for text-cube structure and entity-count per document. Selection of entity : Entity(phrase) are defined by user to be studied under the text-cube document structure. User defined entity could be protein names, chemicals, disease or signs and symptoms etc. Search and listing of entity based document: The search functionality in Elasticsearch DSL package uses index name, parameters and query to list the document from indexed database. The query includes all representatives of the specific entity e.g. synonyms, abbreviations. Entity count : Then each document from the list is analysed one by one to count the entity also called term frequency, tf(p,c) . With the help of text-cube metadata, for each of the document in cell of the text-cube, the entity count is recorded as PMID to entity count mapping. Text-cube metadata update : Once the entity count is completed, text cube metadata is updated by adding PMID to entity count mapping . Following are the additional metadata prepared: - count the total occurance of each entity cntP(c) within each cell, - count the total number of documents df(p,c) within a cell in which entity appears. - calculate normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] using quantities obtained above. Import required libearies import sys import json from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q from collections import Counter year_constraints = None Setting up input output directories input_index_dir = ../../elasticsearch-6.2.2/data input_file_pmid_and_cat = input/pmid_and_cat.json # a file containing cell to pmid mapping input_file_entity_list = ../data/entities.txt # a file containing all entities output_file_paper_entity_count = output/paper_entity_count.txt # an output file for entity count per PMID output_file_paper_category = output/paper_category.txt # an output file containing PMID to cell mapping Read in paper info and entity info Read in paper info and entity info with open (input_file_pmid_and_cat, r ) as f_in: pmid_and_cat = json . load(f_in) concerned_pmid_set = set ( map ( lambda x: x[ 0 ], pmid_and_cat)) entity_count_per_pmid = {pmid: Counter() for pmid in concerned_pmid_set} entity_dict = {} with open (input_file_entity_list, r ) as f_in: for line in f_in: # synonums seperated by | and represented by the first one on each line line_split = line . strip() . split( | ) entity_dict[line_split[ 0 ]] = line_split Search and count entities: to optimize and find count from indexer Search and count entities: to optimize and find count from indexer es = Elasticsearch(timeout = 300 ) k = 0 for entity_rep in entity_dict: for entity in entity_dict[entity_rep]: #entity_space_sep = .join(map(lambda x: if x == _ else x, entity)) entity_space_sep = entity . replace( _ , ) # s = Search(using=es, index= pmc_all_index ).\\ query( match , abstract = entity_space_sep) s = Search(using = es, index = pubmed )\\ . params(request_timeout = 300 )\\ . query( match_phrase , abstract = entity_space_sep) num_hits = 0 num_valid_hits = 0 num_counts = 0 for hit in s . scan(): num_hits += 1 cur_pmid = str (hit . pmid) if cur_pmid not in concerned_pmid_set: continue #if hit.PMCflag != 0: # continue if year_constraints is not None : # to-do print ( To add year constraint handler. ) abs_lower = hit . abstract . lower() . replace( - , ) entity_lower = entity_space_sep . lower() . replace( - , ) entity_cnt = abs_lower . count(entity_lower) if entity_cnt == 0 : #print ---------- , entity_space_sep, ---------- #print abs_lower continue entity_count_per_pmid[cur_pmid][entity_rep] += entity_cnt num_valid_hits += 1 num_counts += entity_cnt #print(entity, # hits: , num_hits, # valid hits: , num_valid_hits, # counts: , num_counts) k = k + 1 if k % 1000 == 0 : print (k, entity counted! ) Entity count metadata update in each Cell Output ## paper entity count paper category with open (output_file_paper_entity_count, w ) as f_out_entity_count,\\ open (output_file_paper_category, w ) as f_out_category: f_out_category . write( doc_id \\t label_id \\n ) paper_new_id = 1 for cur_pmid, cur_cat in pmid_and_cat: if len (entity_count_per_pmid[cur_pmid]) == 0 : continue # print paper category f_out_category . write( str (cur_pmid) + \\t + str (cur_cat) + \\n ) # print paper entity count f_out_entity_count . write( str (cur_pmid)) for entity in entity_count_per_pmid[cur_pmid]: f_out_entity_count . write( + entity + | + \\ str (entity_count_per_pmid[cur_pmid][entity])) f_out_entity_count . write( \\n ) paper_new_id += 1 PMID to Entity count Dictionary with open ( paper_entity_count.txt ) as ff: pmid2pcount = {} PMID2PCOUNT = [] for line in ff: item = line . split() pmid = item[ 0 ] if len (item) 1 : prot_freq = {} for pf in item[ 1 :]: pfs = pf . split( | ) prot_freq . update({pfs[ 0 ]:pfs[ 1 ]}) pmid2pcount . update({pmid:prot_freq}) with open ( pmid2pcount.json , w ) as fp: json . dump(pmid2pcount, fp) PMID to Category Dictionary with open ( paper_category.txt ) as f: allpmids = [] PMID2CVD = [] cvd2pmids = {} dis0 = [] dis1 = [] for line in f: item = line . split() if item[ 0 ] != doc_id : if item[ 0 ] in pmid2pcount: allpmids . append(item[ 0 ]) PMID2CVD . append({ pmid :item[ 0 ], cat :item[ 1 ]}) if item[ 1 ] == 0 : dis0 . append(item[ 0 ]) elif item[ 1 ] == 1 : dis1 . append(item[ 0 ]) cvd2pmids . update({dis[ 0 ]:dis0,dis[ 1 ]:dis1}) with open ( cat2pmids.json , w ) as fp: json . dump(cvd2pmids, fp)","title":"Entity Search"},{"location":"caseolap/EntityCount/#entity-count","text":"CaseOLAP score calculation requires the entity count per document. This step provides the data which connects metadata for text-cube structure and entity-count per document. Selection of entity : Entity(phrase) are defined by user to be studied under the text-cube document structure. User defined entity could be protein names, chemicals, disease or signs and symptoms etc. Search and listing of entity based document: The search functionality in Elasticsearch DSL package uses index name, parameters and query to list the document from indexed database. The query includes all representatives of the specific entity e.g. synonyms, abbreviations. Entity count : Then each document from the list is analysed one by one to count the entity also called term frequency, tf(p,c) . With the help of text-cube metadata, for each of the document in cell of the text-cube, the entity count is recorded as PMID to entity count mapping. Text-cube metadata update : Once the entity count is completed, text cube metadata is updated by adding PMID to entity count mapping . Following are the additional metadata prepared: - count the total occurance of each entity cntP(c) within each cell, - count the total number of documents df(p,c) within a cell in which entity appears. - calculate normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] using quantities obtained above.","title":"Entity Count"},{"location":"caseolap/EntityCount/#import-required-libearies","text":"import sys import json from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q from collections import Counter year_constraints = None","title":"Import required libearies"},{"location":"caseolap/EntityCount/#setting-up-input-output-directories","text":"input_index_dir = ../../elasticsearch-6.2.2/data input_file_pmid_and_cat = input/pmid_and_cat.json # a file containing cell to pmid mapping input_file_entity_list = ../data/entities.txt # a file containing all entities output_file_paper_entity_count = output/paper_entity_count.txt # an output file for entity count per PMID output_file_paper_category = output/paper_category.txt # an output file containing PMID to cell mapping","title":"Setting up input output directories"},{"location":"caseolap/EntityCount/#read-in-paper-info-and-entity-info","text":"Read in paper info and entity info with open (input_file_pmid_and_cat, r ) as f_in: pmid_and_cat = json . load(f_in) concerned_pmid_set = set ( map ( lambda x: x[ 0 ], pmid_and_cat)) entity_count_per_pmid = {pmid: Counter() for pmid in concerned_pmid_set} entity_dict = {} with open (input_file_entity_list, r ) as f_in: for line in f_in: # synonums seperated by | and represented by the first one on each line line_split = line . strip() . split( | ) entity_dict[line_split[ 0 ]] = line_split","title":"Read in paper info and entity info"},{"location":"caseolap/EntityCount/#search-and-count-entities-to-optimize-and-find-count-from-indexer","text":"Search and count entities: to optimize and find count from indexer es = Elasticsearch(timeout = 300 ) k = 0 for entity_rep in entity_dict: for entity in entity_dict[entity_rep]: #entity_space_sep = .join(map(lambda x: if x == _ else x, entity)) entity_space_sep = entity . replace( _ , ) # s = Search(using=es, index= pmc_all_index ).\\ query( match , abstract = entity_space_sep) s = Search(using = es, index = pubmed )\\ . params(request_timeout = 300 )\\ . query( match_phrase , abstract = entity_space_sep) num_hits = 0 num_valid_hits = 0 num_counts = 0 for hit in s . scan(): num_hits += 1 cur_pmid = str (hit . pmid) if cur_pmid not in concerned_pmid_set: continue #if hit.PMCflag != 0: # continue if year_constraints is not None : # to-do print ( To add year constraint handler. ) abs_lower = hit . abstract . lower() . replace( - , ) entity_lower = entity_space_sep . lower() . replace( - , ) entity_cnt = abs_lower . count(entity_lower) if entity_cnt == 0 : #print ---------- , entity_space_sep, ---------- #print abs_lower continue entity_count_per_pmid[cur_pmid][entity_rep] += entity_cnt num_valid_hits += 1 num_counts += entity_cnt #print(entity, # hits: , num_hits, # valid hits: , num_valid_hits, # counts: , num_counts) k = k + 1 if k % 1000 == 0 : print (k, entity counted! )","title":"Search and count entities: to optimize and find count from indexer"},{"location":"caseolap/EntityCount/#entity-count-metadata-update-in-each-cell","text":"Output ## paper entity count paper category with open (output_file_paper_entity_count, w ) as f_out_entity_count,\\ open (output_file_paper_category, w ) as f_out_category: f_out_category . write( doc_id \\t label_id \\n ) paper_new_id = 1 for cur_pmid, cur_cat in pmid_and_cat: if len (entity_count_per_pmid[cur_pmid]) == 0 : continue # print paper category f_out_category . write( str (cur_pmid) + \\t + str (cur_cat) + \\n ) # print paper entity count f_out_entity_count . write( str (cur_pmid)) for entity in entity_count_per_pmid[cur_pmid]: f_out_entity_count . write( + entity + | + \\ str (entity_count_per_pmid[cur_pmid][entity])) f_out_entity_count . write( \\n ) paper_new_id += 1","title":"Entity count metadata update in each Cell"},{"location":"caseolap/EntityCount/#pmid-to-entity-count-dictionary","text":"with open ( paper_entity_count.txt ) as ff: pmid2pcount = {} PMID2PCOUNT = [] for line in ff: item = line . split() pmid = item[ 0 ] if len (item) 1 : prot_freq = {} for pf in item[ 1 :]: pfs = pf . split( | ) prot_freq . update({pfs[ 0 ]:pfs[ 1 ]}) pmid2pcount . update({pmid:prot_freq}) with open ( pmid2pcount.json , w ) as fp: json . dump(pmid2pcount, fp)","title":"PMID to Entity count Dictionary"},{"location":"caseolap/EntityCount/#pmid-to-category-dictionary","text":"with open ( paper_category.txt ) as f: allpmids = [] PMID2CVD = [] cvd2pmids = {} dis0 = [] dis1 = [] for line in f: item = line . split() if item[ 0 ] != doc_id : if item[ 0 ] in pmid2pcount: allpmids . append(item[ 0 ]) PMID2CVD . append({ pmid :item[ 0 ], cat :item[ 1 ]}) if item[ 1 ] == 0 : dis0 . append(item[ 0 ]) elif item[ 1 ] == 1 : dis1 . append(item[ 0 ]) cvd2pmids . update({dis[ 0 ]:dis0,dis[ 1 ]:dis1}) with open ( cat2pmids.json , w ) as fp: json . dump(cvd2pmids, fp)","title":"PMID to Category Dictionary"},{"location":"caseolap/Indexing/","text":"Indexing and Search Indexing and search : This step prepares an indexed database which facilitates the entity search and counting operation . First, initialize the ElasticSearch index object. Then the index is populated in batches with bulk indexing functionality available in Elasticsearch package. Installation of required python package : Install and import elasticsearch and elasticsearch_dsl library to the current python environment. import time import re import sys import os from collections import defaultdict from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q Index initialization : Index initialization is done with the index information which includes index name, type name, number of shards, number of replicas. INDEX_NAME = pubmed TYPE_NAME = pubmed_meta NUMBER_SHARDS = 1 # keep this as one if no clusterNUMBER_REPLICAS = 0 following is the defined schema totally 5 fields: pmid, date, author list, journal name, mesh heading list request_body = { settings : { number_of_shards : NUMBER_SHARDS, number_of_replicas : NUMBER_REPLICAS }, mappings : { TYPE_NAME: { properties : { pmid : { type : keyword }, # date : { # type : long # }, # author_list : { # type : keyword # }, # journal_name : { # type : keyword # }, mesh_heading : { type : text , similarity : BM25 }, abstract :{ type : text } } } } } es = Elasticsearch() if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( Deleting index %s , Response: %s % (INDEX_NAME, res)) res = es . indices . create(index = INDEX_NAME, body = request_body) print ( Create index %s , Response: %s % (INDEX_NAME, res)) Bulk indexing : In the first step of bulk indexing, data bulk is created with two components. - First component is a dictionary with metadata information of index name, type name and bluck id which is \u2018pmid\u2019 key. - Prepare the second component which is data dictionary with all information of \u2018title\u2019,\u2019abstract\u2019,\u2019MeSH\u2019 etc. inputFilePath = ./pubmed.json logFilePath = ./index_pubmed_log_20171001.txt INDEX_NAME = pubmed TYPE_NAME = pubmed_meta es = Elasticsearch() mesh2pmid = dict () ic = 0 ir = 0 with open (inputFilePath, r ) as fin, open (logFilePath, w ) as fout: start = time . time() bulk_size = 5000 # number of document processed in each bulk index bulk_data = [] # data in bulk index cnt = 0 for line in fin: ## each line is single document try : cnt += 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ pmid ] = paperInfo . get( PMID , -1 ) #update MeSH Heading data_dict[ mesh_heading ] = \\ . join(paperInfo[ MeshHeadingList ]) # update Abstract data_dict[ abstract ] = paperInfo . get( Abstract , )\\ . lower() . replace( - , ) ## Put current data into the bulk op_dict = { index : { _index : INDEX_NAME, _type : TYPE_NAME, _id : data_dict[ pmid ] } } bulk_data . append(op_dict) bulk_data . append(data_dict) ## Start Bulk indexing if cnt % bulk_size == 0 and cnt != 0 : ic += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data,\\ request_timeout = 500 ) fout . write( bulk indexing... %s , \\ escaped time %s (seconds) \\n \\ % ( cnt, tmp - start ) ) if ic % 100 == 0 : print ( i bulk indexing... %s , \\ escaped time %s (seconds) \\ % ( cnt, tmp - start ) ) bulk_data = [] except : cnt -= 1 print ( XXXX Unexpected Error happened at line: XXXX ) #print(line) ## indexing those left papers if bulk_data: ir += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data,\\ request_timeout = 500 ) fout . write( bulk indexing... %s , \\ escaped time %s (seconds) \\n \\ % ( cnt, tmp - start ) ) if ir % 100 == 0 : print ( r bulk indexing... %s , \\ escaped time %s (seconds) \\ % ( cnt, tmp - start ) ) bulk_data = [] end = time . time() fout . write( Finish PubMed meta-data indexing. \\ Total escaped time %s (seconds) \\n \\ % (end - start) ) print ( Finish PubMed meta-data indexing. \\ Total escaped time %s (seconds) \\ % (end - start) ) Search functionality : One can perform search operation over data index created by Elasticsearch application. Once a search operation is initiated for user defined entity, it gathers information of that entity as a ranked list. Following are the steps for search operation: - start the Elasticsearch server - implement Elasticsearch DSL search functionality with index name, parameters and query - iterate over all hits obtained in search result to find desired entity","title":"Indexing Documents"},{"location":"caseolap/Indexing/#indexing-and-search","text":"Indexing and search : This step prepares an indexed database which facilitates the entity search and counting operation . First, initialize the ElasticSearch index object. Then the index is populated in batches with bulk indexing functionality available in Elasticsearch package. Installation of required python package : Install and import elasticsearch and elasticsearch_dsl library to the current python environment. import time import re import sys import os from collections import defaultdict from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q Index initialization : Index initialization is done with the index information which includes index name, type name, number of shards, number of replicas. INDEX_NAME = pubmed TYPE_NAME = pubmed_meta NUMBER_SHARDS = 1 # keep this as one if no clusterNUMBER_REPLICAS = 0 following is the defined schema totally 5 fields: pmid, date, author list, journal name, mesh heading list request_body = { settings : { number_of_shards : NUMBER_SHARDS, number_of_replicas : NUMBER_REPLICAS }, mappings : { TYPE_NAME: { properties : { pmid : { type : keyword }, # date : { # type : long # }, # author_list : { # type : keyword # }, # journal_name : { # type : keyword # }, mesh_heading : { type : text , similarity : BM25 }, abstract :{ type : text } } } } } es = Elasticsearch() if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( Deleting index %s , Response: %s % (INDEX_NAME, res)) res = es . indices . create(index = INDEX_NAME, body = request_body) print ( Create index %s , Response: %s % (INDEX_NAME, res)) Bulk indexing : In the first step of bulk indexing, data bulk is created with two components. - First component is a dictionary with metadata information of index name, type name and bluck id which is \u2018pmid\u2019 key. - Prepare the second component which is data dictionary with all information of \u2018title\u2019,\u2019abstract\u2019,\u2019MeSH\u2019 etc. inputFilePath = ./pubmed.json logFilePath = ./index_pubmed_log_20171001.txt INDEX_NAME = pubmed TYPE_NAME = pubmed_meta es = Elasticsearch() mesh2pmid = dict () ic = 0 ir = 0 with open (inputFilePath, r ) as fin, open (logFilePath, w ) as fout: start = time . time() bulk_size = 5000 # number of document processed in each bulk index bulk_data = [] # data in bulk index cnt = 0 for line in fin: ## each line is single document try : cnt += 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ pmid ] = paperInfo . get( PMID , -1 ) #update MeSH Heading data_dict[ mesh_heading ] = \\ . join(paperInfo[ MeshHeadingList ]) # update Abstract data_dict[ abstract ] = paperInfo . get( Abstract , )\\ . lower() . replace( - , ) ## Put current data into the bulk op_dict = { index : { _index : INDEX_NAME, _type : TYPE_NAME, _id : data_dict[ pmid ] } } bulk_data . append(op_dict) bulk_data . append(data_dict) ## Start Bulk indexing if cnt % bulk_size == 0 and cnt != 0 : ic += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data,\\ request_timeout = 500 ) fout . write( bulk indexing... %s , \\ escaped time %s (seconds) \\n \\ % ( cnt, tmp - start ) ) if ic % 100 == 0 : print ( i bulk indexing... %s , \\ escaped time %s (seconds) \\ % ( cnt, tmp - start ) ) bulk_data = [] except : cnt -= 1 print ( XXXX Unexpected Error happened at line: XXXX ) #print(line) ## indexing those left papers if bulk_data: ir += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data,\\ request_timeout = 500 ) fout . write( bulk indexing... %s , \\ escaped time %s (seconds) \\n \\ % ( cnt, tmp - start ) ) if ir % 100 == 0 : print ( r bulk indexing... %s , \\ escaped time %s (seconds) \\ % ( cnt, tmp - start ) ) bulk_data = [] end = time . time() fout . write( Finish PubMed meta-data indexing. \\ Total escaped time %s (seconds) \\n \\ % (end - start) ) print ( Finish PubMed meta-data indexing. \\ Total escaped time %s (seconds) \\ % (end - start) ) Search functionality : One can perform search operation over data index created by Elasticsearch application. Once a search operation is initiated for user defined entity, it gathers information of that entity as a ranked list. Following are the steps for search operation: - start the Elasticsearch server - implement Elasticsearch DSL search functionality with index name, parameters and query - iterate over all hits obtained in search result to find desired entity","title":"Indexing and Search"},{"location":"caseolap/Parsing/","text":"Data Parsingn Pipeline This pipeline will parse the extracted data and convert it into data structures compatible with the CaseOLAP pipeline. Installation of required python package : Install and import lxml, itertools and json libraries into the current python environment. import re import itertools import json import sys import os import time import traceback from lxml import etree Set up output data dir DATA_DIR = ./ MeSH statistics dictionary mesh_statistics = {} Data parsing strategy : The extracted data is an XML file, and text data is embedded in the tree structure of XML document. The following are steps for parsing data: - Implement the etree functionality in lxml module to dig into the tree structure of XML document. - The separate components e.g. PMID, authors, abstract , MeSH etc. of the data is obtained by using tags representing these components. - Implement the chain functionality in itertools to creates an iterator that returns elements from the iterables which was obtained by implementing etree functionality in lxml . # Search the tag in the xml element # Return tag s text if tag exists, return empty string if doesn t def get_text (element, tag): e = element . find(tag) if e is not None : return e . text else : return # !ELEMENT AuthorList (Author+) # !ELEMENT Author (((LastName, ForeName?, Initials?, Suffix?)\\ # | CollectiveName), Identifier*, AffiliationInfo*) def parse_author (authors): result = [] for author in authors: item = {} item[ LastName ] = get_text(author, LastName ) item[ ForeName ] = get_text(author, ForeName ) item[ Initials ] = get_text(author, Initials ) item[ Suffix ] = get_text(author, Suffix ) item[ CollectiveName ] = get_text(author, CollectiveName ) result . append(item) return result Creation of dictionary of parsed data : A python dictionary is created with all the components as key-value pair. This JSON-like data structure makes it compatible for indexing and searching in Elasticsearch which is described in step 3 of protocol. Creation of MeSH to PMID mapping : During the creation of dictionary of parsed data, MeSH to PMID mapping table can also be created. This mapping is used to create Text-cube(in step 4) document structure as an requirement of CaseOLAP algorithm. def parse_pubmed_file (file, pubmed_output_file, pmid2mesh_output_file): print ( Start parsing %s % file) sys . stdout . flush() t1 = time . time() f = open (file, r ) tree = etree . parse(f) articles = itertools . chain(tree . findall( PubmedArticle ),\\ tree . findall( BookDocument )) count = 0 noabs = 0 for article in articles: count += 1 result = {} pmid2mesh = {} # PMID - Exactly One Occurrance result[ PMID ] = get_text(article, .//PMID ) pmid2mesh[ PMID ] = get_text(article, .//PMID ) # # Article title - Zero or One Occurrences # result[ ArticleTitle ] = get_text(article, .//ArticleTitle ) # Abstract - Zero or One Occurrences abstractList = article . find( .//Abstract ) if abstractList != None : try : abstract = \\n . join([line . text for line in abstractList . \\ findall( AbstractText )]) result[ Abstract ] = abstract except : result[ Abstract ] = noabs += 1 else : result[ Abstract ] = noabs += 1 # # Author List - Zero or More Occurrences # authors = article.findall( .//Author ) # result[ AuthorList ] = parse_author(authors) # # Journal - Exactly One Occurrance # journal = article.find( .//Journal ) # result[ Journal ] = get_text(journal, Title ) result[ PubDate ] = {} result[ PubDate ][ Year ] = get_text(journal,\\ JournalIssue/PubDate/Year ) # result[ PubDate ][ Month ] = get_text(journal,\\ # JournalIssue/PubDate/Month ) # result[ PubDate ][ Day ] = get_text(journal,\\ # JournalIssue/PubDate/Day ) # result[ PubDate ][ Season ] = get_text(journal,\\ # JournalIssue/PubDate/Season ) # result[ PubDate ][ MedlineDate ] = get_text(journal,\\ # JournalIssue/PubDate/MedlineDate ) # MeshHeading - Zero or More Occurrences headings = article . findall( .//MeshHeading ) result[ MeshHeadingList ] = [] pmid2mesh[ MeshHeadingList ] = [] if headings: for heading in headings: descriptor_names = heading . findall( DescriptorName ) qualifier_names = heading . findall( QualifierName ) if descriptor_names: for descriptor_name in descriptor_names: result[ MeshHeadingList ] . append(descriptor_name . text) pmid2mesh[ MeshHeadingList ] . append(descriptor_name . text) if qualifier_names: for qualifier_name in qualifier_names: result[ MeshHeadingList ] . append(qualifier_name . text) pmid2mesh[ MeshHeadingList ] . append(qualifier_name . text) mesh_count = len (result[ MeshHeadingList ]) if mesh_count in mesh_statistics: mesh_statistics[mesh_count] += 1 else : mesh_statistics[mesh_count] = 1 # Dump to pubmed json file json . dump(result, pubmed_output_file) pubmed_output_file . write( \\n ) # Dump to pmid2mesh json file json . dump(pmid2mesh, pmid2mesh_output_file) pmid2mesh_output_file . write( \\n ) print ( Finish parsing %s , totally %d articles parsed. Total time: %f s \\ % (file, count, time . time() - t1)) print ( %d acticles no abstracts % (noabs)) sys . stdout . flush() f . close() Setting up directories in parsing loop def parse_dir (source_dir, pubmed_output_file,pmid2mesh_output_file): if os . path . isdir(source_dir): for file in os . listdir(source_dir): if re . search( r ^pubmed18n\\d\\d\\d\\d.xml$ , file) is not None : try : parse_pubmed_file(os . path . join(source_dir, file),\\ pubmed_output_file, pmid2mesh_output_file) except : print ( XXXX Unexpected error happended when parsing %s XXXX % file) print (traceback . print_exc()) sys . stdout . flush() Run the Parsing Pipeline t1 = time . time() pubmed_output_file_path = os . path . join(DATA_DIR, data/pubmed.json ) pmid2mesh_output_file_path = os . path . join(DATA_DIR, pmid2mesh/pmid2mesh_from_parsing.json ) pubmed_output_file = open (pubmed_output_file_path, w ) pmid2mesh_output_file = open (pmid2mesh_output_file_path, w ) parse_dir(os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/baseline ),\\ pubmed_output_file, pmid2mesh_output_file) parse_dir(os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/updatefiles ),\\ pubmed_output_file, pmid2mesh_output_file) pubmed_output_file . close() pmid2mesh_output_file . close() mesh_file = open (os . path . join(DATA_DIR, data/mesh_statistics.json ), w ) json . dump(mesh_statistics, mesh_file) mesh_file . close() print ( ==== Parsing finished, results dumped to %s ==== % pubmed_output_file_path) print ( ==== TOTAL TIME: %f s ==== % (time . time() - t1)) MeSH to PMID Mapping inputFilePath = data/pubmed.json meshFilePath = mesh2pmid/ mesh2pmid_output_file = open (meshFilePath + mesh2pmid.json , w ) mesh2pmid = dict () with open (inputFilePath, r ) as fin: start = time . time() k = 0 for line in fin: ## each line is single document try : k = k + 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ pmid ] = paperInfo . get( PMID , -1 ) #update MeSH Heading data_dict[ mesh_heading ] = . join(paperInfo[ MeshHeadingList ]) # collect Mesh2PMID if data_dict[ pmid ] != -1 : for mesh in paperInfo[ MeshHeadingList ]: if mesh not in mesh2pmid: mesh2pmid[mesh] = [] mesh2pmid[mesh] . append(data_dict[ pmid ]) if k % 500000 == 0 : print (k, done! ) #break except : print ( XXXX Unexpected Error happened at line: XXXX ) # Dumping rest papers for key,value in mesh2pmid . items(): json . dump({key:value}, mesh2pmid_output_file) mesh2pmid_output_file . write( \\n ) mesh2pmid = dict () end = time . time() print ( Finish Total escaped time %s (seconds) % (end - start) )","title":"Parsing Pipeline"},{"location":"caseolap/Parsing/#data-parsingn-pipeline","text":"This pipeline will parse the extracted data and convert it into data structures compatible with the CaseOLAP pipeline. Installation of required python package : Install and import lxml, itertools and json libraries into the current python environment. import re import itertools import json import sys import os import time import traceback from lxml import etree","title":"Data Parsingn Pipeline"},{"location":"caseolap/Parsing/#set-up-output-data-dir","text":"DATA_DIR = ./ MeSH statistics dictionary mesh_statistics = {} Data parsing strategy : The extracted data is an XML file, and text data is embedded in the tree structure of XML document. The following are steps for parsing data: - Implement the etree functionality in lxml module to dig into the tree structure of XML document. - The separate components e.g. PMID, authors, abstract , MeSH etc. of the data is obtained by using tags representing these components. - Implement the chain functionality in itertools to creates an iterator that returns elements from the iterables which was obtained by implementing etree functionality in lxml . # Search the tag in the xml element # Return tag s text if tag exists, return empty string if doesn t def get_text (element, tag): e = element . find(tag) if e is not None : return e . text else : return # !ELEMENT AuthorList (Author+) # !ELEMENT Author (((LastName, ForeName?, Initials?, Suffix?)\\ # | CollectiveName), Identifier*, AffiliationInfo*) def parse_author (authors): result = [] for author in authors: item = {} item[ LastName ] = get_text(author, LastName ) item[ ForeName ] = get_text(author, ForeName ) item[ Initials ] = get_text(author, Initials ) item[ Suffix ] = get_text(author, Suffix ) item[ CollectiveName ] = get_text(author, CollectiveName ) result . append(item) return result Creation of dictionary of parsed data : A python dictionary is created with all the components as key-value pair. This JSON-like data structure makes it compatible for indexing and searching in Elasticsearch which is described in step 3 of protocol. Creation of MeSH to PMID mapping : During the creation of dictionary of parsed data, MeSH to PMID mapping table can also be created. This mapping is used to create Text-cube(in step 4) document structure as an requirement of CaseOLAP algorithm. def parse_pubmed_file (file, pubmed_output_file, pmid2mesh_output_file): print ( Start parsing %s % file) sys . stdout . flush() t1 = time . time() f = open (file, r ) tree = etree . parse(f) articles = itertools . chain(tree . findall( PubmedArticle ),\\ tree . findall( BookDocument )) count = 0 noabs = 0 for article in articles: count += 1 result = {} pmid2mesh = {} # PMID - Exactly One Occurrance result[ PMID ] = get_text(article, .//PMID ) pmid2mesh[ PMID ] = get_text(article, .//PMID ) # # Article title - Zero or One Occurrences # result[ ArticleTitle ] = get_text(article, .//ArticleTitle ) # Abstract - Zero or One Occurrences abstractList = article . find( .//Abstract ) if abstractList != None : try : abstract = \\n . join([line . text for line in abstractList . \\ findall( AbstractText )]) result[ Abstract ] = abstract except : result[ Abstract ] = noabs += 1 else : result[ Abstract ] = noabs += 1 # # Author List - Zero or More Occurrences # authors = article.findall( .//Author ) # result[ AuthorList ] = parse_author(authors) # # Journal - Exactly One Occurrance # journal = article.find( .//Journal ) # result[ Journal ] = get_text(journal, Title ) result[ PubDate ] = {} result[ PubDate ][ Year ] = get_text(journal,\\ JournalIssue/PubDate/Year ) # result[ PubDate ][ Month ] = get_text(journal,\\ # JournalIssue/PubDate/Month ) # result[ PubDate ][ Day ] = get_text(journal,\\ # JournalIssue/PubDate/Day ) # result[ PubDate ][ Season ] = get_text(journal,\\ # JournalIssue/PubDate/Season ) # result[ PubDate ][ MedlineDate ] = get_text(journal,\\ # JournalIssue/PubDate/MedlineDate ) # MeshHeading - Zero or More Occurrences headings = article . findall( .//MeshHeading ) result[ MeshHeadingList ] = [] pmid2mesh[ MeshHeadingList ] = [] if headings: for heading in headings: descriptor_names = heading . findall( DescriptorName ) qualifier_names = heading . findall( QualifierName ) if descriptor_names: for descriptor_name in descriptor_names: result[ MeshHeadingList ] . append(descriptor_name . text) pmid2mesh[ MeshHeadingList ] . append(descriptor_name . text) if qualifier_names: for qualifier_name in qualifier_names: result[ MeshHeadingList ] . append(qualifier_name . text) pmid2mesh[ MeshHeadingList ] . append(qualifier_name . text) mesh_count = len (result[ MeshHeadingList ]) if mesh_count in mesh_statistics: mesh_statistics[mesh_count] += 1 else : mesh_statistics[mesh_count] = 1 # Dump to pubmed json file json . dump(result, pubmed_output_file) pubmed_output_file . write( \\n ) # Dump to pmid2mesh json file json . dump(pmid2mesh, pmid2mesh_output_file) pmid2mesh_output_file . write( \\n ) print ( Finish parsing %s , totally %d articles parsed. Total time: %f s \\ % (file, count, time . time() - t1)) print ( %d acticles no abstracts % (noabs)) sys . stdout . flush() f . close() Setting up directories in parsing loop def parse_dir (source_dir, pubmed_output_file,pmid2mesh_output_file): if os . path . isdir(source_dir): for file in os . listdir(source_dir): if re . search( r ^pubmed18n\\d\\d\\d\\d.xml$ , file) is not None : try : parse_pubmed_file(os . path . join(source_dir, file),\\ pubmed_output_file, pmid2mesh_output_file) except : print ( XXXX Unexpected error happended when parsing %s XXXX % file) print (traceback . print_exc()) sys . stdout . flush() Run the Parsing Pipeline t1 = time . time() pubmed_output_file_path = os . path . join(DATA_DIR, data/pubmed.json ) pmid2mesh_output_file_path = os . path . join(DATA_DIR, pmid2mesh/pmid2mesh_from_parsing.json ) pubmed_output_file = open (pubmed_output_file_path, w ) pmid2mesh_output_file = open (pmid2mesh_output_file_path, w ) parse_dir(os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/baseline ),\\ pubmed_output_file, pmid2mesh_output_file) parse_dir(os . path . join(DATA_DIR, ftp.ncbi.nlm.nih.gov/pubmed/updatefiles ),\\ pubmed_output_file, pmid2mesh_output_file) pubmed_output_file . close() pmid2mesh_output_file . close() mesh_file = open (os . path . join(DATA_DIR, data/mesh_statistics.json ), w ) json . dump(mesh_statistics, mesh_file) mesh_file . close() print ( ==== Parsing finished, results dumped to %s ==== % pubmed_output_file_path) print ( ==== TOTAL TIME: %f s ==== % (time . time() - t1))","title":"Set up output data dir"},{"location":"caseolap/Parsing/#mesh-to-pmid-mapping","text":"inputFilePath = data/pubmed.json meshFilePath = mesh2pmid/ mesh2pmid_output_file = open (meshFilePath + mesh2pmid.json , w ) mesh2pmid = dict () with open (inputFilePath, r ) as fin: start = time . time() k = 0 for line in fin: ## each line is single document try : k = k + 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ pmid ] = paperInfo . get( PMID , -1 ) #update MeSH Heading data_dict[ mesh_heading ] = . join(paperInfo[ MeshHeadingList ]) # collect Mesh2PMID if data_dict[ pmid ] != -1 : for mesh in paperInfo[ MeshHeadingList ]: if mesh not in mesh2pmid: mesh2pmid[mesh] = [] mesh2pmid[mesh] . append(data_dict[ pmid ]) if k % 500000 == 0 : print (k, done! ) #break except : print ( XXXX Unexpected Error happened at line: XXXX ) # Dumping rest papers for key,value in mesh2pmid . items(): json . dump({key:value}, mesh2pmid_output_file) mesh2pmid_output_file . write( \\n ) mesh2pmid = dict () end = time . time() print ( Finish Total escaped time %s (seconds) % (end - start) )","title":"MeSH to PMID Mapping"},{"location":"caseolap/TextCube/","text":"Text-cube creation Text-cube creation is an intelligent data engineering step in CaseOLAP which outputs a functional document structure with dimensions and cells informed by user provided document metadata. Each cell within the text-cube corresponds to a subset of documents. Following are the steps to create a text-cube. Selection of user defined categories : User selects the MeSH descriptors associated with the defined categories. Using those MeSH descriptors, cells of the documents are prepared. MeSH to PMID mapping prepared at step 2.4 is used to populate user defined cells in text-cube. Implementation of MeSH descriptors : U.S. National Library of Medicine provides the Medical Subject Heading (MeSH) in the hierarchical tree(data structure) with node ids. This permits searching for publications at varying levels of specificity. With given set of root node ids, one can select all documents for a specific cell by collecting all descendant nodes. Text-cube metadata preparation : A collection of user provided metadata(cell name, associated MeSH, PMID etc) representing each text document in the cell is prepared. There could be the significant number of documents falling under two or more cells. A dimensional hierarchy is implemented to organize the text-cube, providing each cell with a specific cell context (e.g., a parent cell, child cell, or sibling cell). Following are the steps to prepare cell-document metadata preparation: - provide the name of the cell, - make a list of document id (PMID) within each cell, - count the number of documents in each cell. Import required libraries import json import sys import time Set up input and out file address input_file_meshtree = input/mtrees2018.bin # MeSH Tree input_file_mesh2pmid = mesh2pmid.json # MeSH to PMID mapping input_file_input_cat = input/categories.txt # file containing MeSH root nodes for each category output_file_pmid_and_cat = pmid_and_cat.json # file containing pmid to cell mapping concerned_cat = [] Collection of all decendent MeSH nods in MeSH tree Find corresponding MeSH Terms for each category. with open (input_file_input_cat, r ) as f_in_input_cat: for line in f_in_input_cat: concerned_cat . append(line . strip() . split()) num_cat = len (concerned_cat) term_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_meshtree, r ) as f_in_meshtree: for line in f_in_meshtree: term_tree = line . strip() . split( ; ) cur_term = term_tree[ 0 ] cur_tree = term_tree[ 1 ] for i in range (num_cat): for cur_cat_tree in concerned_cat[i]: if cur_cat_tree in cur_tree: term_set_per_cat[i] . add(cur_term) Application of Mesh to PMID mapping to find documents for each cell Find corresponding papers for each category. pmid_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_mesh2pmid, r ) as f_in: start = time . time() k = 0 for line in f_in: mesh2pmid = {} Info = json . loads(line . strip()) for key,value in Info . items(): mesh2pmid . update({key:value}) k = k + 1 if k % 1000 == 0 : print (k, done! ) #break for i in range (num_cat): for cur_term in term_set_per_cat[i]: if cur_term == key: pmid_set_per_cat[i] = pmid_set_per_cat[i] \\ | set (mesh2pmid[cur_term]) Creation of PMID to Cell mapping Serialize papers pmid_and_cat = [] for i in range (num_cat): for cur_pmid in pmid_set_per_cat[i]: pmid_and_cat . append([cur_pmid, i]) with open (output_file_pmid_and_cat, w ) as f_out: json . dump(pmid_and_cat, f_out)","title":"Text-Cube Creation"},{"location":"caseolap/TextCube/#text-cube-creation","text":"Text-cube creation is an intelligent data engineering step in CaseOLAP which outputs a functional document structure with dimensions and cells informed by user provided document metadata. Each cell within the text-cube corresponds to a subset of documents. Following are the steps to create a text-cube. Selection of user defined categories : User selects the MeSH descriptors associated with the defined categories. Using those MeSH descriptors, cells of the documents are prepared. MeSH to PMID mapping prepared at step 2.4 is used to populate user defined cells in text-cube. Implementation of MeSH descriptors : U.S. National Library of Medicine provides the Medical Subject Heading (MeSH) in the hierarchical tree(data structure) with node ids. This permits searching for publications at varying levels of specificity. With given set of root node ids, one can select all documents for a specific cell by collecting all descendant nodes. Text-cube metadata preparation : A collection of user provided metadata(cell name, associated MeSH, PMID etc) representing each text document in the cell is prepared. There could be the significant number of documents falling under two or more cells. A dimensional hierarchy is implemented to organize the text-cube, providing each cell with a specific cell context (e.g., a parent cell, child cell, or sibling cell). Following are the steps to prepare cell-document metadata preparation: - provide the name of the cell, - make a list of document id (PMID) within each cell, - count the number of documents in each cell.","title":"Text-cube creation"},{"location":"caseolap/TextCube/#import-required-libraries","text":"import json import sys import time Set up input and out file address input_file_meshtree = input/mtrees2018.bin # MeSH Tree input_file_mesh2pmid = mesh2pmid.json # MeSH to PMID mapping input_file_input_cat = input/categories.txt # file containing MeSH root nodes for each category output_file_pmid_and_cat = pmid_and_cat.json # file containing pmid to cell mapping concerned_cat = []","title":"Import required libraries"},{"location":"caseolap/TextCube/#collection-of-all-decendent-mesh-nods-in-mesh-tree","text":"Find corresponding MeSH Terms for each category. with open (input_file_input_cat, r ) as f_in_input_cat: for line in f_in_input_cat: concerned_cat . append(line . strip() . split()) num_cat = len (concerned_cat) term_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_meshtree, r ) as f_in_meshtree: for line in f_in_meshtree: term_tree = line . strip() . split( ; ) cur_term = term_tree[ 0 ] cur_tree = term_tree[ 1 ] for i in range (num_cat): for cur_cat_tree in concerned_cat[i]: if cur_cat_tree in cur_tree: term_set_per_cat[i] . add(cur_term)","title":"Collection of all decendent MeSH nods in MeSH tree"},{"location":"caseolap/TextCube/#application-of-mesh-to-pmid-mapping-to-find-documents-for-each-cell","text":"Find corresponding papers for each category. pmid_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_mesh2pmid, r ) as f_in: start = time . time() k = 0 for line in f_in: mesh2pmid = {} Info = json . loads(line . strip()) for key,value in Info . items(): mesh2pmid . update({key:value}) k = k + 1 if k % 1000 == 0 : print (k, done! ) #break for i in range (num_cat): for cur_term in term_set_per_cat[i]: if cur_term == key: pmid_set_per_cat[i] = pmid_set_per_cat[i] \\ | set (mesh2pmid[cur_term])","title":"Application of Mesh to PMID mapping to find documents for each cell"},{"location":"caseolap/TextCube/#creation-of-pmid-to-cell-mapping","text":"Serialize papers pmid_and_cat = [] for i in range (num_cat): for cur_pmid in pmid_set_per_cat[i]: pmid_and_cat . append([cur_pmid, i]) with open (output_file_pmid_and_cat, w ) as f_out: json . dump(pmid_and_cat, f_out)","title":"Creation of PMID to Cell mapping"},{"location":"caseolap/workflow/","text":"Cloud Articture and Workflow The detail of the workflow is shown in the figure below: Image source: www.jove.com Figure : Platform architecture and workflow","title":"CaseOLAP Workflow"},{"location":"caseolap/workflow/#cloud-articture-and-workflow","text":"The detail of the workflow is shown in the figure below: Image source: www.jove.com Figure : Platform architecture and workflow","title":"Cloud Articture and Workflow"},{"location":"drug/drugbank/","text":"Parsing Drugs Data This notebook Explains how to parse Drug Data obtained from Drub Bank. The DrugBank database is a unique bioinformatics and cheminformatics resource that combines detailed drug data with comprehensive drug target information. Source import re import itertools import json import sys import os import time import traceback from lxml import etree def get_text (element, tag): e = element . find(tag) if e is not None : return e . text else : return database = open ( ./data/fulldatabase.xml , r ) import xml.etree.ElementTree as ET tree = ET . parse(database) root = tree . getroot() k = 0 f = open ( alldrugs.txt , w ) Data = [] sdata = [] name = None for drug in root: k = k + 1 name = drug . find( {http://www.drugbank.ca}name ) if name is not None : d_name = name . text line = name . text state = drug . find( {http://www.drugbank.ca}state ) if state is not None : d_state = state . text description = drug . find( {http://www.drugbank.ca}description ) if description is not None : d_description = description . text indication = drug . find( {http://www.drugbank.ca}indication ) if indication is not None : d_indication = indication . text #---------dosages -------- dosages = drug . find( {http://www.drugbank.ca}dosages ) D = [] for dosage in dosages: d = {} for item,n in zip (dosage,[ from , route , strength ]): d . update({n:item . text}) D . append(d) #-----------------Targets ----- targets = drug . find( {http://www.drugbank.ca}targets ) T = [] for t in targets: T . append(t . text) #----------pathways ------ pathways = drug . find( {http://www.drugbank.ca}pathways ) P = [] for t in pathways: P . append(t . text) #----------synonyms ---------- synonyms = drug . find( {http://www.drugbank.ca}synonyms ) S = [] for t in synonyms: S . append(t . text) if len (t . text) 3 : line = line + | + t . text sdata . append({ name :d_name,\\ synonyms :S}) Data . append({ name :d_name,\\ description :d_description,\\ state : d_state,\\ indication : d_indication,\\ dosages : D,\\ synonyms :S}) f . write(line) f . write( \\n ) import json as json with open ( Drugs.json , w ) as f: json . dump(Data,f) with open ( syn.json , w ) as f: json . dump(Data,f) import pandas as pd DF = pd . DataFrame(sdata) DF . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name synonyms 0 Lepirudin [Hirudin variant-1, Lepirudin recombinant] 1 Cetuximab [Cetuximab, C\u00e9tuximab, Cetuximabum, Immunoglob... 2 Dornase alfa [Deoxyribonuclease (human clone 18-1 protein m... 3 Denileukin diftitox [Denileukin, Interleukin-2/diptheria toxin fus... 4 Etanercept [Etanercept-szzs, RHU TNFR:FC, RHU-TNFR:FC, TN... DF . to_csv( syn.csv ) DF . shape (11922, 2) import pandas as pd import json as json with open ( Drugs.json , r ) as ff: Data = json . load(ff) import pandas as pd DF = pd . DataFrame(Data) DF . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } description dosages indication name state synonyms 0 Lepirudin is identical to natural hirudin exce... [{'from': 'Injection, solution, concentrate', ... For the treatment of heparin-induced thrombocy... Lepirudin liquid [Hirudin variant-1, Lepirudin recombinant] 1 Cetuximab is an epidermal growth factor recept... [{'from': 'Injection, solution', 'route': 'Int... Cetuximab, used in combination with irinotecan... Cetuximab liquid [Cetuximab, C\u00e9tuximab, Cetuximabum, Immunoglob... 2 Dornase alfa is a biosynthetic form of human d... [{'from': 'Solution', 'route': 'Respiratory (i... Used as adjunct therapy in the treatment of cy... Dornase alfa liquid [Deoxyribonuclease (human clone 18-1 protein m... 3 A recombinant DNA-derived cytotoxic protein co... [{'from': 'Injection, solution', 'route': 'Int... For treatment of cutaneous T-cell lymphoma Denileukin diftitox liquid [Denileukin, Interleukin-2/diptheria toxin fus... 4 Dimeric fusion protein consisting of the extra... [{'from': 'Injection, powder, for solution', '... Etanercept is indicated for the treatment of m... Etanercept liquid [Etanercept-szzs, RHU TNFR:FC, RHU-TNFR:FC, TN... DF = DF . set_index( name ) DF . to_csv( Drugsall.csv )","title":"DrugBank"},{"location":"drug/drugbank/#parsing-drugs-data","text":"This notebook Explains how to parse Drug Data obtained from Drub Bank. The DrugBank database is a unique bioinformatics and cheminformatics resource that combines detailed drug data with comprehensive drug target information. Source import re import itertools import json import sys import os import time import traceback from lxml import etree def get_text (element, tag): e = element . find(tag) if e is not None : return e . text else : return database = open ( ./data/fulldatabase.xml , r ) import xml.etree.ElementTree as ET tree = ET . parse(database) root = tree . getroot() k = 0 f = open ( alldrugs.txt , w ) Data = [] sdata = [] name = None for drug in root: k = k + 1 name = drug . find( {http://www.drugbank.ca}name ) if name is not None : d_name = name . text line = name . text state = drug . find( {http://www.drugbank.ca}state ) if state is not None : d_state = state . text description = drug . find( {http://www.drugbank.ca}description ) if description is not None : d_description = description . text indication = drug . find( {http://www.drugbank.ca}indication ) if indication is not None : d_indication = indication . text #---------dosages -------- dosages = drug . find( {http://www.drugbank.ca}dosages ) D = [] for dosage in dosages: d = {} for item,n in zip (dosage,[ from , route , strength ]): d . update({n:item . text}) D . append(d) #-----------------Targets ----- targets = drug . find( {http://www.drugbank.ca}targets ) T = [] for t in targets: T . append(t . text) #----------pathways ------ pathways = drug . find( {http://www.drugbank.ca}pathways ) P = [] for t in pathways: P . append(t . text) #----------synonyms ---------- synonyms = drug . find( {http://www.drugbank.ca}synonyms ) S = [] for t in synonyms: S . append(t . text) if len (t . text) 3 : line = line + | + t . text sdata . append({ name :d_name,\\ synonyms :S}) Data . append({ name :d_name,\\ description :d_description,\\ state : d_state,\\ indication : d_indication,\\ dosages : D,\\ synonyms :S}) f . write(line) f . write( \\n ) import json as json with open ( Drugs.json , w ) as f: json . dump(Data,f) with open ( syn.json , w ) as f: json . dump(Data,f) import pandas as pd DF = pd . DataFrame(sdata) DF . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name synonyms 0 Lepirudin [Hirudin variant-1, Lepirudin recombinant] 1 Cetuximab [Cetuximab, C\u00e9tuximab, Cetuximabum, Immunoglob... 2 Dornase alfa [Deoxyribonuclease (human clone 18-1 protein m... 3 Denileukin diftitox [Denileukin, Interleukin-2/diptheria toxin fus... 4 Etanercept [Etanercept-szzs, RHU TNFR:FC, RHU-TNFR:FC, TN... DF . to_csv( syn.csv ) DF . shape (11922, 2) import pandas as pd import json as json with open ( Drugs.json , r ) as ff: Data = json . load(ff) import pandas as pd DF = pd . DataFrame(Data) DF . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } description dosages indication name state synonyms 0 Lepirudin is identical to natural hirudin exce... [{'from': 'Injection, solution, concentrate', ... For the treatment of heparin-induced thrombocy... Lepirudin liquid [Hirudin variant-1, Lepirudin recombinant] 1 Cetuximab is an epidermal growth factor recept... [{'from': 'Injection, solution', 'route': 'Int... Cetuximab, used in combination with irinotecan... Cetuximab liquid [Cetuximab, C\u00e9tuximab, Cetuximabum, Immunoglob... 2 Dornase alfa is a biosynthetic form of human d... [{'from': 'Solution', 'route': 'Respiratory (i... Used as adjunct therapy in the treatment of cy... Dornase alfa liquid [Deoxyribonuclease (human clone 18-1 protein m... 3 A recombinant DNA-derived cytotoxic protein co... [{'from': 'Injection, solution', 'route': 'Int... For treatment of cutaneous T-cell lymphoma Denileukin diftitox liquid [Denileukin, Interleukin-2/diptheria toxin fus... 4 Dimeric fusion protein consisting of the extra... [{'from': 'Injection, powder, for solution', '... Etanercept is indicated for the treatment of m... Etanercept liquid [Etanercept-szzs, RHU TNFR:FC, RHU-TNFR:FC, TN... DF = DF . set_index( name ) DF . to_csv( Drugsall.csv )","title":"Parsing Drugs Data"},{"location":"hcluster/hierarchical/","text":"Hierarchical Clustering (with cutoff) import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns % matplotlib inline import json CVD = [ CVA , IHD , CM , ARR , VD , CHD ] clrs = [ navy , green , firebrick ,\\ mediumslateblue , darkgoldenrod , deepskyblue ] def rearrang (olddf): col = [ CVA , IHD , CM , ARR , VD , CHD ] newdf = pd . DataFrame() for t in col: newdf[t] = olddf[t] return newdf with open ( ../../1.DATA/uniprot/protein2uniprot.json , r ) as f: protein2uniprot = json . load(f) print ( all data: , len (protein2uniprot)) all data: 2869 data = pd . read_csv( ../../1.DATA/score/score.csv ) data = data . set_index( Protein ) ndf = rearrang(data) ndf . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD Protein small_ubiquitin-related_modifier_1 0.041144 0.012216 0.078019 0.000000 0.000000 0.024314 metalloproteinase_inhibitor_4 0.042887 0.054740 0.095265 0.045032 0.034227 0.005072 ndf . shape (2869, 6) ndata = ndf . copy(deep = True ) ndf . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD count 2869.000000 2869.000000 2869.000000 2869.000000 2869.000000 2869.000000 mean 0.040107 0.034860 0.026862 0.010698 0.007240 0.011746 std 0.060346 0.050428 0.038751 0.028798 0.024030 0.032627 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 50% 0.022209 0.018526 0.013534 0.000000 0.000000 0.000000 75% 0.057229 0.045417 0.037451 0.007516 0.000000 0.009734 max 0.686945 0.419997 0.343774 0.305472 0.365424 0.595544 Set Cutoff solid_cutoff = [ 0.15 , 0.15 , 0.15 , 0.15 , 0.15 , 0.15 ] mdata = ndata . copy(deep = True ) dis = [ CVA , IHD , CM , ARR , VD , CHD ] idx = list (mdata . index) data_dict = [] for item in idx: data = mdata . loc[item,:] lst = [data[ 0 ],data[ 1 ],data[ 2 ],data[ 3 ],data[ 4 ],data[ 5 ]] m = max (lst) for e,cut in zip (lst,solid_cutoff): if e == m: if e cut: data_dict . append({ protein :item,\\ CVA :data[ 0 ],\\ IHD :data[ 1 ],\\ CM :data[ 2 ],\\ ARR : data[ 3 ],\\ VD :data[ 4 ],\\ CHD :data[ 5 ]}) cdata = pd . DataFrame(data_dict) cdata . index = cdata[ protein ] cdata = cdata . drop( protein , axis = 1 ) cdata . shape (313, 6) Clustering size = ( 50 , 50 ) g = sns . clustermap(cdata . T . corr(),\\ figsize = size,\\ cmap = YlGnBu ,\\ metric = seuclidean ) g . savefig( cutoff/cluster-cutoff-solid.pdf , format = pdf , dpi = 400 ) indx = g . dendrogram_row . reordered_ind protein_cluster = [] for num in indx: for i,ndx in enumerate (cdata . index): if num == i: #print(i+1,ndx) protein_cluster . append({ id :i, protein : ndx,\\ ARR : list (cdata . loc[ndx,:])[ 0 ],\\ CHD : list (cdata . loc[ndx,:])[ 1 ],\\ CM : list (cdata . loc[ndx,:])[ 2 ],\\ CVA : list (cdata . loc[ndx,:])[ 3 ],\\ IHD : list (cdata . loc[ndx,:])[ 4 ],\\ VD : list (cdata . loc[ndx,:])[ 5 ]}) protein_cluster_df = pd . DataFrame(protein_cluster) protein_cluster_df = protein_cluster_df . set_index( protein ) protein_cluster_df = rearrang(protein_cluster_df) protein_cluster_df . head( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD protein methylenetetrahydrofolate_reductase 0.222450 0.155409 0.000390 0.005741 0.005021 0.100177 coagulation_factor_xii 0.167927 0.052323 0.000000 0.004614 0.000000 0.028020 matrix_metalloproteinase-9 0.245860 0.132204 0.072137 0.043005 0.061532 0.022252 Heatmap protein_cluster_df . max() CVA 0.686945 IHD 0.419997 CM 0.343774 ARR 0.305472 VD 0.365424 CHD 0.595544 dtype: float64 plt . figure(figsize = [ 22 , 22 ]) sns . heatmap(protein_cluster_df,\\ cmap = YlGnBu ,\\ #cmap = sns.cubehelix_palette(1000),\\ #cmap = sns.cubehelix_palette(8, start=.5, rot=-.75),\\ #cmap = sns.color_palette( Blues ),\\ yticklabels = False ,\\ vmin = 0.15 ,vmax = 0.70 ) plt . savefig( cutoff/heatmap-cutoff-solid.pdf ) Barplot protein_cluster_df . plot . barh(stacked = True ,figsize = ( 50 , 100 ),color = clrs) plt . gca() . invert_yaxis() plt . legend(fontsize = 20 ) plt . savefig( cutoff/barplot-cutoff-solid.pdf ) Final Result U = [] index = list (protein_cluster_df . index) for p in index: u = protein2uniprot[p] U . append(u) protein_cluster_df[ uniprot ] = U protein_cluster_df . to_csv( cutoff/cluster-list-cutoff-solid.csv )","title":"Hierarchical Clustering"},{"location":"hcluster/hierarchical/#hierarchical-clustering-with-cutoff","text":"import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns % matplotlib inline import json CVD = [ CVA , IHD , CM , ARR , VD , CHD ] clrs = [ navy , green , firebrick ,\\ mediumslateblue , darkgoldenrod , deepskyblue ] def rearrang (olddf): col = [ CVA , IHD , CM , ARR , VD , CHD ] newdf = pd . DataFrame() for t in col: newdf[t] = olddf[t] return newdf with open ( ../../1.DATA/uniprot/protein2uniprot.json , r ) as f: protein2uniprot = json . load(f) print ( all data: , len (protein2uniprot)) all data: 2869 data = pd . read_csv( ../../1.DATA/score/score.csv ) data = data . set_index( Protein ) ndf = rearrang(data) ndf . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD Protein small_ubiquitin-related_modifier_1 0.041144 0.012216 0.078019 0.000000 0.000000 0.024314 metalloproteinase_inhibitor_4 0.042887 0.054740 0.095265 0.045032 0.034227 0.005072 ndf . shape (2869, 6) ndata = ndf . copy(deep = True ) ndf . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD count 2869.000000 2869.000000 2869.000000 2869.000000 2869.000000 2869.000000 mean 0.040107 0.034860 0.026862 0.010698 0.007240 0.011746 std 0.060346 0.050428 0.038751 0.028798 0.024030 0.032627 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 50% 0.022209 0.018526 0.013534 0.000000 0.000000 0.000000 75% 0.057229 0.045417 0.037451 0.007516 0.000000 0.009734 max 0.686945 0.419997 0.343774 0.305472 0.365424 0.595544","title":"Hierarchical Clustering (with cutoff)"},{"location":"hcluster/hierarchical/#set-cutoff","text":"solid_cutoff = [ 0.15 , 0.15 , 0.15 , 0.15 , 0.15 , 0.15 ] mdata = ndata . copy(deep = True ) dis = [ CVA , IHD , CM , ARR , VD , CHD ] idx = list (mdata . index) data_dict = [] for item in idx: data = mdata . loc[item,:] lst = [data[ 0 ],data[ 1 ],data[ 2 ],data[ 3 ],data[ 4 ],data[ 5 ]] m = max (lst) for e,cut in zip (lst,solid_cutoff): if e == m: if e cut: data_dict . append({ protein :item,\\ CVA :data[ 0 ],\\ IHD :data[ 1 ],\\ CM :data[ 2 ],\\ ARR : data[ 3 ],\\ VD :data[ 4 ],\\ CHD :data[ 5 ]}) cdata = pd . DataFrame(data_dict) cdata . index = cdata[ protein ] cdata = cdata . drop( protein , axis = 1 ) cdata . shape (313, 6)","title":"Set Cutoff"},{"location":"hcluster/hierarchical/#clustering","text":"size = ( 50 , 50 ) g = sns . clustermap(cdata . T . corr(),\\ figsize = size,\\ cmap = YlGnBu ,\\ metric = seuclidean ) g . savefig( cutoff/cluster-cutoff-solid.pdf , format = pdf , dpi = 400 ) indx = g . dendrogram_row . reordered_ind protein_cluster = [] for num in indx: for i,ndx in enumerate (cdata . index): if num == i: #print(i+1,ndx) protein_cluster . append({ id :i, protein : ndx,\\ ARR : list (cdata . loc[ndx,:])[ 0 ],\\ CHD : list (cdata . loc[ndx,:])[ 1 ],\\ CM : list (cdata . loc[ndx,:])[ 2 ],\\ CVA : list (cdata . loc[ndx,:])[ 3 ],\\ IHD : list (cdata . loc[ndx,:])[ 4 ],\\ VD : list (cdata . loc[ndx,:])[ 5 ]}) protein_cluster_df = pd . DataFrame(protein_cluster) protein_cluster_df = protein_cluster_df . set_index( protein ) protein_cluster_df = rearrang(protein_cluster_df) protein_cluster_df . head( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD protein methylenetetrahydrofolate_reductase 0.222450 0.155409 0.000390 0.005741 0.005021 0.100177 coagulation_factor_xii 0.167927 0.052323 0.000000 0.004614 0.000000 0.028020 matrix_metalloproteinase-9 0.245860 0.132204 0.072137 0.043005 0.061532 0.022252","title":"Clustering"},{"location":"hcluster/hierarchical/#heatmap","text":"protein_cluster_df . max() CVA 0.686945 IHD 0.419997 CM 0.343774 ARR 0.305472 VD 0.365424 CHD 0.595544 dtype: float64 plt . figure(figsize = [ 22 , 22 ]) sns . heatmap(protein_cluster_df,\\ cmap = YlGnBu ,\\ #cmap = sns.cubehelix_palette(1000),\\ #cmap = sns.cubehelix_palette(8, start=.5, rot=-.75),\\ #cmap = sns.color_palette( Blues ),\\ yticklabels = False ,\\ vmin = 0.15 ,vmax = 0.70 ) plt . savefig( cutoff/heatmap-cutoff-solid.pdf )","title":"Heatmap"},{"location":"hcluster/hierarchical/#barplot","text":"protein_cluster_df . plot . barh(stacked = True ,figsize = ( 50 , 100 ),color = clrs) plt . gca() . invert_yaxis() plt . legend(fontsize = 20 ) plt . savefig( cutoff/barplot-cutoff-solid.pdf )","title":"Barplot"},{"location":"hcluster/hierarchical/#final-result","text":"U = [] index = list (protein_cluster_df . index) for p in index: u = protein2uniprot[p] U . append(u) protein_cluster_df[ uniprot ] = U protein_cluster_df . to_csv( cutoff/cluster-list-cutoff-solid.csv )","title":"Final Result"},{"location":"icd11/icd/","text":"ICD-11 Data ICD is the foundation for the identification of health trends and statistics globally, and the international standard for reporting diseases and health conditions. It is the diagnostic classification standard for all clinical and research purposes. ICD defines the universe of diseases, disorders, injuries and other related health conditions, listed in a comprehensive, hierarchical fashion that allows for: easy storage, retrieval and analysis of health information for evidenced-based decision-making; sharing and comparing health information between hospitals, regions, settings and countries; and data comparisons in the same location across different time periods. Uses include monitoring of the incidence and prevalence of diseases, observing reimbursements and resource allocation trends, and keeping track of safety and quality guidelines. They also include the counting of deaths as well as diseases, injuries, symptoms, reasons for encounter, factors that influence health status, and external causes of disease. Source View Sample ICD 11 Tree for Cardiovascular Disease Getting Client Key for API connection By default one client Id and client secret is provided to be able to authenticate your software to use the ICD APIs. You may however, add more clients or remove some of them. (IMPORTANT!!! If a client key is removed, clients configured with this Id/secret will not be able access the API anymore) ClientId: xxxxxx ClientSecret: xxxxxxx import requests import json ClientId = xxxxxx ClientSecret = xxxxxx token_endpoint = https://icdaccessmanagement.who.int/connect/token client_id = ClientId client_secret = ClientSecret scope = icdapi_access grant_type = client_credentials Get the OAUTH2 token # set data to post payload = { client_id : client_id, client_secret : client_secret, scope : scope, grant_type : grant_type} # make request r = requests . post(token_endpoint, data = payload, verify = True ) . json() token = r[ access_token ] Access ICD API uri = https://id.who.int/icd/entity # HTTP header fields to set headers = { Authorization : Bearer + token, Accept : application/json , Accept-Language : en , API-Version : v2 } # make request r = requests . get(uri, headers = headers, verify = True ) # print the result # print (r.text) print (r . json()) { @context : http://id.who.int/icd/contexts/contextForTopLevel.json , @id : http://id.who.int/icd/entity , title : { @language : en , @value : International Classification of Diseases 11th Revision }, releaseId : 2019-04 , releaseDate : 2019-04-06 , child : [ http://id.who.int/icd/entity/1435254666 , http://id.who.int/icd/entity/1630407678 , http://id.who.int/icd/entity/1766440644 , http://id.who.int/icd/entity/1954798891 , http://id.who.int/icd/entity/21500692 , http://id.who.int/icd/entity/334423054 , http://id.who.int/icd/entity/274880002 , http://id.who.int/icd/entity/1296093776 , http://id.who.int/icd/entity/868865918 , http://id.who.int/icd/entity/1218729044 , http://id.who.int/icd/entity/426429380 , http://id.who.int/icd/entity/197934298 , http://id.who.int/icd/entity/1256772020 , http://id.who.int/icd/entity/1639304259 , http://id.who.int/icd/entity/1473673350 , http://id.who.int/icd/entity/30659757 , http://id.who.int/icd/entity/577470983 , http://id.who.int/icd/entity/714000734 , http://id.who.int/icd/entity/1306203631 , http://id.who.int/icd/entity/223744320 , http://id.who.int/icd/entity/1843895818 , http://id.who.int/icd/entity/435227771 , http://id.who.int/icd/entity/850137482 , http://id.who.int/icd/entity/1249056269 , http://id.who.int/icd/entity/1596590595 , http://id.who.int/icd/entity/718687701 , http://id.who.int/icd/entity/231358748 , http://id.who.int/icd/entity/979408586 , http://id.who.int/icd/entity/1801349023 ], browserUrl : NA } D = r . json() ROOTS = [] for item in D[ child ]: ROOTS . append(item . split( / )[ - 1 ]) with open ( ROOTS.json , w ) as ff: json . dump(ROOTS,ff) #ROOTS MMS Data uri = https://id.who.int/icd/entity/1435254666 # HTTP header fields to set headers = { Authorization : Bearer + token, Accept : application/json , Accept-Language : en , API-Version : v2 } # make request r = requests . get(uri, headers = headers, verify = True ) # print the result # print (r.text) print (r . json()) { @context : http://id.who.int/icd/contexts/contextForFoundationEntity.json , @id : http://id.who.int/icd/entity/1435254666 , parent : [ http://id.who.int/icd/entity ], child : [ http://id.who.int/icd/entity/588616678 , http://id.who.int/icd/entity/1904876434 , http://id.who.int/icd/entity/979278646 , http://id.who.int/icd/entity/1539889147 , http://id.who.int/icd/entity/1412960686 , http://id.who.int/icd/entity/1935092859 , http://id.who.int/icd/entity/487269828 , http://id.who.int/icd/entity/1000704511 , http://id.who.int/icd/entity/1104303944 , http://id.who.int/icd/entity/1585949804 , http://id.who.int/icd/entity/1959883044 , http://id.who.int/icd/entity/921595235 , http://id.who.int/icd/entity/1251496839 , http://id.who.int/icd/entity/1136802325 , http://id.who.int/icd/entity/145723401 , http://id.who.int/icd/entity/985510409 , http://id.who.int/icd/entity/293771399 , http://id.who.int/icd/entity/5960175 , http://id.who.int/icd/entity/911707612 , http://id.who.int/icd/entity/1965146397 , http://id.who.int/icd/entity/142052508 , http://id.who.int/icd/entity/1760597414 , http://id.who.int/icd/entity/458687859 , http://id.who.int/icd/entity/2143513892 ], browserUrl : NA , title : { @language : en , @value : Certain infectious or parasitic diseases }, synonym : [{ label : { @language : en , @value : infection NOS }}, { label : { @language : en , @value : infection of unspecified organism and unspecified site }}, { label : { @language : en , @value : infectious disease NOS }}, { label : { @language : en , @value : infection unknown }}, { label : { @language : en , @value : infection process NOS }}], definition : { @language : en , @value : This chapter includes certain conditions caused by a pathogenic organism or microorganism, such as a bacterium, virus, parasite, or fungus. }, exclusion : [{ label : { @language : en , @value : Infection arising from device, implant or graft, not elsewhere classified }, foundationReference : http://id.who.int/icd/entity/1612485599 }]} uri = http://id.who.int/icd/release/11/2019-04/mms/135352227 # HTTP header fields to set headers = { Authorization : Bearer + token, Accept : application/json , Accept-Language : en , API-Version : v2 } # make request r = requests . get(uri, headers = headers, verify = True ) # print the result # print (r.text) print (r . json()) { @context : http://id.who.int/icd/contexts/contextForLinearizationEntity.json , @id : http://id.who.int/icd/release/11/2019-04/mms/135352227 , parent : [ http://id.who.int/icd/release/11/2019-04/mms/588616678 ], child : [ http://id.who.int/icd/release/11/2019-04/mms/257068234 , http://id.who.int/icd/release/11/2019-04/mms/416025325 , http://id.who.int/icd/release/11/2019-04/mms/2080365623 , http://id.who.int/icd/release/11/2019-04/mms/344162786 , http://id.who.int/icd/release/11/2019-04/mms/250688797 , http://id.who.int/icd/release/11/2019-04/mms/1000894786 , http://id.who.int/icd/release/11/2019-04/mms/794462570 , http://id.who.int/icd/release/11/2019-04/mms/1528414070 , http://id.who.int/icd/release/11/2019-04/mms/1780040028 , http://id.who.int/icd/release/11/2019-04/mms/515117475 , http://id.who.int/icd/release/11/2019-04/mms/135352227/other , http://id.who.int/icd/release/11/2019-04/mms/135352227/unspecified ], browserUrl : https://icd.who.int/browse11/l-m/en#/http%3a%\\ 2f%2fid.who.int%2ficd%2fentity%2f135352227 , code : , source : http://id.who.int/icd/entity/135352227 , classKind : block , blockId : BlockL2-1A0 , codeRange : 1A00-1A0Z , foundationChildElsewhere : [{ label : { @language : en , @value : Abdominal actinomycosis }, foundationReference : http://id.who.int/icd/entity/2143116824 , linearizationReference : http://id.who.int/icd/release/11/2019-04/mms/2143116824 }, { label : { @language : en , @value : Listerial gastroenteritis }, foundationReference : http://id.who.int/icd/entity/974967764 , linearizationReference : http://id.who.int/icd/release/11/2019-04/mms/419706488/other }], title : { @language : en , @value : Bacterial intestinal infections }, definition : { @language : en , @value : Any condition of the intestines, caused by an infection with a bacterial source. }, exclusion : [{ label : { @language : en , @value : Bacterial foodborne intoxications }, foundationReference : http://id.who.int/icd/entity/1834648119 , linearizationReference : http://id.who.int/icd/release/11/2019-04/mms/1834648119 }]} A Sample File Parser def file_parser (file): with open (file , r ) as f: FDATA = [] for line in f: raw = json . loads(line) item = collections . defaultdict( lambda : Key Not found ) for key,value in raw . items(): item[key] = value #print(item) title = item[ title ][ @value ] ID = item[ @id ] . split( / )[ - 1 ] parents = [] if item[ parent ] != Key Not found : for pt in item[ parent ]: parents . append(pt . split( / )[ - 1 ]) childs = [] if item[ child ] != Key Not found : for ct in item[ child ]: childs . append(ct . split( / )[ - 1 ]) definition = item[ definition ] if definition != Key Not found : deff = definition[ @value ] else : deff = definition synonym = item[ synonym ] if synonym != Key Not found : syn = [] for t in synonym: syn . append(t[ @value ]) else : syn = synonym FDATA . append({ID :{ title :title,\\ synonym : syn,\\ definition :deff,\\ parents :parents,\\ childs :childs}}) return FDATA View Sample ICD 11 tree cor Cardiovascular Disease here","title":"ICD 11 Codes"},{"location":"icd11/icd/#icd-11-data","text":"ICD is the foundation for the identification of health trends and statistics globally, and the international standard for reporting diseases and health conditions. It is the diagnostic classification standard for all clinical and research purposes. ICD defines the universe of diseases, disorders, injuries and other related health conditions, listed in a comprehensive, hierarchical fashion that allows for: easy storage, retrieval and analysis of health information for evidenced-based decision-making; sharing and comparing health information between hospitals, regions, settings and countries; and data comparisons in the same location across different time periods. Uses include monitoring of the incidence and prevalence of diseases, observing reimbursements and resource allocation trends, and keeping track of safety and quality guidelines. They also include the counting of deaths as well as diseases, injuries, symptoms, reasons for encounter, factors that influence health status, and external causes of disease. Source View Sample ICD 11 Tree for Cardiovascular Disease","title":"ICD-11 Data"},{"location":"icd11/icd/#getting-client-key-for-api-connection","text":"By default one client Id and client secret is provided to be able to authenticate your software to use the ICD APIs. You may however, add more clients or remove some of them. (IMPORTANT!!! If a client key is removed, clients configured with this Id/secret will not be able access the API anymore) ClientId: xxxxxx ClientSecret: xxxxxxx import requests import json ClientId = xxxxxx ClientSecret = xxxxxx token_endpoint = https://icdaccessmanagement.who.int/connect/token client_id = ClientId client_secret = ClientSecret scope = icdapi_access grant_type = client_credentials","title":"Getting Client Key for API connection"},{"location":"icd11/icd/#get-the-oauth2-token","text":"# set data to post payload = { client_id : client_id, client_secret : client_secret, scope : scope, grant_type : grant_type} # make request r = requests . post(token_endpoint, data = payload, verify = True ) . json() token = r[ access_token ]","title":"Get the OAUTH2 token"},{"location":"icd11/icd/#access-icd-api","text":"uri = https://id.who.int/icd/entity # HTTP header fields to set headers = { Authorization : Bearer + token, Accept : application/json , Accept-Language : en , API-Version : v2 } # make request r = requests . get(uri, headers = headers, verify = True ) # print the result # print (r.text) print (r . json()) { @context : http://id.who.int/icd/contexts/contextForTopLevel.json , @id : http://id.who.int/icd/entity , title : { @language : en , @value : International Classification of Diseases 11th Revision }, releaseId : 2019-04 , releaseDate : 2019-04-06 , child : [ http://id.who.int/icd/entity/1435254666 , http://id.who.int/icd/entity/1630407678 , http://id.who.int/icd/entity/1766440644 , http://id.who.int/icd/entity/1954798891 , http://id.who.int/icd/entity/21500692 , http://id.who.int/icd/entity/334423054 , http://id.who.int/icd/entity/274880002 , http://id.who.int/icd/entity/1296093776 , http://id.who.int/icd/entity/868865918 , http://id.who.int/icd/entity/1218729044 , http://id.who.int/icd/entity/426429380 , http://id.who.int/icd/entity/197934298 , http://id.who.int/icd/entity/1256772020 , http://id.who.int/icd/entity/1639304259 , http://id.who.int/icd/entity/1473673350 , http://id.who.int/icd/entity/30659757 , http://id.who.int/icd/entity/577470983 , http://id.who.int/icd/entity/714000734 , http://id.who.int/icd/entity/1306203631 , http://id.who.int/icd/entity/223744320 , http://id.who.int/icd/entity/1843895818 , http://id.who.int/icd/entity/435227771 , http://id.who.int/icd/entity/850137482 , http://id.who.int/icd/entity/1249056269 , http://id.who.int/icd/entity/1596590595 , http://id.who.int/icd/entity/718687701 , http://id.who.int/icd/entity/231358748 , http://id.who.int/icd/entity/979408586 , http://id.who.int/icd/entity/1801349023 ], browserUrl : NA } D = r . json() ROOTS = [] for item in D[ child ]: ROOTS . append(item . split( / )[ - 1 ]) with open ( ROOTS.json , w ) as ff: json . dump(ROOTS,ff) #ROOTS","title":"Access ICD API"},{"location":"icd11/icd/#mms-data","text":"uri = https://id.who.int/icd/entity/1435254666 # HTTP header fields to set headers = { Authorization : Bearer + token, Accept : application/json , Accept-Language : en , API-Version : v2 } # make request r = requests . get(uri, headers = headers, verify = True ) # print the result # print (r.text) print (r . json()) { @context : http://id.who.int/icd/contexts/contextForFoundationEntity.json , @id : http://id.who.int/icd/entity/1435254666 , parent : [ http://id.who.int/icd/entity ], child : [ http://id.who.int/icd/entity/588616678 , http://id.who.int/icd/entity/1904876434 , http://id.who.int/icd/entity/979278646 , http://id.who.int/icd/entity/1539889147 , http://id.who.int/icd/entity/1412960686 , http://id.who.int/icd/entity/1935092859 , http://id.who.int/icd/entity/487269828 , http://id.who.int/icd/entity/1000704511 , http://id.who.int/icd/entity/1104303944 , http://id.who.int/icd/entity/1585949804 , http://id.who.int/icd/entity/1959883044 , http://id.who.int/icd/entity/921595235 , http://id.who.int/icd/entity/1251496839 , http://id.who.int/icd/entity/1136802325 , http://id.who.int/icd/entity/145723401 , http://id.who.int/icd/entity/985510409 , http://id.who.int/icd/entity/293771399 , http://id.who.int/icd/entity/5960175 , http://id.who.int/icd/entity/911707612 , http://id.who.int/icd/entity/1965146397 , http://id.who.int/icd/entity/142052508 , http://id.who.int/icd/entity/1760597414 , http://id.who.int/icd/entity/458687859 , http://id.who.int/icd/entity/2143513892 ], browserUrl : NA , title : { @language : en , @value : Certain infectious or parasitic diseases }, synonym : [{ label : { @language : en , @value : infection NOS }}, { label : { @language : en , @value : infection of unspecified organism and unspecified site }}, { label : { @language : en , @value : infectious disease NOS }}, { label : { @language : en , @value : infection unknown }}, { label : { @language : en , @value : infection process NOS }}], definition : { @language : en , @value : This chapter includes certain conditions caused by a pathogenic organism or microorganism, such as a bacterium, virus, parasite, or fungus. }, exclusion : [{ label : { @language : en , @value : Infection arising from device, implant or graft, not elsewhere classified }, foundationReference : http://id.who.int/icd/entity/1612485599 }]} uri = http://id.who.int/icd/release/11/2019-04/mms/135352227 # HTTP header fields to set headers = { Authorization : Bearer + token, Accept : application/json , Accept-Language : en , API-Version : v2 } # make request r = requests . get(uri, headers = headers, verify = True ) # print the result # print (r.text) print (r . json()) { @context : http://id.who.int/icd/contexts/contextForLinearizationEntity.json , @id : http://id.who.int/icd/release/11/2019-04/mms/135352227 , parent : [ http://id.who.int/icd/release/11/2019-04/mms/588616678 ], child : [ http://id.who.int/icd/release/11/2019-04/mms/257068234 , http://id.who.int/icd/release/11/2019-04/mms/416025325 , http://id.who.int/icd/release/11/2019-04/mms/2080365623 , http://id.who.int/icd/release/11/2019-04/mms/344162786 , http://id.who.int/icd/release/11/2019-04/mms/250688797 , http://id.who.int/icd/release/11/2019-04/mms/1000894786 , http://id.who.int/icd/release/11/2019-04/mms/794462570 , http://id.who.int/icd/release/11/2019-04/mms/1528414070 , http://id.who.int/icd/release/11/2019-04/mms/1780040028 , http://id.who.int/icd/release/11/2019-04/mms/515117475 , http://id.who.int/icd/release/11/2019-04/mms/135352227/other , http://id.who.int/icd/release/11/2019-04/mms/135352227/unspecified ], browserUrl : https://icd.who.int/browse11/l-m/en#/http%3a%\\ 2f%2fid.who.int%2ficd%2fentity%2f135352227 , code : , source : http://id.who.int/icd/entity/135352227 , classKind : block , blockId : BlockL2-1A0 , codeRange : 1A00-1A0Z , foundationChildElsewhere : [{ label : { @language : en , @value : Abdominal actinomycosis }, foundationReference : http://id.who.int/icd/entity/2143116824 , linearizationReference : http://id.who.int/icd/release/11/2019-04/mms/2143116824 }, { label : { @language : en , @value : Listerial gastroenteritis }, foundationReference : http://id.who.int/icd/entity/974967764 , linearizationReference : http://id.who.int/icd/release/11/2019-04/mms/419706488/other }], title : { @language : en , @value : Bacterial intestinal infections }, definition : { @language : en , @value : Any condition of the intestines, caused by an infection with a bacterial source. }, exclusion : [{ label : { @language : en , @value : Bacterial foodborne intoxications }, foundationReference : http://id.who.int/icd/entity/1834648119 , linearizationReference : http://id.who.int/icd/release/11/2019-04/mms/1834648119 }]}","title":"MMS Data"},{"location":"icd11/icd/#a-sample-file-parser","text":"def file_parser (file): with open (file , r ) as f: FDATA = [] for line in f: raw = json . loads(line) item = collections . defaultdict( lambda : Key Not found ) for key,value in raw . items(): item[key] = value #print(item) title = item[ title ][ @value ] ID = item[ @id ] . split( / )[ - 1 ] parents = [] if item[ parent ] != Key Not found : for pt in item[ parent ]: parents . append(pt . split( / )[ - 1 ]) childs = [] if item[ child ] != Key Not found : for ct in item[ child ]: childs . append(ct . split( / )[ - 1 ]) definition = item[ definition ] if definition != Key Not found : deff = definition[ @value ] else : deff = definition synonym = item[ synonym ] if synonym != Key Not found : syn = [] for t in synonym: syn . append(t[ @value ]) else : syn = synonym FDATA . append({ID :{ title :title,\\ synonym : syn,\\ definition :deff,\\ parents :parents,\\ childs :childs}}) return FDATA","title":"A Sample File Parser"},{"location":"icd11/icd/#view-sample-icd-11-tree-cor-cardiovascular-disease-here","text":"","title":"View Sample ICD 11 tree cor Cardiovascular Disease here"},{"location":"mesh/mesh/","text":"MeSH Tree Data import json from collections import Counter MeSH Tree This MeSH tree data is available at National Library of Medicine (NLM) website. MeSH descriptors are organized in 16 categories: category A for anatomic terms, category B for organisms, C for diseases, D for drugs and chemicals, etc. Each category is further divided into subcategories. Within each subcategory, descriptors are arrayed hierarchically from most general to most specific in up to thirteen hierarchical levels. Because of the branching structure of the hierarchies, these lists are sometimes referred to as \"trees\". Each MeSH descriptor appears in at least one place in the trees, and may appear in as many additional places as may be appropriate. Those who use MeSH should find the most specific MeSH descriptor that is available to represent each concept of interest. Source View CVD MeSH Tree Online meshtree_file = ./input/mtrees2020.bin Tree = [] id2name = {} name2id = {} with open (meshtree_file, r ) as ftree: for line in ftree: term_tree = line . strip() . split( ; ) cur_term = term_tree[ 0 ] cur_tree = term_tree[ 1 ] id2name . update({cur_tree:cur_term}) name2id . update({cur_term:cur_tree}) Tree . append({ id :cur_tree , name :cur_term}) CVDTree = [] for name,ID in name2id . items(): if ID[ 0 : 3 ] == C14 : CVDTree . append({ name : name, ID :ID}) len ( list (CVDTree)) 204 CVD = pd . DataFrame(CVDTree) CVD = CVD . set_index( name ) CVD = CVD . sort_values( ID ,ascending = True ) CVD . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID name Cardiovascular Diseases C14 Cardiovascular Infections C14.260 Syphilis, Cardiovascular C14.260.500 Tuberculosis, Cardiovascular C14.260.750 Heart Diseases C14.280 Carcinoid Heart Disease C14.280.104 Cardiac Conduction System Disease C14.280.123 Pre-Excitation Syndromes C14.280.123.750 Lown-Ganong-Levine Syndrome C14.280.123.750.560 Pre-Excitation, Mahaim-Type C14.280.123.750.770 Cardiac Tamponade C14.280.155 Cardiomyopathies C14.280.238 Cardiomyopathy, Dilated C14.280.238.070 Cardiomyopathy, Restrictive C14.280.238.160 Chagas Cardiomyopathy C14.280.238.190 Endocardial Fibroelastosis C14.280.238.281 Endomyocardial Fibrosis C14.280.238.406 Myocarditis C14.280.238.625 Endocarditis C14.280.282 Endocarditis, Bacterial C14.280.282.407 Endocarditis, Subacute Bacterial C14.280.282.407.407 Endocarditis, Non-Infective C14.280.282.703 Heart Arrest C14.280.383 Out-of-Hospital Cardiac Arrest C14.280.383.610 Heart Failure C14.280.434 Cardio-Renal Syndrome C14.280.434.156 Heart Failure, Diastolic C14.280.434.611 Heart Failure, Systolic C14.280.434.676 Heart Neoplasms C14.280.459 Heart Rupture C14.280.470 Heart Rupture, Post-Infarction C14.280.470.475 Ventricular Septal Rupture C14.280.470.475.900 Heart Valve Diseases C14.280.484 Aortic Valve Insufficiency C14.280.484.095 Cardiomyopathy, Hypertrophic C14.280.484.150.070.160 Heart Valve Prolapse C14.280.484.400 Aortic Valve Prolapse C14.280.484.400.100 Mitral Valve Prolapse C14.280.484.400.500 Tricuspid Valve Prolapse C14.280.484.400.875 Mitral Valve Insufficiency C14.280.484.461 Mitral Valve Stenosis C14.280.484.517 Pulmonary Valve Insufficiency C14.280.484.660 Tricuspid Valve Insufficiency C14.280.484.856 Tricuspid Valve Stenosis C14.280.484.911 Pericardial Effusion C14.280.695 Pericarditis C14.280.720 Pericarditis, Constrictive C14.280.720.595 Pericarditis, Tuberculous C14.280.720.801 Pneumopericardium C14.280.763 Pulmonary Heart Disease C14.280.832 CVD . to_csv( cvd.csv ) View CVD MeSH Tree Online","title":"MeSH Tree"},{"location":"mesh/mesh/#mesh-tree-data","text":"import json from collections import Counter","title":"MeSH Tree Data"},{"location":"mesh/mesh/#mesh-tree","text":"This MeSH tree data is available at National Library of Medicine (NLM) website. MeSH descriptors are organized in 16 categories: category A for anatomic terms, category B for organisms, C for diseases, D for drugs and chemicals, etc. Each category is further divided into subcategories. Within each subcategory, descriptors are arrayed hierarchically from most general to most specific in up to thirteen hierarchical levels. Because of the branching structure of the hierarchies, these lists are sometimes referred to as \"trees\". Each MeSH descriptor appears in at least one place in the trees, and may appear in as many additional places as may be appropriate. Those who use MeSH should find the most specific MeSH descriptor that is available to represent each concept of interest. Source View CVD MeSH Tree Online meshtree_file = ./input/mtrees2020.bin Tree = [] id2name = {} name2id = {} with open (meshtree_file, r ) as ftree: for line in ftree: term_tree = line . strip() . split( ; ) cur_term = term_tree[ 0 ] cur_tree = term_tree[ 1 ] id2name . update({cur_tree:cur_term}) name2id . update({cur_term:cur_tree}) Tree . append({ id :cur_tree , name :cur_term}) CVDTree = [] for name,ID in name2id . items(): if ID[ 0 : 3 ] == C14 : CVDTree . append({ name : name, ID :ID}) len ( list (CVDTree)) 204 CVD = pd . DataFrame(CVDTree) CVD = CVD . set_index( name ) CVD = CVD . sort_values( ID ,ascending = True ) CVD . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID name Cardiovascular Diseases C14 Cardiovascular Infections C14.260 Syphilis, Cardiovascular C14.260.500 Tuberculosis, Cardiovascular C14.260.750 Heart Diseases C14.280 Carcinoid Heart Disease C14.280.104 Cardiac Conduction System Disease C14.280.123 Pre-Excitation Syndromes C14.280.123.750 Lown-Ganong-Levine Syndrome C14.280.123.750.560 Pre-Excitation, Mahaim-Type C14.280.123.750.770 Cardiac Tamponade C14.280.155 Cardiomyopathies C14.280.238 Cardiomyopathy, Dilated C14.280.238.070 Cardiomyopathy, Restrictive C14.280.238.160 Chagas Cardiomyopathy C14.280.238.190 Endocardial Fibroelastosis C14.280.238.281 Endomyocardial Fibrosis C14.280.238.406 Myocarditis C14.280.238.625 Endocarditis C14.280.282 Endocarditis, Bacterial C14.280.282.407 Endocarditis, Subacute Bacterial C14.280.282.407.407 Endocarditis, Non-Infective C14.280.282.703 Heart Arrest C14.280.383 Out-of-Hospital Cardiac Arrest C14.280.383.610 Heart Failure C14.280.434 Cardio-Renal Syndrome C14.280.434.156 Heart Failure, Diastolic C14.280.434.611 Heart Failure, Systolic C14.280.434.676 Heart Neoplasms C14.280.459 Heart Rupture C14.280.470 Heart Rupture, Post-Infarction C14.280.470.475 Ventricular Septal Rupture C14.280.470.475.900 Heart Valve Diseases C14.280.484 Aortic Valve Insufficiency C14.280.484.095 Cardiomyopathy, Hypertrophic C14.280.484.150.070.160 Heart Valve Prolapse C14.280.484.400 Aortic Valve Prolapse C14.280.484.400.100 Mitral Valve Prolapse C14.280.484.400.500 Tricuspid Valve Prolapse C14.280.484.400.875 Mitral Valve Insufficiency C14.280.484.461 Mitral Valve Stenosis C14.280.484.517 Pulmonary Valve Insufficiency C14.280.484.660 Tricuspid Valve Insufficiency C14.280.484.856 Tricuspid Valve Stenosis C14.280.484.911 Pericardial Effusion C14.280.695 Pericarditis C14.280.720 Pericarditis, Constrictive C14.280.720.595 Pericarditis, Tuberculous C14.280.720.801 Pneumopericardium C14.280.763 Pulmonary Heart Disease C14.280.832 CVD . to_csv( cvd.csv )","title":"MeSH Tree"},{"location":"mesh/mesh/#view-cvd-mesh-tree-online","text":"","title":"View CVD MeSH Tree Online"},{"location":"mongoDB/MongoPopulate/","text":"Populate MongoDB This notebook demonstrate how to populate MongDB Database with Pubmed documents. import pymongo import json client = pymongo . MongoClient( mongodb://localhost:27017/ ) db = client[ PubMed ] collection = db[ documents ] db . documents . estimated_document_count() 4645413 Show database names client . list_database_names() [ PubMed , admin , config , local ] To drop the database #client.drop_database( PubMed ) Populate the Database k = 0 with open ( ../caseolap/data/pubmed.json , r ) as file: while k 5 : for line in file: item = json . loads(line . strip()) print (item . keys()) k = k + 1 break dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) k = 0 j = 1 selected = {} with open ( ../caseolap/data/pubmed.json , r ) as file: for line in file: data = {} item = json . loads(line . strip()) #print(item.keys()) PMID = item[ PMID ] try : status = selected[PMID] except : data . update({ _id : item[ PMID ], title : item[ ArticleTitle ],\\ abstract : item[ Abstract ],\\ MeSH : item[ MeshHeadingList ],\\ journal : item[ Journal ],\\ date :item[ PubDate ], location : item[ Country ]}) selected . update({PMID: True }) collection . insert_one(data) if k % 500000 == 0 : print (j * 0.5 , million documents incerted in database successfully ) j = j + 1 k = k + 1 x = collection . find_one() print (x) { _id : 21103068 , title : A Direct Comparison of the Anticancer Activities of Digitoxin MeON-Neoglycosides and O-Glycosides: Oligosaccharide Chain Length-Dependent Induction of Caspase-9-Mediated Apoptosis. , abstract : Digitoxin is a cardiac glycoside currently being investigated for potential use in oncology. While a number of structure-activity relationship studies have been conducted, an investigation of anticancer activity as a function of oligosaccharide chain length has not yet been performed. We generated mono-, di-, and tri-O-digitoxoside derivatives of digitoxin and compared their activity to the corresponding MeON-neoglycosides. Both classes of cardenolide derivatives display comparable oligosaccharide chain length-dependent cytotoxicity toward human cancer cell lines. Further investigation revealed that both classes of compounds induce caspase-9-mediated apoptosis in non-small cell lung cancer cells (NCI-H460). Since O-glycosides and MeON-neoglycosides share a similar mode of action, the convenience of MeON-neoglycosylation could be exploited in future SAR work to rapidly survey large numbers of carbohydrates to prioritize selected O-glycoside candidates for traditional synthesis. , MeSH : [], journal : ACS medicinal chemistry letters , date : { Year : 2010 , Month : Jul , Day : 12 , Season : , MedlineDate : }, location : United States }","title":"Populate MongoDB"},{"location":"mongoDB/MongoPopulate/#populate-mongodb","text":"This notebook demonstrate how to populate MongDB Database with Pubmed documents. import pymongo import json client = pymongo . MongoClient( mongodb://localhost:27017/ ) db = client[ PubMed ] collection = db[ documents ] db . documents . estimated_document_count() 4645413","title":"Populate MongoDB"},{"location":"mongoDB/MongoPopulate/#show-database-names","text":"client . list_database_names() [ PubMed , admin , config , local ]","title":"Show database names"},{"location":"mongoDB/MongoPopulate/#to-drop-the-database","text":"#client.drop_database( PubMed )","title":"To drop the database"},{"location":"mongoDB/MongoPopulate/#populate-the-database","text":"k = 0 with open ( ../caseolap/data/pubmed.json , r ) as file: while k 5 : for line in file: item = json . loads(line . strip()) print (item . keys()) k = k + 1 break dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) dict_keys([ PMID , ArticleTitle , Abstract , MeshHeadingList , Journal , PubDate , Country ]) k = 0 j = 1 selected = {} with open ( ../caseolap/data/pubmed.json , r ) as file: for line in file: data = {} item = json . loads(line . strip()) #print(item.keys()) PMID = item[ PMID ] try : status = selected[PMID] except : data . update({ _id : item[ PMID ], title : item[ ArticleTitle ],\\ abstract : item[ Abstract ],\\ MeSH : item[ MeshHeadingList ],\\ journal : item[ Journal ],\\ date :item[ PubDate ], location : item[ Country ]}) selected . update({PMID: True }) collection . insert_one(data) if k % 500000 == 0 : print (j * 0.5 , million documents incerted in database successfully ) j = j + 1 k = k + 1 x = collection . find_one() print (x) { _id : 21103068 , title : A Direct Comparison of the Anticancer Activities of Digitoxin MeON-Neoglycosides and O-Glycosides: Oligosaccharide Chain Length-Dependent Induction of Caspase-9-Mediated Apoptosis. , abstract : Digitoxin is a cardiac glycoside currently being investigated for potential use in oncology. While a number of structure-activity relationship studies have been conducted, an investigation of anticancer activity as a function of oligosaccharide chain length has not yet been performed. We generated mono-, di-, and tri-O-digitoxoside derivatives of digitoxin and compared their activity to the corresponding MeON-neoglycosides. Both classes of cardenolide derivatives display comparable oligosaccharide chain length-dependent cytotoxicity toward human cancer cell lines. Further investigation revealed that both classes of compounds induce caspase-9-mediated apoptosis in non-small cell lung cancer cells (NCI-H460). Since O-glycosides and MeON-neoglycosides share a similar mode of action, the convenience of MeON-neoglycosylation could be exploited in future SAR work to rapidly survey large numbers of carbohydrates to prioritize selected O-glycoside candidates for traditional synthesis. , MeSH : [], journal : ACS medicinal chemistry letters , date : { Year : 2010 , Month : Jul , Day : 12 , Season : , MedlineDate : }, location : United States }","title":"Populate the Database"},{"location":"pca/PCA-CVD/","text":"PCA : CVD import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline import plotly.plotly as py import plotly.graph_objs as go import plotly.tools as tls from sklearn.decomposition import PCA from scipy.stats import norm import matplotlib.pyplot as plt from matplotlib.backends.backend_pdf import PdfPages from scipy.cluster.hierarchy import dendrogram, linkage sns . set(font_scale = 1.8 ) Data data = pd . read_csv( ../datareader/score/score.csv ) data = data . set_index( Protein ) def rearrang (olddf): col = [ CVA , IHD , CM , ARR , VD , CHD ] newdf = pd . DataFrame() for t in col: newdf[t] = olddf[t] return newdf ndf = rearrang(data) ndf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD Protein small_ubiquitin-related_modifier_1 0.041144 0.012216 0.078019 0.000000 0.000000 0.024314 metalloproteinase_inhibitor_4 0.042887 0.054740 0.095265 0.045032 0.034227 0.005072 aromatic-l-amino-acid_decarboxylase 0.055959 0.010260 0.011459 0.070661 0.000000 0.007809 nadph_oxidase_activator_1 0.035732 0.000000 0.000000 0.000000 0.000000 0.000000 tumor_necrosis_factor_ligand_superfamily_member_14 0.035732 0.000000 0.000000 0.000000 0.000000 0.000000 def feature_norm (df): dff = df . copy(deep = True ) fchr = [ CVA , IHD , CM , ARR , VD , CHD ] for t in fchr: dff[t] = (df[t] - df[t] . min()) / (df[t] . max() - df[t] . min()) return dff ndfn = feature_norm(ndf) ndfn . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD Protein small_ubiquitin-related_modifier_1 0.059894 0.029087 0.226948 0.000000 0.000000 0.040827 metalloproteinase_inhibitor_4 0.062432 0.130334 0.277115 0.147418 0.093665 0.008517 aromatic-l-amino-acid_decarboxylase 0.081461 0.024430 0.033333 0.231319 0.000000 0.013113 nadph_oxidase_activator_1 0.052016 0.000000 0.000000 0.000000 0.000000 0.000000 tumor_necrosis_factor_ligand_superfamily_member_14 0.052016 0.000000 0.000000 0.000000 0.000000 0.000000 Biplot: VCD cvddata = ndf . copy(deep = True ) # TODO: Apply PCA by fitting the good data with only two dimensions pca = PCA(n_components = 2 ) pca . fit(cvddata . T) # TODO: Transform the good data using the PCA fit above reduced_data = pca . transform(cvddata . T) # Create a DataFrame for the reduced data reduced_data = pd . DataFrame(reduced_data) reduced_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 2.729121 -1.000078 1 0.659602 2.072654 2 -0.460644 0.529615 3 -0.871578 -0.387538 4 -0.961345 -0.462087 5 -1.095155 -0.752566 def biplot (reduced_data, pca,fname): names = [ CVA , IHD , CM , ARR , VD , CHD ] clrs = [ navy , green , firebrick , mediumslateblue , darkgoldenrod , deepskyblue ] fig, ax = plt . subplots(figsize = ( 10 , 10 )) # scatterplot of the reduced CVDs ax . scatter(x = reduced_data . loc[:, 0 ] * 50 ,\\ y = reduced_data . loc[:, 1 ] * 50 , facecolors = clrs,\\ edgecolors = b ,\\ s = 2000 ,\\ alpha = 0.5 ) for i,x,y in zip ([ 0 , 1 , 2 , 3 , 4 , 5 ],reduced_data . loc[:, 0 ] * 50 ,reduced_data . loc[:, 1 ] * 50 ): ax . annotate(names[i], xy = (x - 6.0 , y - 2.0 ), xytext = (x - 6.0 , y - 2.0 ),fontsize = 15 ) feature_vectors = pca . components_ . T # we use scaling factors to make the arrows easier to see asize, tpos = 700 , 500 , # projections of the original features for i, v in enumerate (feature_vectors): ax . arrow( 0 , 0 , v[ 0 ] * asize, v[ 1 ] * asize, head_width = 0.01 ,\\ head_length = 0.02 ,\\ linewidth = 0.1 ,\\ color = red ) plt . axis([ - 90 , 150 , - 90 , 120 ]) ax . set_xlabel( Dimension 1 , fontsize = 20 ) ax . set_ylabel( Dimension 2 , fontsize = 20 ) ax . set_title( PC plane with original feature projections. , fontsize = 16 ); plt . axhline(y = 0 , color = k ) plt . axvline(x = 0 , color = k ) plt . savefig(fname) return ax # Create a biplot biplot(reduced_data, pca, fname = CVD-biplot.pdf ) matplotlib.axes._subplots.AxesSubplot at 0x7f10760c59b0 Normalized data PCA cvddata = ndfn . copy(deep = True ) # TODO: Apply PCA by fitting the good data with only two dimensions pca = PCA(n_components = 2 ) pca . fit(cvddata . T) # TODO: Transform the good data using the PCA fit above reduced_data = pca . transform(cvddata . T) # Create a DataFrame for the reduced data reduced_data = pd . DataFrame(reduced_data) def nbiplot (reduced_data, pca,fname): names = [ CVA , IHD , CM , ARR , VD , CHD ] clrs = [ navy , green , firebrick , mediumslateblue , darkgoldenrod , deepskyblue ] fig, ax = plt . subplots(figsize = ( 10 , 10 )) # scatterplot of the reduced CVDs ax . scatter(x = reduced_data . loc[:, 0 ] * 20 ,\\ y = reduced_data . loc[:, 1 ] * 20 , facecolors = clrs,\\ edgecolors = b ,\\ s = 2000 ,\\ alpha = 0.5 ) for i,x,y in zip ([ 0 , 1 , 2 , 3 , 4 , 5 ],reduced_data . loc[:, 0 ] * 20 ,reduced_data . loc[:, 1 ] * 20 ): ax . annotate(names[i], xy = (x - 3.0 , y - 1.0 ), xytext = (x - 3.0 , y - 1.0 ),fontsize = 15 ) feature_vectors = pca . components_ . T # we use scaling factors to make the arrows easier to see asize, tpos = 700 , 500 , # projections of the original features for i, v in enumerate (feature_vectors): ax . arrow( 0 , 0 , v[ 0 ] * asize, v[ 1 ] * asize, head_width = 0.01 ,\\ head_length = 0.02 ,\\ linewidth = 0.1 ,\\ color = red ) plt . axis([ - 90 , 150 , - 90 , 120 ]) ax . set_xlabel( Dimension 1 , fontsize = 20 ) ax . set_ylabel( Dimension 2 , fontsize = 20 ) ax . set_title( PC plane with original feature projections. , fontsize = 16 ); plt . axhline(y = 0 , color = k ) plt . axvline(x = 0 , color = k ) plt . savefig(fname) return ax # Create a biplot nbiplot(reduced_data, pca, fname = CVD-nbiplot.pdf ) matplotlib.axes._subplots.AxesSubplot at 0x7f107737d518","title":"Principle Component Analysis"},{"location":"pca/PCA-CVD/#pca-cvd","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline import plotly.plotly as py import plotly.graph_objs as go import plotly.tools as tls from sklearn.decomposition import PCA from scipy.stats import norm import matplotlib.pyplot as plt from matplotlib.backends.backend_pdf import PdfPages from scipy.cluster.hierarchy import dendrogram, linkage sns . set(font_scale = 1.8 )","title":"PCA : CVD"},{"location":"pca/PCA-CVD/#data","text":"data = pd . read_csv( ../datareader/score/score.csv ) data = data . set_index( Protein ) def rearrang (olddf): col = [ CVA , IHD , CM , ARR , VD , CHD ] newdf = pd . DataFrame() for t in col: newdf[t] = olddf[t] return newdf ndf = rearrang(data) ndf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD Protein small_ubiquitin-related_modifier_1 0.041144 0.012216 0.078019 0.000000 0.000000 0.024314 metalloproteinase_inhibitor_4 0.042887 0.054740 0.095265 0.045032 0.034227 0.005072 aromatic-l-amino-acid_decarboxylase 0.055959 0.010260 0.011459 0.070661 0.000000 0.007809 nadph_oxidase_activator_1 0.035732 0.000000 0.000000 0.000000 0.000000 0.000000 tumor_necrosis_factor_ligand_superfamily_member_14 0.035732 0.000000 0.000000 0.000000 0.000000 0.000000 def feature_norm (df): dff = df . copy(deep = True ) fchr = [ CVA , IHD , CM , ARR , VD , CHD ] for t in fchr: dff[t] = (df[t] - df[t] . min()) / (df[t] . max() - df[t] . min()) return dff ndfn = feature_norm(ndf) ndfn . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVA IHD CM ARR VD CHD Protein small_ubiquitin-related_modifier_1 0.059894 0.029087 0.226948 0.000000 0.000000 0.040827 metalloproteinase_inhibitor_4 0.062432 0.130334 0.277115 0.147418 0.093665 0.008517 aromatic-l-amino-acid_decarboxylase 0.081461 0.024430 0.033333 0.231319 0.000000 0.013113 nadph_oxidase_activator_1 0.052016 0.000000 0.000000 0.000000 0.000000 0.000000 tumor_necrosis_factor_ligand_superfamily_member_14 0.052016 0.000000 0.000000 0.000000 0.000000 0.000000","title":"Data"},{"location":"pca/PCA-CVD/#biplot-vcd","text":"cvddata = ndf . copy(deep = True ) # TODO: Apply PCA by fitting the good data with only two dimensions pca = PCA(n_components = 2 ) pca . fit(cvddata . T) # TODO: Transform the good data using the PCA fit above reduced_data = pca . transform(cvddata . T) # Create a DataFrame for the reduced data reduced_data = pd . DataFrame(reduced_data) reduced_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 2.729121 -1.000078 1 0.659602 2.072654 2 -0.460644 0.529615 3 -0.871578 -0.387538 4 -0.961345 -0.462087 5 -1.095155 -0.752566 def biplot (reduced_data, pca,fname): names = [ CVA , IHD , CM , ARR , VD , CHD ] clrs = [ navy , green , firebrick , mediumslateblue , darkgoldenrod , deepskyblue ] fig, ax = plt . subplots(figsize = ( 10 , 10 )) # scatterplot of the reduced CVDs ax . scatter(x = reduced_data . loc[:, 0 ] * 50 ,\\ y = reduced_data . loc[:, 1 ] * 50 , facecolors = clrs,\\ edgecolors = b ,\\ s = 2000 ,\\ alpha = 0.5 ) for i,x,y in zip ([ 0 , 1 , 2 , 3 , 4 , 5 ],reduced_data . loc[:, 0 ] * 50 ,reduced_data . loc[:, 1 ] * 50 ): ax . annotate(names[i], xy = (x - 6.0 , y - 2.0 ), xytext = (x - 6.0 , y - 2.0 ),fontsize = 15 ) feature_vectors = pca . components_ . T # we use scaling factors to make the arrows easier to see asize, tpos = 700 , 500 , # projections of the original features for i, v in enumerate (feature_vectors): ax . arrow( 0 , 0 , v[ 0 ] * asize, v[ 1 ] * asize, head_width = 0.01 ,\\ head_length = 0.02 ,\\ linewidth = 0.1 ,\\ color = red ) plt . axis([ - 90 , 150 , - 90 , 120 ]) ax . set_xlabel( Dimension 1 , fontsize = 20 ) ax . set_ylabel( Dimension 2 , fontsize = 20 ) ax . set_title( PC plane with original feature projections. , fontsize = 16 ); plt . axhline(y = 0 , color = k ) plt . axvline(x = 0 , color = k ) plt . savefig(fname) return ax # Create a biplot biplot(reduced_data, pca, fname = CVD-biplot.pdf ) matplotlib.axes._subplots.AxesSubplot at 0x7f10760c59b0","title":"Biplot: VCD"},{"location":"pca/PCA-CVD/#normalized-data-pca","text":"cvddata = ndfn . copy(deep = True ) # TODO: Apply PCA by fitting the good data with only two dimensions pca = PCA(n_components = 2 ) pca . fit(cvddata . T) # TODO: Transform the good data using the PCA fit above reduced_data = pca . transform(cvddata . T) # Create a DataFrame for the reduced data reduced_data = pd . DataFrame(reduced_data) def nbiplot (reduced_data, pca,fname): names = [ CVA , IHD , CM , ARR , VD , CHD ] clrs = [ navy , green , firebrick , mediumslateblue , darkgoldenrod , deepskyblue ] fig, ax = plt . subplots(figsize = ( 10 , 10 )) # scatterplot of the reduced CVDs ax . scatter(x = reduced_data . loc[:, 0 ] * 20 ,\\ y = reduced_data . loc[:, 1 ] * 20 , facecolors = clrs,\\ edgecolors = b ,\\ s = 2000 ,\\ alpha = 0.5 ) for i,x,y in zip ([ 0 , 1 , 2 , 3 , 4 , 5 ],reduced_data . loc[:, 0 ] * 20 ,reduced_data . loc[:, 1 ] * 20 ): ax . annotate(names[i], xy = (x - 3.0 , y - 1.0 ), xytext = (x - 3.0 , y - 1.0 ),fontsize = 15 ) feature_vectors = pca . components_ . T # we use scaling factors to make the arrows easier to see asize, tpos = 700 , 500 , # projections of the original features for i, v in enumerate (feature_vectors): ax . arrow( 0 , 0 , v[ 0 ] * asize, v[ 1 ] * asize, head_width = 0.01 ,\\ head_length = 0.02 ,\\ linewidth = 0.1 ,\\ color = red ) plt . axis([ - 90 , 150 , - 90 , 120 ]) ax . set_xlabel( Dimension 1 , fontsize = 20 ) ax . set_ylabel( Dimension 2 , fontsize = 20 ) ax . set_title( PC plane with original feature projections. , fontsize = 16 ); plt . axhline(y = 0 , color = k ) plt . axvline(x = 0 , color = k ) plt . savefig(fname) return ax # Create a biplot nbiplot(reduced_data, pca, fname = CVD-nbiplot.pdf ) matplotlib.axes._subplots.AxesSubplot at 0x7f107737d518","title":"Normalized data PCA"},{"location":"reactome/Pathways/","text":"Extraction of Pathways (Reactome) REACTOME is an open-source, open access, manually curated and peer-reviewed pathway database. OuREACTOME is an open-source, open access, manually curated and peer-reviewed pathway database. Our goal is to provide intuitive bioinformatics tools for the visualization, interpretation and analysis of pathway knowledge to support basic and clinical research, genome analysis, modeling, systems biology and education. Founded in 2003, the Reactome project is led by Lincoln Stein of OICR, Peter D\u2019Eustachio of NYULMC, Henning Hermjakob of EMBL-EBI, and Guanming Wu of OHSU.r goal is to provide intuitive bioinformatics tools for the visualization, interpretation and analysis of pathway knowledge to support basic and clinical research, genome analysis, modeling, systems biology and education. Founded in 2003, the Reactome project is led by Lincoln Stein of OICR, Peter D\u2019Eustachio of NYULMC, Henning Hermjakob of EMBL-EBI, and Guanming Wu of OHSU. Source import json as json import pandas as pd DATA = [] HUMAN = [] with open ( ReactomePathways.txt , r ) as f1: for line in f1: #print(line.split( \\t )) sl = line . split( \\t ) RID = sl[ 0 ] name = sl[ 1 ] specis = sl[ 2 ] if specis[ - 1 ] == \\n : specis = specis[ 0 : - 1 ] DATA . append({ RID :RID, name :name, specis :specis}) if specis == Homo sapiens : HUMAN . append({ RID :RID, name :name, specis :specis}) len (DATA) 20751 len (HUMAN) 2255 HDF = pd . DataFrame(HUMAN) HDF . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } RID name specis 0 R-HSA-164843 2-LTR circle formation Homo sapiens 1 R-HSA-73843 5-Phosphoribose 1-diphosphate biosynthesis Homo sapiens 2 R-HSA-1971475 A tetrasaccharide linker sequence is required ... Homo sapiens 3 R-HSA-5619084 ABC transporter disorders Homo sapiens 4 R-HSA-1369062 ABC transporters in lipid homeostasis Homo sapiens 5 R-HSA-382556 ABC-family proteins mediated transport Homo sapiens 6 R-HSA-9033807 ABO blood group biosynthesis Homo sapiens 7 R-HSA-418592 ADP signalling through P2Y purinoceptor 1 Homo sapiens 8 R-HSA-392170 ADP signalling through P2Y purinoceptor 12 Homo sapiens 9 R-HSA-198323 AKT phosphorylates targets in the cytosol Homo sapiens 10 R-HSA-198693 AKT phosphorylates targets in the nucleus Homo sapiens 11 R-HSA-211163 AKT-mediated inactivation of FOXO1A Homo sapiens 12 R-HSA-112122 ALKBH2 mediated reversal of alkylation damage Homo sapiens 13 R-HSA-112126 ALKBH3 mediated reversal of alkylation damage Homo sapiens 14 R-HSA-4839748 AMER1 mutants destabilize the destruction complex Homo sapiens 15 R-HSA-163680 AMPK inhibits chREBP transcriptional activatio... Homo sapiens 16 R-HSA-5467333 APC truncation mutants are not K63 polyubiquit... Homo sapiens 17 R-HSA-5467337 APC truncation mutants have impaired AXIN binding Homo sapiens 18 R-HSA-179409 APC-Cdc20 mediated degradation of Nek2A Homo sapiens 19 R-HSA-174143 APC/C-mediated degradation of cell cycle proteins Homo sapiens 20 R-HSA-174048 APC/C:Cdc20 mediated degradation of Cyclin B Homo sapiens 21 R-HSA-174154 APC/C:Cdc20 mediated degradation of Securin Homo sapiens 22 R-HSA-176409 APC/C:Cdc20 mediated degradation of mitotic pr... Homo sapiens 23 R-HSA-174178 APC/C:Cdh1 mediated degradation of Cdc20 and o... Homo sapiens 24 R-HSA-179419 APC:Cdc20 mediated degradation of cell cycle p... Homo sapiens 25 R-HSA-5649702 APEX1-Independent Resolution of AP Sites via t... Homo sapiens 26 R-HSA-180689 APOBEC3G mediated resistance to HIV-1 infection Homo sapiens 27 R-HSA-5624958 ARL13B-mediated ciliary trafficking of INPP5E Homo sapiens 28 R-HSA-170984 ARMS-mediated activation Homo sapiens 29 R-HSA-380994 ATF4 activates genes in response to endoplasmi... Homo sapiens 30 R-HSA-381183 ATF6 (ATF6-alpha) activates chaperone genes Homo sapiens 31 R-HSA-381033 ATF6 (ATF6-alpha) activates chaperones Homo sapiens 32 R-HSA-1296025 ATP sensitive Potassium channels Homo sapiens 33 R-HSA-450408 AUF1 (hnRNP D0) binds and destabilizes mRNA Homo sapiens 34 R-HSA-8854518 AURKA Activation by TPX2 Homo sapiens 35 R-HSA-5467340 AXIN missense mutants destabilize the destruct... Homo sapiens 36 R-HSA-4839735 AXIN mutants destabilize the destruction compl... Homo sapiens 37 R-HSA-2161541 Abacavir metabolism Homo sapiens 38 R-HSA-2161517 Abacavir transmembrane transport Homo sapiens 39 R-HSA-2161522 Abacavir transport and metabolism Homo sapiens 40 R-HSA-73930 Abasic sugar-phosphate removal via the single-... Homo sapiens 41 R-HSA-2978092 Abnormal conversion of 2-oxoglutarate to 2-hyd... Homo sapiens 42 R-HSA-167242 Abortive elongation of HIV-1 transcript in the... Homo sapiens 43 R-HSA-156582 Acetylation Homo sapiens 44 R-HSA-264642 Acetylcholine Neurotransmitter Release Cycle Homo sapiens 45 R-HSA-181431 Acetylcholine binding and downstream events Homo sapiens 46 R-HSA-399997 Acetylcholine regulates insulin secretion Homo sapiens 47 R-HSA-1300645 Acrosome Reaction and Sperm:Oocyte Membrane Bi... Homo sapiens 48 R-HSA-2122948 Activated NOTCH1 Transmits Signal to the Nucleus Homo sapiens 49 R-HSA-9032845 Activated NTRK2 signals through CDK5 Homo sapiens HDF . to_csv( Allpathways_HomoSpiens.csv ) with open ( pathways.txt , w ) as file: for item in HDF[ name ]: file . write(item) file . write( \\n ) Extraction of All Uniprot to Pathways import json as json import pandas as pd DATA = [] HUMAN = [] with open ( UniProt2Reactome_All_Levels.txt , r ) as f1: for line in f1: #print(line.split( \\t )) sl = line . split( \\t ) #print(sl) UID = sl[ 0 ] RID = sl[ 1 ] WAD = sl[ 2 ] name = sl[ 3 ] GEN = sl[ 4 ] specis = sl[ 5 ] if specis[ - 1 ] == \\n : specis = specis[ 0 : - 1 ] DATA . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN : GEN,\\ specis :specis\\ }) if specis == Homo sapiens : HUMAN . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN :GEN,\\ specis :specis\\ }) len (DATA) 784328 len (HUMAN) 125359 HDF = pd . DataFrame(HUMAN) HDF . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } GEN RID UID WAD name specis 0 TAS R-HSA-109582 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 1 TAS R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 2 IEA R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 3 TAS R-HSA-166658 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 4 TAS R-HSA-166663 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 5 TAS R-HSA-166786 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 6 TAS R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 7 IEA R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 8 TAS R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 9 IEA R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 10 TAS R-HSA-173623 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 11 TAS R-HSA-198933 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 12 TAS R-HSA-202733 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 13 TAS R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 14 IEA R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 15 IEA R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 16 TAS R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 17 TAS R-HSA-2029482 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Regulation of actin dynamics for phagocytic cu... Homo sapiens 18 TAS R-HSA-2029485 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Role of phospholipids in phagocytosis Homo sapiens 19 TAS R-HSA-2168880 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Scavenging of heme from plasma Homo sapiens 20 TAS R-HSA-2173782 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Binding and Uptake of Ligands by Scavenger Rec... Homo sapiens 21 TAS R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 22 IEA R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 23 TAS R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 24 IEA R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 25 TAS R-HSA-2871796 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated MAPK activation Homo sapiens 26 TAS R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 27 IEA R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 28 TAS R-HSA-2871837 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated NF-kB activation Homo sapiens 29 TAS R-HSA-5653656 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... Vesicle-mediated transport Homo sapiens 30 TAS R-HSA-5690714 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... CD22 mediated BCR regulation Homo sapiens 31 TAS R-HSA-977606 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-97... Regulation of Complement cascade Homo sapiens 32 TAS R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 33 IEA R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 34 TAS R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 35 IEA R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 36 TAS R-HSA-109582 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 37 TAS R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 38 IEA R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 39 TAS R-HSA-166658 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 40 TAS R-HSA-166663 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 41 TAS R-HSA-166786 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 42 TAS R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 43 IEA R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 44 TAS R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 45 IEA R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 46 TAS R-HSA-173623 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 47 TAS R-HSA-198933 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 48 TAS R-HSA-202733 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 49 TAS R-HSA-2029480 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens HDF . to_csv( Uniprot2Pathways_HomoSpiens.csv ) Uniprot to Pathways unames = list ( set (HDF[ name ])) len (unames) 2229 Name2uniprot = [] for name in unames: alluniprot = [] for ID,nm in zip (HDF[ UID ],HDF[ name ]): if nm == name: alluniprot . append(ID) Name2uniprot . append({ Pathway : name, UIDs :alluniprot, count : len (alluniprot)}) NUP = pd . DataFrame(Name2uniprot) NUP . to_csv( Pathways2Uniprot.csv )","title":"Reactome"},{"location":"reactome/Pathways/#extraction-of-pathways-reactome","text":"REACTOME is an open-source, open access, manually curated and peer-reviewed pathway database. OuREACTOME is an open-source, open access, manually curated and peer-reviewed pathway database. Our goal is to provide intuitive bioinformatics tools for the visualization, interpretation and analysis of pathway knowledge to support basic and clinical research, genome analysis, modeling, systems biology and education. Founded in 2003, the Reactome project is led by Lincoln Stein of OICR, Peter D\u2019Eustachio of NYULMC, Henning Hermjakob of EMBL-EBI, and Guanming Wu of OHSU.r goal is to provide intuitive bioinformatics tools for the visualization, interpretation and analysis of pathway knowledge to support basic and clinical research, genome analysis, modeling, systems biology and education. Founded in 2003, the Reactome project is led by Lincoln Stein of OICR, Peter D\u2019Eustachio of NYULMC, Henning Hermjakob of EMBL-EBI, and Guanming Wu of OHSU. Source import json as json import pandas as pd DATA = [] HUMAN = [] with open ( ReactomePathways.txt , r ) as f1: for line in f1: #print(line.split( \\t )) sl = line . split( \\t ) RID = sl[ 0 ] name = sl[ 1 ] specis = sl[ 2 ] if specis[ - 1 ] == \\n : specis = specis[ 0 : - 1 ] DATA . append({ RID :RID, name :name, specis :specis}) if specis == Homo sapiens : HUMAN . append({ RID :RID, name :name, specis :specis}) len (DATA) 20751 len (HUMAN) 2255 HDF = pd . DataFrame(HUMAN) HDF . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } RID name specis 0 R-HSA-164843 2-LTR circle formation Homo sapiens 1 R-HSA-73843 5-Phosphoribose 1-diphosphate biosynthesis Homo sapiens 2 R-HSA-1971475 A tetrasaccharide linker sequence is required ... Homo sapiens 3 R-HSA-5619084 ABC transporter disorders Homo sapiens 4 R-HSA-1369062 ABC transporters in lipid homeostasis Homo sapiens 5 R-HSA-382556 ABC-family proteins mediated transport Homo sapiens 6 R-HSA-9033807 ABO blood group biosynthesis Homo sapiens 7 R-HSA-418592 ADP signalling through P2Y purinoceptor 1 Homo sapiens 8 R-HSA-392170 ADP signalling through P2Y purinoceptor 12 Homo sapiens 9 R-HSA-198323 AKT phosphorylates targets in the cytosol Homo sapiens 10 R-HSA-198693 AKT phosphorylates targets in the nucleus Homo sapiens 11 R-HSA-211163 AKT-mediated inactivation of FOXO1A Homo sapiens 12 R-HSA-112122 ALKBH2 mediated reversal of alkylation damage Homo sapiens 13 R-HSA-112126 ALKBH3 mediated reversal of alkylation damage Homo sapiens 14 R-HSA-4839748 AMER1 mutants destabilize the destruction complex Homo sapiens 15 R-HSA-163680 AMPK inhibits chREBP transcriptional activatio... Homo sapiens 16 R-HSA-5467333 APC truncation mutants are not K63 polyubiquit... Homo sapiens 17 R-HSA-5467337 APC truncation mutants have impaired AXIN binding Homo sapiens 18 R-HSA-179409 APC-Cdc20 mediated degradation of Nek2A Homo sapiens 19 R-HSA-174143 APC/C-mediated degradation of cell cycle proteins Homo sapiens 20 R-HSA-174048 APC/C:Cdc20 mediated degradation of Cyclin B Homo sapiens 21 R-HSA-174154 APC/C:Cdc20 mediated degradation of Securin Homo sapiens 22 R-HSA-176409 APC/C:Cdc20 mediated degradation of mitotic pr... Homo sapiens 23 R-HSA-174178 APC/C:Cdh1 mediated degradation of Cdc20 and o... Homo sapiens 24 R-HSA-179419 APC:Cdc20 mediated degradation of cell cycle p... Homo sapiens 25 R-HSA-5649702 APEX1-Independent Resolution of AP Sites via t... Homo sapiens 26 R-HSA-180689 APOBEC3G mediated resistance to HIV-1 infection Homo sapiens 27 R-HSA-5624958 ARL13B-mediated ciliary trafficking of INPP5E Homo sapiens 28 R-HSA-170984 ARMS-mediated activation Homo sapiens 29 R-HSA-380994 ATF4 activates genes in response to endoplasmi... Homo sapiens 30 R-HSA-381183 ATF6 (ATF6-alpha) activates chaperone genes Homo sapiens 31 R-HSA-381033 ATF6 (ATF6-alpha) activates chaperones Homo sapiens 32 R-HSA-1296025 ATP sensitive Potassium channels Homo sapiens 33 R-HSA-450408 AUF1 (hnRNP D0) binds and destabilizes mRNA Homo sapiens 34 R-HSA-8854518 AURKA Activation by TPX2 Homo sapiens 35 R-HSA-5467340 AXIN missense mutants destabilize the destruct... Homo sapiens 36 R-HSA-4839735 AXIN mutants destabilize the destruction compl... Homo sapiens 37 R-HSA-2161541 Abacavir metabolism Homo sapiens 38 R-HSA-2161517 Abacavir transmembrane transport Homo sapiens 39 R-HSA-2161522 Abacavir transport and metabolism Homo sapiens 40 R-HSA-73930 Abasic sugar-phosphate removal via the single-... Homo sapiens 41 R-HSA-2978092 Abnormal conversion of 2-oxoglutarate to 2-hyd... Homo sapiens 42 R-HSA-167242 Abortive elongation of HIV-1 transcript in the... Homo sapiens 43 R-HSA-156582 Acetylation Homo sapiens 44 R-HSA-264642 Acetylcholine Neurotransmitter Release Cycle Homo sapiens 45 R-HSA-181431 Acetylcholine binding and downstream events Homo sapiens 46 R-HSA-399997 Acetylcholine regulates insulin secretion Homo sapiens 47 R-HSA-1300645 Acrosome Reaction and Sperm:Oocyte Membrane Bi... Homo sapiens 48 R-HSA-2122948 Activated NOTCH1 Transmits Signal to the Nucleus Homo sapiens 49 R-HSA-9032845 Activated NTRK2 signals through CDK5 Homo sapiens HDF . to_csv( Allpathways_HomoSpiens.csv ) with open ( pathways.txt , w ) as file: for item in HDF[ name ]: file . write(item) file . write( \\n )","title":"Extraction of Pathways (Reactome)"},{"location":"reactome/Pathways/#extraction-of-all-uniprot-to-pathways","text":"import json as json import pandas as pd DATA = [] HUMAN = [] with open ( UniProt2Reactome_All_Levels.txt , r ) as f1: for line in f1: #print(line.split( \\t )) sl = line . split( \\t ) #print(sl) UID = sl[ 0 ] RID = sl[ 1 ] WAD = sl[ 2 ] name = sl[ 3 ] GEN = sl[ 4 ] specis = sl[ 5 ] if specis[ - 1 ] == \\n : specis = specis[ 0 : - 1 ] DATA . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN : GEN,\\ specis :specis\\ }) if specis == Homo sapiens : HUMAN . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN :GEN,\\ specis :specis\\ }) len (DATA) 784328 len (HUMAN) 125359 HDF = pd . DataFrame(HUMAN) HDF . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } GEN RID UID WAD name specis 0 TAS R-HSA-109582 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 1 TAS R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 2 IEA R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 3 TAS R-HSA-166658 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 4 TAS R-HSA-166663 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 5 TAS R-HSA-166786 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 6 TAS R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 7 IEA R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 8 TAS R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 9 IEA R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 10 TAS R-HSA-173623 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 11 TAS R-HSA-198933 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 12 TAS R-HSA-202733 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 13 TAS R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 14 IEA R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 15 IEA R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 16 TAS R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 17 TAS R-HSA-2029482 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Regulation of actin dynamics for phagocytic cu... Homo sapiens 18 TAS R-HSA-2029485 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Role of phospholipids in phagocytosis Homo sapiens 19 TAS R-HSA-2168880 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Scavenging of heme from plasma Homo sapiens 20 TAS R-HSA-2173782 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Binding and Uptake of Ligands by Scavenger Rec... Homo sapiens 21 TAS R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 22 IEA R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 23 TAS R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 24 IEA R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 25 TAS R-HSA-2871796 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated MAPK activation Homo sapiens 26 TAS R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 27 IEA R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 28 TAS R-HSA-2871837 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated NF-kB activation Homo sapiens 29 TAS R-HSA-5653656 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... Vesicle-mediated transport Homo sapiens 30 TAS R-HSA-5690714 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... CD22 mediated BCR regulation Homo sapiens 31 TAS R-HSA-977606 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-97... Regulation of Complement cascade Homo sapiens 32 TAS R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 33 IEA R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 34 TAS R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 35 IEA R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 36 TAS R-HSA-109582 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 37 TAS R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 38 IEA R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 39 TAS R-HSA-166658 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 40 TAS R-HSA-166663 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 41 TAS R-HSA-166786 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 42 TAS R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 43 IEA R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 44 TAS R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 45 IEA R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 46 TAS R-HSA-173623 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 47 TAS R-HSA-198933 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 48 TAS R-HSA-202733 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 49 TAS R-HSA-2029480 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens HDF . to_csv( Uniprot2Pathways_HomoSpiens.csv )","title":"Extraction of All Uniprot to Pathways"},{"location":"reactome/Pathways/#uniprot-to-pathways","text":"unames = list ( set (HDF[ name ])) len (unames) 2229 Name2uniprot = [] for name in unames: alluniprot = [] for ID,nm in zip (HDF[ UID ],HDF[ name ]): if nm == name: alluniprot . append(ID) Name2uniprot . append({ Pathway : name, UIDs :alluniprot, count : len (alluniprot)}) NUP = pd . DataFrame(Name2uniprot) NUP . to_csv( Pathways2Uniprot.csv )","title":"Uniprot to Pathways"},{"location":"reactome/Uniprot2Pathways/","text":"Extraction of All Uniprot to Pathways import json as json import pandas as pd DATA = [] HUMAN = [] with open ( UniProt2Reactome_All_Levels.txt , r ) as f1: for line in f1: #print(line.split( \\t )) sl = line . split( \\t ) #print(sl) UID = sl[ 0 ] RID = sl[ 1 ] WAD = sl[ 2 ] name = sl[ 3 ] GEN = sl[ 4 ] specis = sl[ 5 ] if specis[ - 1 ] == \\n : specis = specis[ 0 : - 1 ] DATA . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN : GEN,\\ specis :specis\\ }) if specis == Homo sapiens : HUMAN . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN :GEN,\\ specis :specis\\ }) len (DATA) 784328 len (HUMAN) 125359 HDF = pd . DataFrame(HUMAN) HDF . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } GEN RID UID WAD name specis 0 TAS R-HSA-109582 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 1 TAS R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 2 IEA R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 3 TAS R-HSA-166658 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 4 TAS R-HSA-166663 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 5 TAS R-HSA-166786 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 6 TAS R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 7 IEA R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 8 TAS R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 9 IEA R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 10 TAS R-HSA-173623 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 11 TAS R-HSA-198933 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 12 TAS R-HSA-202733 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 13 TAS R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 14 IEA R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 15 IEA R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 16 TAS R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 17 TAS R-HSA-2029482 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Regulation of actin dynamics for phagocytic cu... Homo sapiens 18 TAS R-HSA-2029485 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Role of phospholipids in phagocytosis Homo sapiens 19 TAS R-HSA-2168880 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Scavenging of heme from plasma Homo sapiens 20 TAS R-HSA-2173782 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Binding and Uptake of Ligands by Scavenger Rec... Homo sapiens 21 TAS R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 22 IEA R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 23 TAS R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 24 IEA R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 25 TAS R-HSA-2871796 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated MAPK activation Homo sapiens 26 TAS R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 27 IEA R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 28 TAS R-HSA-2871837 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated NF-kB activation Homo sapiens 29 TAS R-HSA-5653656 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... Vesicle-mediated transport Homo sapiens 30 TAS R-HSA-5690714 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... CD22 mediated BCR regulation Homo sapiens 31 TAS R-HSA-977606 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-97... Regulation of Complement cascade Homo sapiens 32 TAS R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 33 IEA R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 34 TAS R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 35 IEA R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 36 TAS R-HSA-109582 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 37 TAS R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 38 IEA R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 39 TAS R-HSA-166658 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 40 TAS R-HSA-166663 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 41 TAS R-HSA-166786 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 42 TAS R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 43 IEA R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 44 TAS R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 45 IEA R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 46 TAS R-HSA-173623 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 47 TAS R-HSA-198933 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 48 TAS R-HSA-202733 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 49 TAS R-HSA-2029480 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens HDF . to_csv( Uniprot2Pathways_HomoSpiens.csv ) Uniprot to Pathways unames = list ( set (HDF[ name ])) len (unames) 2229 Name2uniprot = [] for name in unames: alluniprot = [] for ID,nm in zip (HDF[ UID ],HDF[ name ]): if nm == name: alluniprot . append(ID) Name2uniprot . append({ Pathway : name, UIDs :alluniprot, count : len (alluniprot)}) NUP = pd . DataFrame(Name2uniprot) NUP . to_csv( Pathways2Uniprot.csv )","title":"Uniprot2Pathways"},{"location":"reactome/Uniprot2Pathways/#extraction-of-all-uniprot-to-pathways","text":"import json as json import pandas as pd DATA = [] HUMAN = [] with open ( UniProt2Reactome_All_Levels.txt , r ) as f1: for line in f1: #print(line.split( \\t )) sl = line . split( \\t ) #print(sl) UID = sl[ 0 ] RID = sl[ 1 ] WAD = sl[ 2 ] name = sl[ 3 ] GEN = sl[ 4 ] specis = sl[ 5 ] if specis[ - 1 ] == \\n : specis = specis[ 0 : - 1 ] DATA . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN : GEN,\\ specis :specis\\ }) if specis == Homo sapiens : HUMAN . append({ UID : UID,\\ RID :RID,\\ WAD :WAD,\\ name :name,\\ GEN :GEN,\\ specis :specis\\ }) len (DATA) 784328 len (HUMAN) 125359 HDF = pd . DataFrame(HUMAN) HDF . head( 50 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } GEN RID UID WAD name specis 0 TAS R-HSA-109582 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 1 TAS R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 2 IEA R-HSA-1280218 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 3 TAS R-HSA-166658 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 4 TAS R-HSA-166663 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 5 TAS R-HSA-166786 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 6 TAS R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 7 IEA R-HSA-168249 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 8 TAS R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 9 IEA R-HSA-168256 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 10 TAS R-HSA-173623 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 11 TAS R-HSA-198933 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 12 TAS R-HSA-202733 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 13 TAS R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 14 IEA R-HSA-2029480 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens 15 IEA R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 16 TAS R-HSA-2029481 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... FCGR activation Homo sapiens 17 TAS R-HSA-2029482 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Regulation of actin dynamics for phagocytic cu... Homo sapiens 18 TAS R-HSA-2029485 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-20... Role of phospholipids in phagocytosis Homo sapiens 19 TAS R-HSA-2168880 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Scavenging of heme from plasma Homo sapiens 20 TAS R-HSA-2173782 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-21... Binding and Uptake of Ligands by Scavenger Rec... Homo sapiens 21 TAS R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 22 IEA R-HSA-2454202 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-24... Fc epsilon receptor (FCERI) signaling Homo sapiens 23 TAS R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 24 IEA R-HSA-2730905 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-27... Role of LAT2/NTAL/LAB on calcium mobilization Homo sapiens 25 TAS R-HSA-2871796 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated MAPK activation Homo sapiens 26 TAS R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 27 IEA R-HSA-2871809 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated Ca+2 mobilization Homo sapiens 28 TAS R-HSA-2871837 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-28... FCERI mediated NF-kB activation Homo sapiens 29 TAS R-HSA-5653656 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... Vesicle-mediated transport Homo sapiens 30 TAS R-HSA-5690714 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-56... CD22 mediated BCR regulation Homo sapiens 31 TAS R-HSA-977606 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-97... Regulation of Complement cascade Homo sapiens 32 TAS R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 33 IEA R-HSA-983695 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Antigen activates B Cell Receptor (BCR) leadin... Homo sapiens 34 TAS R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 35 IEA R-HSA-983705 A0A075B6P5 https://reactome.org/PathwayBrowser/#/R-HSA-98... Signaling by the B Cell Receptor (BCR) Homo sapiens 36 TAS R-HSA-109582 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-10... Hemostasis Homo sapiens 37 TAS R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 38 IEA R-HSA-1280218 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-12... Adaptive Immune System Homo sapiens 39 TAS R-HSA-166658 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Complement cascade Homo sapiens 40 TAS R-HSA-166663 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Initial triggering of complement Homo sapiens 41 TAS R-HSA-166786 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Creation of C4 and C2 activators Homo sapiens 42 TAS R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 43 IEA R-HSA-168249 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Innate Immune System Homo sapiens 44 TAS R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 45 IEA R-HSA-168256 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-16... Immune System Homo sapiens 46 TAS R-HSA-173623 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-17... Classical antibody-mediated complement activation Homo sapiens 47 TAS R-HSA-198933 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-19... Immunoregulatory interactions between a Lympho... Homo sapiens 48 TAS R-HSA-202733 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Cell surface interactions at the vascular wall Homo sapiens 49 TAS R-HSA-2029480 A0A075B6S6 https://reactome.org/PathwayBrowser/#/R-HSA-20... Fcgamma receptor (FCGR) dependent phagocytosis Homo sapiens HDF . to_csv( Uniprot2Pathways_HomoSpiens.csv )","title":"Extraction of All Uniprot to Pathways"},{"location":"reactome/Uniprot2Pathways/#uniprot-to-pathways","text":"unames = list ( set (HDF[ name ])) len (unames) 2229 Name2uniprot = [] for name in unames: alluniprot = [] for ID,nm in zip (HDF[ UID ],HDF[ name ]): if nm == name: alluniprot . append(ID) Name2uniprot . append({ Pathway : name, UIDs :alluniprot, count : len (alluniprot)}) NUP = pd . DataFrame(Name2uniprot) NUP . to_csv( Pathways2Uniprot.csv )","title":"Uniprot to Pathways"},{"location":"setup/anaconda/","text":"Installing Python To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python. Note- Linux: For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook Note - Cloud For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Setting up Python"},{"location":"setup/anaconda/#installing-python","text":"To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python.","title":"Installing Python"},{"location":"setup/anaconda/#note-linux","text":"For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook","title":"Note- Linux:"},{"location":"setup/anaconda/#note-cloud","text":"For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Note - Cloud"},{"location":"setup/elasticsearch/","text":"Indexing of ICD 11 Documents Indexing of documents is done in two steps Import the required libraries from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q Initialization of Index Prepare Name and number of shards, clusters etc. INDEX_NAME = icd11 NUMBER_SHARDS = 1 NUMBER_REPLICAS = 0 Prepare request body request_body = { settings : { number_of_shards : NUMBER_SHARDS, number_of_replicas : NUMBER_REPLICAS }, mappings : { properties : { id : { type : keyword }, tree :{ type : text }, name :{ type : text }, root :{ type : text }, degree :{ type : integer }, definition :{ type : text }, synonym :{ type : text } } } } Call Elasticsearch es = Elasticsearch() Facilitate deleting old indexing if already exist if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( Deleting index %s , Response: %s % (INDEX_NAME, res)) Create new Indexing res = es . indices . create(index = INDEX_NAME, body = request_body) print ( Create index %s , Response: %s % (INDEX_NAME, res)) Populating the Index For each item in the data list, create the data dictionary and op dictionary create data dictionary data_dict = {} data_dict[ id ] = item[ id ] data_dict[ tree ] = item[ tree ] data_dict[ root ] = item[ root ] data_dict[ name ] = item[ name ] data_dict[ parents ] = item[ parents ] data_dict[ childs ] = item[ childs ] data_dict[ sibls ] = item[ sibls ] data_dict[ degree ] = item[ degree ] data_dict[ synonym ] = item[ synonym ] data_dict[ definition ] = item[ definition ] create op dictionary op_dict = { index : { _index : INDEX_NAME, _id : data_dict[ id ] } } Add each data dictionary into bulk data list Put current data into the bulk bulk_data . append(op_dict) bulk_data . append(data_dict) Push the doc dictionary to Indexing INDEX_NAME = icd11 bulk_size = 50 es = Elasticsearch() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 )","title":"Setting up Elasticsearch"},{"location":"setup/elasticsearch/#indexing-of-icd-11-documents","text":"Indexing of documents is done in two steps","title":"Indexing of ICD 11 Documents"},{"location":"setup/elasticsearch/#import-the-required-libraries","text":"from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q","title":"Import the required libraries"},{"location":"setup/elasticsearch/#initialization-of-index","text":"Prepare Name and number of shards, clusters etc. INDEX_NAME = icd11 NUMBER_SHARDS = 1 NUMBER_REPLICAS = 0 Prepare request body request_body = { settings : { number_of_shards : NUMBER_SHARDS, number_of_replicas : NUMBER_REPLICAS }, mappings : { properties : { id : { type : keyword }, tree :{ type : text }, name :{ type : text }, root :{ type : text }, degree :{ type : integer }, definition :{ type : text }, synonym :{ type : text } } } } Call Elasticsearch es = Elasticsearch() Facilitate deleting old indexing if already exist if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( Deleting index %s , Response: %s % (INDEX_NAME, res)) Create new Indexing res = es . indices . create(index = INDEX_NAME, body = request_body) print ( Create index %s , Response: %s % (INDEX_NAME, res))","title":"Initialization of Index"},{"location":"setup/elasticsearch/#populating-the-index","text":"For each item in the data list, create the data dictionary and op dictionary create data dictionary data_dict = {} data_dict[ id ] = item[ id ] data_dict[ tree ] = item[ tree ] data_dict[ root ] = item[ root ] data_dict[ name ] = item[ name ] data_dict[ parents ] = item[ parents ] data_dict[ childs ] = item[ childs ] data_dict[ sibls ] = item[ sibls ] data_dict[ degree ] = item[ degree ] data_dict[ synonym ] = item[ synonym ] data_dict[ definition ] = item[ definition ] create op dictionary op_dict = { index : { _index : INDEX_NAME, _id : data_dict[ id ] } } Add each data dictionary into bulk data list Put current data into the bulk bulk_data . append(op_dict) bulk_data . append(data_dict) Push the doc dictionary to Indexing INDEX_NAME = icd11 bulk_size = 50 es = Elasticsearch() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 )","title":"Populating the Index"},{"location":"setup/env/","text":"Python Environment Basics To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = /Users/username/anaconda/bin: $PATH or update above command line to your .zsh_config file. Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name If you want to update all packages in an environment, which is often useful, use conda update --all And finally, to list installed packages, it's conda list If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup Environments Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate Saving and loading environments A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, environment.yaml writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml . Listing environments If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root . Removing environments If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ). Using environments One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican . Sharing environments When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda. More to learn To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"Setting up AWS cloud"},{"location":"setup/env/#python-environment","text":"","title":"Python Environment"},{"location":"setup/env/#basics","text":"To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = /Users/username/anaconda/bin: $PATH or update above command line to your .zsh_config file. Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name If you want to update all packages in an environment, which is often useful, use conda update --all And finally, to list installed packages, it's conda list If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup","title":"Basics"},{"location":"setup/env/#environments","text":"Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate","title":"Environments"},{"location":"setup/env/#saving-and-loading-environments","text":"A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, environment.yaml writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml .","title":"Saving and loading environments"},{"location":"setup/env/#listing-environments","text":"If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root .","title":"Listing environments"},{"location":"setup/env/#removing-environments","text":"If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ).","title":"Removing environments"},{"location":"setup/env/#using-environments","text":"One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican .","title":"Using environments"},{"location":"setup/env/#sharing-environments","text":"When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda.","title":"Sharing environments"},{"location":"setup/env/#more-to-learn","text":"To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"More to learn"},{"location":"setup/git/","text":"How to git Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m First commit Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"Setting up Git"},{"location":"setup/git/#how-to-git","text":"Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m First commit Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"How to git"},{"location":"setup/jupyter/","text":"Installing Jupyter Notebook By far the easiest way to install Jupyter is with Anaconda. Jupyter notebooks automatically come with the distribution. You'll be able to use notebooks from the default environment. To install Jupyter notebooks in a conda environment, use conda install jupyter notebook Jupyter notebooks are also available through pip with pip install jupyter notebook Markdown Cheatsheet : https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb Convert: https://nbconvert.readthedocs.io/en/latest/usage.html To create the slideshow from the notebook file, you'll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it. panda presentation: presentation","title":"Installing Jupyter Notebook"},{"location":"setup/jupyter/#installing-jupyter-notebook","text":"By far the easiest way to install Jupyter is with Anaconda. Jupyter notebooks automatically come with the distribution. You'll be able to use notebooks from the default environment. To install Jupyter notebooks in a conda environment, use conda install jupyter notebook Jupyter notebooks are also available through pip with pip install jupyter notebook Markdown Cheatsheet : https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb Convert: https://nbconvert.readthedocs.io/en/latest/usage.html To create the slideshow from the notebook file, you'll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it. panda presentation: presentation","title":"Installing Jupyter Notebook"},{"location":"setup/mongoDB/","text":"Database After setting up mongoDB database and once parsed data is ready, mongoDB database can be populated with pared data and mapping table Install pymongo in the current environment pip install pymongo Import pymongo to current environment import pymongo Call client for mongoDB database client = pymongo . MongoClient( mongodb://localhost:27017/ ) Create new database Delete previously created database client . drop_database( icd11 ) Create a new database with new name database = client[ NAME ] Add Collection in the new database Create new collection inside the database collection = database[ COLLECTION_NAME ] Populating collection with parsed data x = collection . insert_many(Data) Test a sample out of populated data x = collection . find_one() print (x) Add mapping Tables as new collections Create seperate collection within the database for seperate mapping table mapping_collection = db[ MAPPING_NAME ] Populate the collection with mapping table x = mapping_collection . insert_many(MAPPING_TABLE)","title":"Setting up MongoDB"},{"location":"setup/mongoDB/#database","text":"After setting up mongoDB database and once parsed data is ready, mongoDB database can be populated with pared data and mapping table Install pymongo in the current environment pip install pymongo Import pymongo to current environment import pymongo Call client for mongoDB database client = pymongo . MongoClient( mongodb://localhost:27017/ )","title":"Database"},{"location":"setup/mongoDB/#create-new-database","text":"Delete previously created database client . drop_database( icd11 ) Create a new database with new name database = client[ NAME ]","title":"Create new database"},{"location":"setup/mongoDB/#add-collection-in-the-new-database","text":"Create new collection inside the database collection = database[ COLLECTION_NAME ] Populating collection with parsed data x = collection . insert_many(Data) Test a sample out of populated data x = collection . find_one() print (x)","title":"Add Collection in the new database"},{"location":"setup/mongoDB/#add-mapping-tables-as-new-collections","text":"Create seperate collection within the database for seperate mapping table mapping_collection = db[ MAPPING_NAME ] Populate the collection with mapping table x = mapping_collection . insert_many(MAPPING_TABLE)","title":"Add mapping Tables as new collections"},{"location":"setup/neo4j/","text":"Implementation of Neo4j (Individual) Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Setting up Neo4J"},{"location":"setup/neo4j/#implementation-of-neo4j-individual","text":"Install neo4j python driver pip install neo4j Import neo4j to current environment from neo4j import GraphDatabase Create a python driver driver = GraphDatabase . driver(uri = bolt://localhost:7687 ,\\ auth = ( user , password )) Clean the graph database, if using with pre-existing data with driver . session() as session: session . run( MATCH (a) DETACH DELETE a ) Create nodes in the graph for item in Data: with driver . session() as session: session . run( MERGE(a:Disease{ID: $ID}) ON CREATE SET a.code = $code, \\ a.title = $title, a.defn = $defn, \\ a.syns = $syns, a.childs = $childs, \\ a.parents = $parents , ID = item[ id ], code = item[ code ], title = item[ title ],\\ defn = item[ defn ], syns = item[ syns ], childs = item[ childs ],\\ parents = item[ parents ]) Create edge with relationship \"parents\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.parents MERGE (a) -[r:Parent]-(b) ) Create edge with relationship \"children\" with driver . session() as session: session . run( MATCH (a:Disease),(b:Disease) WHERE a.ID in b.childs MERGE (a) -[r:Child]-(b) )","title":"Implementation of Neo4j (Individual)"},{"location":"tsne/tSNE-average/","text":"Cluster: tSNE complete linkage import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns from time import time from scipy import ndimage from sklearn import manifold, datasets from sklearn.cluster import AgglomerativeClustering from sklearn.metrics import silhouette_score % matplotlib inline sns . set(font_scale = 1.8 ) data = pd . read_csv( ../../datareader/score/score.csv ) data . index = data[ Protein ] ndf = data . drop( Protein ,axis = 1 ) ndf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD Protein alpha-1-antitrypsin 0.009000 0.120724 0.036423 0.047402 0.103137 0.023655 mothers_against_decapentaplegic_homolog_4 0.005316 0.080604 0.056011 0.010512 0.039842 0.000000 vascular_endothelial_growth_factor_b 0.000000 0.006129 0.073377 0.028607 0.121075 0.000000 indoleamine_2,3-dioxygenase_1 0.000000 0.000000 0.000000 0.000000 0.034477 0.000000 thyroxine-binding_globulin 0.000000 0.036206 0.020516 0.006959 0.048815 0.024457 Which is Maximum mdata = ndf . copy(deep = True ) dis = [ ARR , CHD , CM , CVA , IHD , VD ] idx = list (mdata . index) nmax = [] for item in idx: data = mdata . loc[item,:] lst = [data[ 0 ],data[ 1 ],data[ 2 ],data[ 3 ],data[ 4 ],data[ 5 ]] m = max (lst) for d,e in zip (dis,lst): if e == m: nmax . append(d) Cluster-Aglo: Complete tdata = ndf . copy(deep = True ) X = np . array(tdata) linkg = average Build the Manifold # 2D embedding of the digits dataset print ( Computing embedding ) Xtsne = manifold . TSNE(n_components = 2 ) . fit_transform(X) x_min, x_max = np . min(Xtsne, axis = 0 ), np . max(Xtsne, axis = 0 ) Xtsne = (Xtsne - x_min) / (x_max - x_min) print ( Done. ) Computing embedding Done. Xtsne array([[0.58949834, 0.7531166 ], [0.60703075, 0.74036276], [0.29339787, 0.49895537], ..., [0.49842042, 0.57060444], [0.97006494, 0.53362143], [0.02934294, 0.732129 ]], dtype=float32) Results def plot_clustering (Xtsne, labels, title = None ): myclc = [ mediumslateblue , deepskyblue , firebrick , navy , green , darkgoldenrod ] Color = [] X = [] Y = [] plt . figure(figsize = ( 15 , 12 )) for i in range (Xtsne . shape[ 0 ]): plt . text(Xtsne[i, 0 ], Xtsne[i, 1 ], str (labels[i]), color = myclc[labels[i]], fontdict = { weight : bold , size : 12 }) Color . append(myclc[labels[i]]) X . append(Xtsne[i, 0 ]) Y . append(Xtsne[i, 1 ]) if title is not None : plt . title(title, size = 20 ) plt . axis([ - 0.25 , 1.25 , - 0.25 , 1.25 ]) #plt.axhline(y=0, color= k ) #plt.axvline(x=0, color= k ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . grid( True ) plt . tight_layout() return X,Y,Color myclc = [ mediumslateblue , deepskyblue , firebrick , navy , green , darkgoldenrod ] clustering = AgglomerativeClustering(linkage = linkg,n_clusters = 6 ) t0 = time() clustering . fit(Xtsne) print ( %s : %.2f s % (linkg, time() - t0)) X,Y,Color = plot_clustering(Xtsne, clustering . labels_, %s linkage % linkg) plt . savefig( average/ + linkg + .png ) plt . show() average : 0.29s len (clustering . labels_) 2702 resultdf = ndf . copy(deep = True ) resultdf[ label ] = list (clustering . labels_) resultdf[ color ] = Color resultdf[ nmax ] = nmax resultdf[ X ] = X resultdf[ Y ] = Y resultdf . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD label color nmax X Y Protein alpha-1-antitrypsin 0.009000 0.120724 0.036423 0.047402 0.103137 0.023655 1 deepskyblue CHD 0.589498 0.753117 mothers_against_decapentaplegic_homolog_4 0.005316 0.080604 0.056011 0.010512 0.039842 0.000000 1 deepskyblue CHD 0.607031 0.740363 vascular_endothelial_growth_factor_b 0.000000 0.006129 0.073377 0.028607 0.121075 0.000000 4 green IHD 0.293398 0.498955 indoleamine_2,3-dioxygenase_1 0.000000 0.000000 0.000000 0.000000 0.034477 0.000000 5 darkgoldenrod IHD 0.949800 0.544562 thyroxine-binding_globulin 0.000000 0.036206 0.020516 0.006959 0.048815 0.024457 4 green IHD 0.399345 0.635551 Add UNiprot data = pd . read_csv( uniprot.csv ) data . index = data[ protein ] udf = data . drop( protein ,axis = 1 ) udf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uniprot_id gene_names has_multiple_IDs protein natriuretic_peptides_b P16860 NPPB NaN angiotensin-converting_enzyme P12821 ACE DCP DCP1 NaN potassium_voltage-gated_channel_subfamily_h_member_2 Q12809 KCNH2 ERG ERG1 HERG NaN c-reactive_protein P02741 CRP PTX1 NaN apolipoprotein_e P02649 APOE NaN idx_cvd = list (resultdf . index) idx_uprt = list (udf . index) uprot = [] for item in idx_cvd: if item in idx_uprt: uprot . append(udf . loc[item,:][ 0 ]) else : print (item, Match not Found ) uprot . append( NAN ) collectrin Match not Found aldo-keto_reductase_family_1_member_c4 Match not Found macrophage_receptor_marco Match not Found zinc_fingers_and_homeoboxes_protein_2 Match not Found transcription_factor_sp3 Match not Found synaptotagmin-9 Match not Found e3_ubiquitin-protein_ligase_nrdp1 Match not Found nuclear_factor_erythroid_2-related_factor_1 Match not Found serine/threonine-protein_phosphatase_2b_catalytic_subunit_gamma_isoform Match not Found myocyte-specific_enhancer_factor_2a Match not Found neuropeptide_s_receptor Match not Found fidgetin Match not Found histone_deacetylase_6 Match not Found transcription_factor_sox-2 Match not Found voltage-gated_potassium_channel_subunit_beta-1 Match not Found segment_polarity_protein_dishevelled_homolog_dvl-3 Match not Found mitochondrial_intermediate_peptidase Match not Found gtp-binding_protein_rit1 Match not Found oligodendrocyte_transcription_factor_1 Match not Found ets_homologous_factor Match not Found troponin_t,_slow_skeletal_muscle Match not Found dna_repair_protein_rad51_homolog_2 Match not Found xin_actin-binding_repeat-containing_protein_1 Match not Found 3-ketoacyl-coa_thiolase,_peroxisomal Match not Found transcription_factor_sox-17 Match not Found cytochrome_p450_2b6 Match not Found cyclin-dependent_kinase_inhibitor_1 Match not Found leukotriene_b4_receptor_1 Match not Found trans-2,3-enoyl-coa_reductase-like Match not Found apoptosis-inducing_factor_1,_mitochondrial Match not Found frizzled-7 Match not Found cytohesin-2 Match not Found kinesin_heavy_chain_isoform_5a Match not Found phosphatidylcholine:ceramide_cholinephosphotransferase_1 Match not Found sulfiredoxin-1 Match not Found ubiquitin_carboxyl-terminal_hydrolase_cyld Match not Found ermin Match not Found wolframin Match not Found receptor-type_tyrosine-protein_phosphatase_beta Match not Found micos_complex_subunit_mic26 Match not Found neuropeptide_s Match not Found basic_helix-loop-helix_transcription_factor_scleraxis Match not Found transmembrane_6_superfamily_member_2 Match not Found dna_repair-scaffolding_protein Match not Found zinc_finger_protein_260 Match not Found retinoic_acid_receptor_rxr-gamma Match not Found electrogenic_sodium_bicarbonate_cotransporter_1 Match not Found zinc_finger_e-box-binding_homeobox_1 Match not Found beta-adrenergic_receptor_kinase_2 Match not Found adp-ribosylation_factor_6 Match not Found uroplakin-3a Match not Found endoplasmic_reticulum_resident_protein_44 Match not Found angiopoietin-related_protein_1 Match not Found tgf-beta-activated_kinase_1_and_map3k7-binding_protein_1 Match not Found rna-binding_protein_10 Match not Found dna-binding_protein_satb1 Match not Found cytosine-5)-methyltransferase_1 Match not Found laminin_subunit_gamma-1 Match not Found anthrax_toxin_receptor_2 Match not Found protein_kinase_c_zeta_type Match not Found cyclin-dependent_kinase_7 Match not Found potassium_channel_subfamily_k_member_2 Match not Found cytosolic_phospholipase_a2_gamma Match not Found aquaporin-3 Match not Found hyaluronidase_ph-20 Match not Found crk-like_protein Match not Found protein_kinase_c_eta_type Match not Found angiopoietin-like_protein_8 Match not Found fibronectin_type_iii_domain-containing_protein_5 Match not Found voltage-dependent_l-type_calcium_channel_subunit_beta-2 Match not Found eukaryotic_elongation_factor_2_kinase Match not Found voltage-gated_potassium_channel_subunit_beta-2 Match not Found complement_c1q_subcomponent_subunit_a Match not Found glutamate_receptor_ionotropic,_delta-2 Match not Found bromodomain-containing_protein_7 Match not Found cytosine-5)-methyltransferase_3b Match not Found ovarian_cancer_g-protein_coupled_receptor_1 Match not Found anoctamin-6 Match not Found complement_c4-b Match not Found apoptosis_regulator_bcl-2 Match not Found succinate_receptor_1 Match not Found aldo-keto_reductase_family_1_member_b15 Match not Found teratocarcinoma-derived_growth_factor_1 Match not Found s-phase_kinase-associated_protein_1 Match not Found talin-2 Match not Found transcription_factor_mafb Match not Found protein_kinase_c_iota_type Match not Found t-box_transcription_factor_tbx20 Match not Found nuclear_receptor_coactivator_7 Match not Found regulator_of_g-protein_signaling_12 Match not Found g-protein_coupled_receptor_family_c_group_6_member_a Match not Found multiple_pdz_domain_protein Match not Found b-cell_lymphoma_3_protein Match not Found bromodomain-containing_protein_4 Match not Found protein_kinase_c_theta_type Match not Found cdgsh_iron-sulfur_domain-containing_protein_1 Match not Found caspase_recruitment_domain-containing_protein_9 Match not Found interleukin-10_receptor_subunit_alpha Match not Found serine/threonine-protein_kinase_d3 Match not Found ubiquitin-like_protein_isg15 Match not Found e3_ubiquitin-protein_ligase_pellino_homolog_1 Match not Found protein_max Match not Found sonic_hedgehog_protein Match not Found matrilin-2 Match not Found src_kinase-associated_phosphoprotein_2 Match not Found toll-like_receptor_5 Match not Found mixed_lineage_kinase_domain-like_protein Match not Found cyclic_amp-dependent_transcription_factor_atf-6_alpha Match not Found proteasome_activator_complex_subunit_2 Match not Found atp-dependent_dna_helicase_q1 Match not Found sphingosine_1-phosphate_receptor_3 Match not Found protein_phosphatase_1g Match not Found hyaluronidase-3 Match not Found serine/threonine-protein_kinase_d2 Match not Found dual_oxidase_2 Match not Found resultdf[ uprot ] = uprot resultdf . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD label color nmax X Y uprot Protein alpha-1-antitrypsin 0.009000 0.120724 0.036423 0.047402 0.103137 0.023655 1 deepskyblue CHD 0.589498 0.753117 P01009 mothers_against_decapentaplegic_homolog_4 0.005316 0.080604 0.056011 0.010512 0.039842 0.000000 1 deepskyblue CHD 0.607031 0.740363 Q13485 vascular_endothelial_growth_factor_b 0.000000 0.006129 0.073377 0.028607 0.121075 0.000000 4 green IHD 0.293398 0.498955 P49765 indoleamine_2,3-dioxygenase_1 0.000000 0.000000 0.000000 0.000000 0.034477 0.000000 5 darkgoldenrod IHD 0.949800 0.544562 P14902 thyroxine-binding_globulin 0.000000 0.036206 0.020516 0.006959 0.048815 0.024457 4 green IHD 0.399345 0.635551 P05543 smoothelin 0.000000 0.010463 0.000000 0.020742 0.092131 0.013016 4 green IHD 0.262940 0.600811 P53814 short-chain_specific_acyl-coa_dehydrogenase,_mitochondrial 0.000000 0.000000 0.023999 0.000000 0.022102 0.000000 2 firebrick CM 0.658054 0.193892 P16219 protein_tyrosine_phosphatase_type_iva_1 0.030680 0.000000 0.022076 0.000000 0.000000 0.000000 1 deepskyblue ARR 0.580443 0.468921 Q93096 creatine_kinase_u-type,_mitochondrial 0.007622 0.000000 0.017123 0.000000 0.341386 0.000000 4 green IHD 0.147972 0.526543 P12532 c-c_chemokine_receptor_type_5 0.000000 0.000000 0.019980 0.050426 0.000000 0.000000 1 deepskyblue CVA 0.501680 0.598933 P51681 resultdf . to_csv( average/ + linkg + .csv ) df0 = resultdf[resultdf[ label ] == 0 ] df0 . to_csv( average/ + linkg + 0.csv ) df1 = resultdf[resultdf[ label ] == 1 ] df1 . to_csv( average/ + linkg + 1.csv ) df2 = resultdf[resultdf[ label ] == 2 ] df2 . to_csv( average/ + linkg + 2.csv ) df3 = resultdf[resultdf[ label ] == 3 ] df3 . to_csv( average/ + linkg + 3.csv ) df4 = resultdf[resultdf[ label ] == 4 ] df4 . to_csv( average/ + linkg + 4.csv ) df5 = resultdf[resultdf[ label ] == 5 ] df5 . to_csv( average/ + linkg + 5.csv ) df0 . shape (519, 12) Seperate cluster plots plt . figure(figsize = [ 22 , 17 ]) plt . subplot( 2 , 3 , 1 ) plt . scatter(df0[ X ],df0[ Y ],color = df0[ color ]) plt . title( Cluster-label-0 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 2 ) plt . scatter(df1[ X ],df1[ Y ],color = df1[ color ]) plt . title( Cluster-label-1 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 3 ) plt . scatter(df2[ X ],df2[ Y ],color = df2[ color ]) plt . title( Cluster-label-2 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 4 ) plt . scatter(df3[ X ],df3[ Y ],color = df3[ color ]) plt . title( Cluster-label-3 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 5 ) plt . scatter(df4[ X ],df4[ Y ],color = df4[ color ]) plt . title( Cluster-label-4 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 6 ) plt . scatter(df5[ X ],df5[ Y ],color = df5[ color ]) plt . title( Cluster-label-5 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . savefig( average/ + linkg + -subplot.png ) plt . show() Cluster In deep resultdf . groupby( label ) . count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD color nmax X Y uprot label 0 519 519 519 519 519 519 519 519 519 519 519 1 676 676 676 676 676 676 676 676 676 676 676 2 287 287 287 287 287 287 287 287 287 287 287 3 112 112 112 112 112 112 112 112 112 112 112 4 911 911 911 911 911 911 911 911 911 911 911 5 197 197 197 197 197 197 197 197 197 197 197 df0 . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD label color nmax X Y uprot Protein sh3_and_multiple_ankyrin_repeat_domains_protein_3 0.00000 0.000000 0.000000 0.039151 0.000000 0.000000 0 mediumslateblue CVA 0.654033 0.959608 Q9BYB0 endophilin-b1 0.00000 0.000000 0.000000 0.043899 0.000000 0.000000 0 mediumslateblue CVA 0.516064 0.954616 Q9Y371 a_disintegrin_and_metalloproteinase_with_thrombospondin_motifs_12 0.00000 0.000000 0.000000 0.000000 0.062291 0.000000 0 mediumslateblue IHD 0.230749 0.748474 P58397 cytokine_receptor_common_subunit_gamma 0.00000 0.000000 0.000000 0.039151 0.000000 0.000000 0 mediumslateblue CVA 0.661077 0.969435 P31785 myelin_basic_protein 0.00000 0.007666 0.002121 0.349511 0.003877 0.003344 0 mediumslateblue CVA 0.352625 0.888755 P02686 phosphate-regulating_neutral_endopeptidase 0.00000 0.000000 0.000000 0.000000 0.062291 0.000000 0 mediumslateblue IHD 0.226173 0.747243 P78562 protein_z-dependent_protease_inhibitor 0.00000 0.000000 0.000000 0.065516 0.032020 0.000000 0 mediumslateblue CVA 0.432626 0.801987 Q9UK55 ste20-like_serine/threonine-protein_kinase 0.00000 0.000000 0.000000 0.040786 0.000000 0.000000 0 mediumslateblue CVA 0.545912 0.965827 Q9H2G2 tissue-type_plasminogen_activator 0.04285 0.013719 0.009599 0.330811 0.231620 0.006301 0 mediumslateblue CVA 0.335407 0.886174 P00750 interleukin-20_receptor_subunit_alpha 0.00000 0.000000 0.000000 0.039151 0.000000 0.000000 0 mediumslateblue CVA 0.653468 0.972735 Q9UHF4 df0[ 0 : 50 ] . drop([ X , Y , label ],axis = 1 ) . plot . barh(stacked = True ,figsize = ( 16 , 25 ),fontsize = 15 ) matplotlib.axes._subplots.AxesSubplot at 0x7f93a8b73358","title":"tSNE"},{"location":"tsne/tSNE-average/#cluster-tsne-complete-linkage","text":"import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns from time import time from scipy import ndimage from sklearn import manifold, datasets from sklearn.cluster import AgglomerativeClustering from sklearn.metrics import silhouette_score % matplotlib inline sns . set(font_scale = 1.8 ) data = pd . read_csv( ../../datareader/score/score.csv ) data . index = data[ Protein ] ndf = data . drop( Protein ,axis = 1 ) ndf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD Protein alpha-1-antitrypsin 0.009000 0.120724 0.036423 0.047402 0.103137 0.023655 mothers_against_decapentaplegic_homolog_4 0.005316 0.080604 0.056011 0.010512 0.039842 0.000000 vascular_endothelial_growth_factor_b 0.000000 0.006129 0.073377 0.028607 0.121075 0.000000 indoleamine_2,3-dioxygenase_1 0.000000 0.000000 0.000000 0.000000 0.034477 0.000000 thyroxine-binding_globulin 0.000000 0.036206 0.020516 0.006959 0.048815 0.024457","title":"Cluster: tSNE complete linkage"},{"location":"tsne/tSNE-average/#which-is-maximum","text":"mdata = ndf . copy(deep = True ) dis = [ ARR , CHD , CM , CVA , IHD , VD ] idx = list (mdata . index) nmax = [] for item in idx: data = mdata . loc[item,:] lst = [data[ 0 ],data[ 1 ],data[ 2 ],data[ 3 ],data[ 4 ],data[ 5 ]] m = max (lst) for d,e in zip (dis,lst): if e == m: nmax . append(d)","title":"Which is Maximum"},{"location":"tsne/tSNE-average/#cluster-aglo-complete","text":"tdata = ndf . copy(deep = True ) X = np . array(tdata) linkg = average","title":"Cluster-Aglo: Complete"},{"location":"tsne/tSNE-average/#build-the-manifold","text":"# 2D embedding of the digits dataset print ( Computing embedding ) Xtsne = manifold . TSNE(n_components = 2 ) . fit_transform(X) x_min, x_max = np . min(Xtsne, axis = 0 ), np . max(Xtsne, axis = 0 ) Xtsne = (Xtsne - x_min) / (x_max - x_min) print ( Done. ) Computing embedding Done. Xtsne array([[0.58949834, 0.7531166 ], [0.60703075, 0.74036276], [0.29339787, 0.49895537], ..., [0.49842042, 0.57060444], [0.97006494, 0.53362143], [0.02934294, 0.732129 ]], dtype=float32)","title":"Build the Manifold"},{"location":"tsne/tSNE-average/#results","text":"def plot_clustering (Xtsne, labels, title = None ): myclc = [ mediumslateblue , deepskyblue , firebrick , navy , green , darkgoldenrod ] Color = [] X = [] Y = [] plt . figure(figsize = ( 15 , 12 )) for i in range (Xtsne . shape[ 0 ]): plt . text(Xtsne[i, 0 ], Xtsne[i, 1 ], str (labels[i]), color = myclc[labels[i]], fontdict = { weight : bold , size : 12 }) Color . append(myclc[labels[i]]) X . append(Xtsne[i, 0 ]) Y . append(Xtsne[i, 1 ]) if title is not None : plt . title(title, size = 20 ) plt . axis([ - 0.25 , 1.25 , - 0.25 , 1.25 ]) #plt.axhline(y=0, color= k ) #plt.axvline(x=0, color= k ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . grid( True ) plt . tight_layout() return X,Y,Color myclc = [ mediumslateblue , deepskyblue , firebrick , navy , green , darkgoldenrod ] clustering = AgglomerativeClustering(linkage = linkg,n_clusters = 6 ) t0 = time() clustering . fit(Xtsne) print ( %s : %.2f s % (linkg, time() - t0)) X,Y,Color = plot_clustering(Xtsne, clustering . labels_, %s linkage % linkg) plt . savefig( average/ + linkg + .png ) plt . show() average : 0.29s len (clustering . labels_) 2702 resultdf = ndf . copy(deep = True ) resultdf[ label ] = list (clustering . labels_) resultdf[ color ] = Color resultdf[ nmax ] = nmax resultdf[ X ] = X resultdf[ Y ] = Y resultdf . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD label color nmax X Y Protein alpha-1-antitrypsin 0.009000 0.120724 0.036423 0.047402 0.103137 0.023655 1 deepskyblue CHD 0.589498 0.753117 mothers_against_decapentaplegic_homolog_4 0.005316 0.080604 0.056011 0.010512 0.039842 0.000000 1 deepskyblue CHD 0.607031 0.740363 vascular_endothelial_growth_factor_b 0.000000 0.006129 0.073377 0.028607 0.121075 0.000000 4 green IHD 0.293398 0.498955 indoleamine_2,3-dioxygenase_1 0.000000 0.000000 0.000000 0.000000 0.034477 0.000000 5 darkgoldenrod IHD 0.949800 0.544562 thyroxine-binding_globulin 0.000000 0.036206 0.020516 0.006959 0.048815 0.024457 4 green IHD 0.399345 0.635551","title":"Results"},{"location":"tsne/tSNE-average/#add-uniprot","text":"data = pd . read_csv( uniprot.csv ) data . index = data[ protein ] udf = data . drop( protein ,axis = 1 ) udf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uniprot_id gene_names has_multiple_IDs protein natriuretic_peptides_b P16860 NPPB NaN angiotensin-converting_enzyme P12821 ACE DCP DCP1 NaN potassium_voltage-gated_channel_subfamily_h_member_2 Q12809 KCNH2 ERG ERG1 HERG NaN c-reactive_protein P02741 CRP PTX1 NaN apolipoprotein_e P02649 APOE NaN idx_cvd = list (resultdf . index) idx_uprt = list (udf . index) uprot = [] for item in idx_cvd: if item in idx_uprt: uprot . append(udf . loc[item,:][ 0 ]) else : print (item, Match not Found ) uprot . append( NAN ) collectrin Match not Found aldo-keto_reductase_family_1_member_c4 Match not Found macrophage_receptor_marco Match not Found zinc_fingers_and_homeoboxes_protein_2 Match not Found transcription_factor_sp3 Match not Found synaptotagmin-9 Match not Found e3_ubiquitin-protein_ligase_nrdp1 Match not Found nuclear_factor_erythroid_2-related_factor_1 Match not Found serine/threonine-protein_phosphatase_2b_catalytic_subunit_gamma_isoform Match not Found myocyte-specific_enhancer_factor_2a Match not Found neuropeptide_s_receptor Match not Found fidgetin Match not Found histone_deacetylase_6 Match not Found transcription_factor_sox-2 Match not Found voltage-gated_potassium_channel_subunit_beta-1 Match not Found segment_polarity_protein_dishevelled_homolog_dvl-3 Match not Found mitochondrial_intermediate_peptidase Match not Found gtp-binding_protein_rit1 Match not Found oligodendrocyte_transcription_factor_1 Match not Found ets_homologous_factor Match not Found troponin_t,_slow_skeletal_muscle Match not Found dna_repair_protein_rad51_homolog_2 Match not Found xin_actin-binding_repeat-containing_protein_1 Match not Found 3-ketoacyl-coa_thiolase,_peroxisomal Match not Found transcription_factor_sox-17 Match not Found cytochrome_p450_2b6 Match not Found cyclin-dependent_kinase_inhibitor_1 Match not Found leukotriene_b4_receptor_1 Match not Found trans-2,3-enoyl-coa_reductase-like Match not Found apoptosis-inducing_factor_1,_mitochondrial Match not Found frizzled-7 Match not Found cytohesin-2 Match not Found kinesin_heavy_chain_isoform_5a Match not Found phosphatidylcholine:ceramide_cholinephosphotransferase_1 Match not Found sulfiredoxin-1 Match not Found ubiquitin_carboxyl-terminal_hydrolase_cyld Match not Found ermin Match not Found wolframin Match not Found receptor-type_tyrosine-protein_phosphatase_beta Match not Found micos_complex_subunit_mic26 Match not Found neuropeptide_s Match not Found basic_helix-loop-helix_transcription_factor_scleraxis Match not Found transmembrane_6_superfamily_member_2 Match not Found dna_repair-scaffolding_protein Match not Found zinc_finger_protein_260 Match not Found retinoic_acid_receptor_rxr-gamma Match not Found electrogenic_sodium_bicarbonate_cotransporter_1 Match not Found zinc_finger_e-box-binding_homeobox_1 Match not Found beta-adrenergic_receptor_kinase_2 Match not Found adp-ribosylation_factor_6 Match not Found uroplakin-3a Match not Found endoplasmic_reticulum_resident_protein_44 Match not Found angiopoietin-related_protein_1 Match not Found tgf-beta-activated_kinase_1_and_map3k7-binding_protein_1 Match not Found rna-binding_protein_10 Match not Found dna-binding_protein_satb1 Match not Found cytosine-5)-methyltransferase_1 Match not Found laminin_subunit_gamma-1 Match not Found anthrax_toxin_receptor_2 Match not Found protein_kinase_c_zeta_type Match not Found cyclin-dependent_kinase_7 Match not Found potassium_channel_subfamily_k_member_2 Match not Found cytosolic_phospholipase_a2_gamma Match not Found aquaporin-3 Match not Found hyaluronidase_ph-20 Match not Found crk-like_protein Match not Found protein_kinase_c_eta_type Match not Found angiopoietin-like_protein_8 Match not Found fibronectin_type_iii_domain-containing_protein_5 Match not Found voltage-dependent_l-type_calcium_channel_subunit_beta-2 Match not Found eukaryotic_elongation_factor_2_kinase Match not Found voltage-gated_potassium_channel_subunit_beta-2 Match not Found complement_c1q_subcomponent_subunit_a Match not Found glutamate_receptor_ionotropic,_delta-2 Match not Found bromodomain-containing_protein_7 Match not Found cytosine-5)-methyltransferase_3b Match not Found ovarian_cancer_g-protein_coupled_receptor_1 Match not Found anoctamin-6 Match not Found complement_c4-b Match not Found apoptosis_regulator_bcl-2 Match not Found succinate_receptor_1 Match not Found aldo-keto_reductase_family_1_member_b15 Match not Found teratocarcinoma-derived_growth_factor_1 Match not Found s-phase_kinase-associated_protein_1 Match not Found talin-2 Match not Found transcription_factor_mafb Match not Found protein_kinase_c_iota_type Match not Found t-box_transcription_factor_tbx20 Match not Found nuclear_receptor_coactivator_7 Match not Found regulator_of_g-protein_signaling_12 Match not Found g-protein_coupled_receptor_family_c_group_6_member_a Match not Found multiple_pdz_domain_protein Match not Found b-cell_lymphoma_3_protein Match not Found bromodomain-containing_protein_4 Match not Found protein_kinase_c_theta_type Match not Found cdgsh_iron-sulfur_domain-containing_protein_1 Match not Found caspase_recruitment_domain-containing_protein_9 Match not Found interleukin-10_receptor_subunit_alpha Match not Found serine/threonine-protein_kinase_d3 Match not Found ubiquitin-like_protein_isg15 Match not Found e3_ubiquitin-protein_ligase_pellino_homolog_1 Match not Found protein_max Match not Found sonic_hedgehog_protein Match not Found matrilin-2 Match not Found src_kinase-associated_phosphoprotein_2 Match not Found toll-like_receptor_5 Match not Found mixed_lineage_kinase_domain-like_protein Match not Found cyclic_amp-dependent_transcription_factor_atf-6_alpha Match not Found proteasome_activator_complex_subunit_2 Match not Found atp-dependent_dna_helicase_q1 Match not Found sphingosine_1-phosphate_receptor_3 Match not Found protein_phosphatase_1g Match not Found hyaluronidase-3 Match not Found serine/threonine-protein_kinase_d2 Match not Found dual_oxidase_2 Match not Found resultdf[ uprot ] = uprot resultdf . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD label color nmax X Y uprot Protein alpha-1-antitrypsin 0.009000 0.120724 0.036423 0.047402 0.103137 0.023655 1 deepskyblue CHD 0.589498 0.753117 P01009 mothers_against_decapentaplegic_homolog_4 0.005316 0.080604 0.056011 0.010512 0.039842 0.000000 1 deepskyblue CHD 0.607031 0.740363 Q13485 vascular_endothelial_growth_factor_b 0.000000 0.006129 0.073377 0.028607 0.121075 0.000000 4 green IHD 0.293398 0.498955 P49765 indoleamine_2,3-dioxygenase_1 0.000000 0.000000 0.000000 0.000000 0.034477 0.000000 5 darkgoldenrod IHD 0.949800 0.544562 P14902 thyroxine-binding_globulin 0.000000 0.036206 0.020516 0.006959 0.048815 0.024457 4 green IHD 0.399345 0.635551 P05543 smoothelin 0.000000 0.010463 0.000000 0.020742 0.092131 0.013016 4 green IHD 0.262940 0.600811 P53814 short-chain_specific_acyl-coa_dehydrogenase,_mitochondrial 0.000000 0.000000 0.023999 0.000000 0.022102 0.000000 2 firebrick CM 0.658054 0.193892 P16219 protein_tyrosine_phosphatase_type_iva_1 0.030680 0.000000 0.022076 0.000000 0.000000 0.000000 1 deepskyblue ARR 0.580443 0.468921 Q93096 creatine_kinase_u-type,_mitochondrial 0.007622 0.000000 0.017123 0.000000 0.341386 0.000000 4 green IHD 0.147972 0.526543 P12532 c-c_chemokine_receptor_type_5 0.000000 0.000000 0.019980 0.050426 0.000000 0.000000 1 deepskyblue CVA 0.501680 0.598933 P51681 resultdf . to_csv( average/ + linkg + .csv ) df0 = resultdf[resultdf[ label ] == 0 ] df0 . to_csv( average/ + linkg + 0.csv ) df1 = resultdf[resultdf[ label ] == 1 ] df1 . to_csv( average/ + linkg + 1.csv ) df2 = resultdf[resultdf[ label ] == 2 ] df2 . to_csv( average/ + linkg + 2.csv ) df3 = resultdf[resultdf[ label ] == 3 ] df3 . to_csv( average/ + linkg + 3.csv ) df4 = resultdf[resultdf[ label ] == 4 ] df4 . to_csv( average/ + linkg + 4.csv ) df5 = resultdf[resultdf[ label ] == 5 ] df5 . to_csv( average/ + linkg + 5.csv ) df0 . shape (519, 12)","title":"Add UNiprot"},{"location":"tsne/tSNE-average/#seperate-cluster-plots","text":"plt . figure(figsize = [ 22 , 17 ]) plt . subplot( 2 , 3 , 1 ) plt . scatter(df0[ X ],df0[ Y ],color = df0[ color ]) plt . title( Cluster-label-0 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 2 ) plt . scatter(df1[ X ],df1[ Y ],color = df1[ color ]) plt . title( Cluster-label-1 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 3 ) plt . scatter(df2[ X ],df2[ Y ],color = df2[ color ]) plt . title( Cluster-label-2 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 4 ) plt . scatter(df3[ X ],df3[ Y ],color = df3[ color ]) plt . title( Cluster-label-3 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 5 ) plt . scatter(df4[ X ],df4[ Y ],color = df4[ color ]) plt . title( Cluster-label-4 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . subplot( 2 , 3 , 6 ) plt . scatter(df5[ X ],df5[ Y ],color = df5[ color ]) plt . title( Cluster-label-5 ) plt . xlabel( dimension 1 , fontsize = 20 ) plt . ylabel( dimension 2 ,fontsize = 20 ) plt . savefig( average/ + linkg + -subplot.png ) plt . show()","title":"Seperate cluster plots"},{"location":"tsne/tSNE-average/#cluster-in-deep","text":"resultdf . groupby( label ) . count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD color nmax X Y uprot label 0 519 519 519 519 519 519 519 519 519 519 519 1 676 676 676 676 676 676 676 676 676 676 676 2 287 287 287 287 287 287 287 287 287 287 287 3 112 112 112 112 112 112 112 112 112 112 112 4 911 911 911 911 911 911 911 911 911 911 911 5 197 197 197 197 197 197 197 197 197 197 197 df0 . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ARR CHD CM CVA IHD VD label color nmax X Y uprot Protein sh3_and_multiple_ankyrin_repeat_domains_protein_3 0.00000 0.000000 0.000000 0.039151 0.000000 0.000000 0 mediumslateblue CVA 0.654033 0.959608 Q9BYB0 endophilin-b1 0.00000 0.000000 0.000000 0.043899 0.000000 0.000000 0 mediumslateblue CVA 0.516064 0.954616 Q9Y371 a_disintegrin_and_metalloproteinase_with_thrombospondin_motifs_12 0.00000 0.000000 0.000000 0.000000 0.062291 0.000000 0 mediumslateblue IHD 0.230749 0.748474 P58397 cytokine_receptor_common_subunit_gamma 0.00000 0.000000 0.000000 0.039151 0.000000 0.000000 0 mediumslateblue CVA 0.661077 0.969435 P31785 myelin_basic_protein 0.00000 0.007666 0.002121 0.349511 0.003877 0.003344 0 mediumslateblue CVA 0.352625 0.888755 P02686 phosphate-regulating_neutral_endopeptidase 0.00000 0.000000 0.000000 0.000000 0.062291 0.000000 0 mediumslateblue IHD 0.226173 0.747243 P78562 protein_z-dependent_protease_inhibitor 0.00000 0.000000 0.000000 0.065516 0.032020 0.000000 0 mediumslateblue CVA 0.432626 0.801987 Q9UK55 ste20-like_serine/threonine-protein_kinase 0.00000 0.000000 0.000000 0.040786 0.000000 0.000000 0 mediumslateblue CVA 0.545912 0.965827 Q9H2G2 tissue-type_plasminogen_activator 0.04285 0.013719 0.009599 0.330811 0.231620 0.006301 0 mediumslateblue CVA 0.335407 0.886174 P00750 interleukin-20_receptor_subunit_alpha 0.00000 0.000000 0.000000 0.039151 0.000000 0.000000 0 mediumslateblue CVA 0.653468 0.972735 Q9UHF4 df0[ 0 : 50 ] . drop([ X , Y , label ],axis = 1 ) . plot . barh(stacked = True ,figsize = ( 16 , 25 ),fontsize = 15 ) matplotlib.axes._subplots.AxesSubplot at 0x7f93a8b73358","title":"Cluster In deep"},{"location":"uniprot/uniprot/","text":"Collecting Proteins and Genes data from Uniprot Database What is Uniprot Database? UniProt is a freely accessible database of protein sequence and functional information, many entries being derived from genome sequencing projects. It contains a large amount of information about the biological function of proteins derived from the research literature. source: Wikipedia Why is this protein list important to current COVID-19 Dataset? Inorder to gather the information about the biomolecular mechanism from the scientific literature (COVID-19 Dataset), one need to have the list of associated Proteins, Genes, Pathways, Drugs etc. This notebook presents the steps to gather Corona Virus associated proteins, Gene names and associated Pathways from Uniprot database. These lits could be useful to look at the textual documents for further NLP processing and to present the entity relationship. 1. Getting Data Step -I Gp to Uniprot Database (https://www.uniprot.org/) and select UniprotKB in search bar. Then inter corona virus into the search bar. Step -II: After you hit search operation, you will get a table like disply of the result. It is multi page table. Step-III: Look at the right most task bar of this table. You can see pen like icon through which you get next window. You can make a selection of the information you want to gather (e.g., Name, Gene, Pathways). Step - IV Once you are done with selection of information, you can go back to previous table and hit download button. You can select the format of the data. Excel file download is one option. 2. Data Wrangling What After getting Protein Data? Lets play around with this data import pandas as pd import matplotlib.pyplot as plt % matplotlib inline file_path = ../input/corona-virus-proteins-from-uniprot-database/corona.csv df = pd . read_csv(file_path) df . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entry Entry name Status Protein names Gene names Organism Virus hosts Pathway 0 A0A3R5SMJ6 A0A3R5SMJ6_WNV unreviewed Genome polyprotein NaN West Nile virus (WNV) Aedes [TaxID: 7158]; Amblyomma variegatum (Tro... NaN 1 M1UFP6 M1UFP6_9FLAV unreviewed Genome polyprotein NaN Bovine viral diarrhea virus 1b NaN NaN 2 P11223 SPIKE_IBVB reviewed Spike glycoprotein (S glycoprotein) (E2) (Pepl... S 2 Avian infectious bronchitis virus (strain Beau... Gallus gallus (Chicken) [TaxID: 9031] NaN 3 P11224 SPIKE_CVMA5 reviewed Spike glycoprotein (S glycoprotein) (E2) (Pepl... S 3 Murine coronavirus (strain A59) (MHV-A59) (Mur... Mus musculus (Mouse) [TaxID: 10090] NaN 4 P0C6X9 R1AB_CVMA5 reviewed Replicase polyprotein 1ab (pp1ab) (ORF1ab poly... rep 1a-1b Murine coronavirus (strain A59) (MHV-A59) (Mur... Mus musculus (Mouse) [TaxID: 10090] NaN There are total 21,876 proteins from different sources df . shape (21876, 8) Q: What are the different Organisms? Can you find the top 50 Organisms? df_organism = pd . DataFrame(df . groupby( Organism ) . count()[ Entry ]) df_organism = df_organism . sort_values(by = Entry , ascending = False ) df_organism[ 0 : 20 ] . plot . barh(figsize = [ 15 , 10 ], fontsize = 20 ) plt . gca() . invert_yaxis() df_organism[ 0 : 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entry Organism Infectious bronchitis virus 8184 Porcine epidemic diarrhea virus 4657 Middle East respiratory syndrome-related coronavirus 1139 Feline coronavirus 983 Human coronavirus OC43 (HCoV-OC43) 630 Avian coronavirus 497 Canine coronavirus 463 Human coronavirus NL63 (HCoV-NL63) 333 Transmissible gastroenteritis virus 258 Porcine deltacoronavirus 235 Bovine coronavirus 225 Human coronavirus 229E (HCoV-229E) 210 Murine hepatitis virus 111 Human coronavirus HKU1 (HCoV-HKU1) 110 Porcine hemagglutinating encephalomyelitis virus 104 Porcine respiratory coronavirus 98 Alphacoronavirus sp. 90 Human immunodeficiency virus 1 81 Bat SARS-like coronavirus 66 Hepatitis B virus (HBV) 66 Q: What are the different Viral hosts? Can you find top Virus hosts? df[ Virus hosts ] = df[ Virus hosts ] . apply( lambda x: str (x)[ 0 : 50 ] ) df_host = pd . DataFrame(df . groupby( Virus hosts ) . count()[ Entry ]) df_host = df_host . sort_values(by = Entry , ascending = False ) df_host[ 1 : 20 ] . plot . barh(figsize = [ 15 , 10 ], fontsize = 20 ) plt . gca() . invert_yaxis() df_host[ 1 : 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entry Virus hosts Homo sapiens (Human) [TaxID: 9606] 1385 Bos taurus (Bovine) [TaxID: 9913] 78 Homo sapiens (Human) [TaxID: 9606]; Pan troglodyte 66 Gallus gallus (Chicken) [TaxID: 9031] 64 Sus scrofa (Pig) [TaxID: 9823] 57 Mus musculus (Mouse) [TaxID: 10090] 54 Meleagris gallopavo (Wild turkey) [TaxID: 9103] 51 Homo sapiens (Human) [TaxID: 9606]; Paguma larvata 50 Alliaria petiolata (Garlic mustard) (Arabis petiol 33 Pipistrellus abramus (Japanese pipistrelle) (Pipis 32 Tylonycteris pachypus (Lesser bamboo bat) [TaxID: 19 Canis lupus familiaris (Dog) (Canis familiaris) [T 16 Rattus norvegicus (Rat) [TaxID: 10116] 11 Rousettus leschenaultii (Leschenault's rousette) [ 11 Equus caballus (Horse) [TaxID: 9796] 9 Felidae (cat family) [TaxID: 9681] 9 Impatiens [TaxID: 35939] 9 Scotophilus kuhlii (Lesser asiatic yellow bat) [Ta 7 Rhinolophus sinicus (Chinese rufous horseshoe bat) 7 3. Cleaning Protein Names, Synonyms and abbreviations def filter (line): proteins = set () line = str (line) line = line . lower() for lines without () or [] terms if ( not in line or [ not in line: proteins . add(line . strip() . replace( , _ )) for line including () terms if ( in line: start = 0 open_in = line . find( ( ) tmp = line[start:open_in] . strip() . replace( , _ ) proteins . add(tmp) while open_in = 0 : start = open_in + 1 end = line . find( ) , start) proteins . add(line[start:end] . strip() . replace( , _ )) open_in = line . find( ( , end) for lines including [] trems if [ in line: raw = line[line . find( [ ):line . find( ] )] #print( THIS IS RAW: , raw[15:-1]) raw = raw[ 15 : - 1 ] lraw = raw . split( ; ) for item in lraw: #print(item) if ( in item: start = 0 open_in = item . find( ( ) tmp = item[start:open_in] . strip() . replace( , _ ) proteins . add(tmp) else : proteins . add(item . strip() . replace( , _ )) return proteins allProteins = [] i = 0 for u,p in zip (df[ Entry ],df[ Protein names ]): print (u, | ,p) print ( ------------ ) print (u, | , filter (p)) print ( =================================================== ) i += 1 if i 4 : break A0A3R5SMJ6 | Genome polyprotein ------------ A0A3R5SMJ6 | { genome_polyprotein } =================================================== M1UFP6 | Genome polyprotein ------------ M1UFP6 | { genome_polyprotein } =================================================== P11223 | Spike glycoprotein (S glycoprotein) (E2) (Peplomer protein) [Cleaved into: Spike protein S1; Spike protein S2; Spike protein S2 ] ------------ P11223 | { peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein } =================================================== P11224 | Spike glycoprotein (S glycoprotein) (E2) (Peplomer protein) [Cleaved into: Spike protein S1; Spike protein S2; Spike protein S2 ] ------------ P11224 | { peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein } =================================================== allProteins = [] for u,p in zip (df[ Entry ],df[ Protein names ]): allProteins . append({ id :u, names : list ( filter (p))}) allProteins[ 0 : 5 ] [{ id : A0A3R5SMJ6 , names : [ genome_polyprotein ]}, { id : M1UFP6 , names : [ genome_polyprotein ]}, { id : P11223 , names : [ peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein ]}, { id : P11224 , names : [ peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein ]}, { id : P0C6X9 , names : [ m-pro , nsp16 , nsp7 , exon , nsp14 , nsp10 , guanine-n7_methyltransferase , ec_3.4.22.- , nendou , non-structural_protein_3 , non-structural_protein_2 , ec_2.1.1.- , 2 -o-methyltransferase , hel , pol , ec_3.6.4.13 , growth_factor-like_peptide , non-structural_protein_7 , gfl , ec_3.4.22.69 , p22 , p27 , non-structural_protein_9 , orf1ab_polyprotein , nsp4 , uridylate-specific_endoribonuclease , 3cl-pro , pp1ab , p65 , host_translation_inhibitor_nsp1 , p15 , nsp1 , ec_3.6.4.12 , nsp2 , ec_2.7.7.48 , p67 , nsp12 , peptide_hd2 , nsp5 , p210 , rdrp , nsp9 , p100 , nsp3 , nsp8 , non-structural_protein_6 , rna-directed_rna_polymerase , p35 , 3c-like_proteinase , papain-like_proteinase , ec_3.1.13.- , non-structural_protein_4 , pl-pro , 3clp , p12 , non-structural_protein_10 , p28 , non-structural_protein_8 , helicase , nsp6 , nsp13 , nsp15 , p44 , ec_3.1.-.- , replicase_polyprotein_1ab , ec_3.4.19.12 , p10 ]}] import json with open ( virus-proteins.json , w ) as fn: json . dump(allProteins,fn)","title":"Uniprot"},{"location":"uniprot/uniprot/#collecting-proteins-and-genes-data-from-uniprot-database","text":"What is Uniprot Database? UniProt is a freely accessible database of protein sequence and functional information, many entries being derived from genome sequencing projects. It contains a large amount of information about the biological function of proteins derived from the research literature. source: Wikipedia Why is this protein list important to current COVID-19 Dataset? Inorder to gather the information about the biomolecular mechanism from the scientific literature (COVID-19 Dataset), one need to have the list of associated Proteins, Genes, Pathways, Drugs etc. This notebook presents the steps to gather Corona Virus associated proteins, Gene names and associated Pathways from Uniprot database. These lits could be useful to look at the textual documents for further NLP processing and to present the entity relationship.","title":"Collecting Proteins and Genes data from Uniprot Database"},{"location":"uniprot/uniprot/#1-getting-data","text":"","title":"1. Getting Data"},{"location":"uniprot/uniprot/#step-i","text":"Gp to Uniprot Database (https://www.uniprot.org/) and select UniprotKB in search bar. Then inter corona virus into the search bar.","title":"Step -I"},{"location":"uniprot/uniprot/#step-ii","text":"After you hit search operation, you will get a table like disply of the result. It is multi page table.","title":"Step -II:"},{"location":"uniprot/uniprot/#step-iii","text":"Look at the right most task bar of this table. You can see pen like icon through which you get next window. You can make a selection of the information you want to gather (e.g., Name, Gene, Pathways).","title":"Step-III:"},{"location":"uniprot/uniprot/#step-iv","text":"Once you are done with selection of information, you can go back to previous table and hit download button. You can select the format of the data. Excel file download is one option.","title":"Step - IV"},{"location":"uniprot/uniprot/#2-data-wrangling","text":"","title":"2. Data Wrangling"},{"location":"uniprot/uniprot/#what-after-getting-protein-data","text":"Lets play around with this data import pandas as pd import matplotlib.pyplot as plt % matplotlib inline file_path = ../input/corona-virus-proteins-from-uniprot-database/corona.csv df = pd . read_csv(file_path) df . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entry Entry name Status Protein names Gene names Organism Virus hosts Pathway 0 A0A3R5SMJ6 A0A3R5SMJ6_WNV unreviewed Genome polyprotein NaN West Nile virus (WNV) Aedes [TaxID: 7158]; Amblyomma variegatum (Tro... NaN 1 M1UFP6 M1UFP6_9FLAV unreviewed Genome polyprotein NaN Bovine viral diarrhea virus 1b NaN NaN 2 P11223 SPIKE_IBVB reviewed Spike glycoprotein (S glycoprotein) (E2) (Pepl... S 2 Avian infectious bronchitis virus (strain Beau... Gallus gallus (Chicken) [TaxID: 9031] NaN 3 P11224 SPIKE_CVMA5 reviewed Spike glycoprotein (S glycoprotein) (E2) (Pepl... S 3 Murine coronavirus (strain A59) (MHV-A59) (Mur... Mus musculus (Mouse) [TaxID: 10090] NaN 4 P0C6X9 R1AB_CVMA5 reviewed Replicase polyprotein 1ab (pp1ab) (ORF1ab poly... rep 1a-1b Murine coronavirus (strain A59) (MHV-A59) (Mur... Mus musculus (Mouse) [TaxID: 10090] NaN There are total 21,876 proteins from different sources df . shape (21876, 8)","title":"What After getting Protein Data?"},{"location":"uniprot/uniprot/#q-what-are-the-different-organisms-can-you-find-the-top-50-organisms","text":"df_organism = pd . DataFrame(df . groupby( Organism ) . count()[ Entry ]) df_organism = df_organism . sort_values(by = Entry , ascending = False ) df_organism[ 0 : 20 ] . plot . barh(figsize = [ 15 , 10 ], fontsize = 20 ) plt . gca() . invert_yaxis() df_organism[ 0 : 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entry Organism Infectious bronchitis virus 8184 Porcine epidemic diarrhea virus 4657 Middle East respiratory syndrome-related coronavirus 1139 Feline coronavirus 983 Human coronavirus OC43 (HCoV-OC43) 630 Avian coronavirus 497 Canine coronavirus 463 Human coronavirus NL63 (HCoV-NL63) 333 Transmissible gastroenteritis virus 258 Porcine deltacoronavirus 235 Bovine coronavirus 225 Human coronavirus 229E (HCoV-229E) 210 Murine hepatitis virus 111 Human coronavirus HKU1 (HCoV-HKU1) 110 Porcine hemagglutinating encephalomyelitis virus 104 Porcine respiratory coronavirus 98 Alphacoronavirus sp. 90 Human immunodeficiency virus 1 81 Bat SARS-like coronavirus 66 Hepatitis B virus (HBV) 66","title":"Q: What are the different Organisms? Can you find the top 50 Organisms?"},{"location":"uniprot/uniprot/#q-what-are-the-different-viral-hosts-can-you-find-top-virus-hosts","text":"df[ Virus hosts ] = df[ Virus hosts ] . apply( lambda x: str (x)[ 0 : 50 ] ) df_host = pd . DataFrame(df . groupby( Virus hosts ) . count()[ Entry ]) df_host = df_host . sort_values(by = Entry , ascending = False ) df_host[ 1 : 20 ] . plot . barh(figsize = [ 15 , 10 ], fontsize = 20 ) plt . gca() . invert_yaxis() df_host[ 1 : 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entry Virus hosts Homo sapiens (Human) [TaxID: 9606] 1385 Bos taurus (Bovine) [TaxID: 9913] 78 Homo sapiens (Human) [TaxID: 9606]; Pan troglodyte 66 Gallus gallus (Chicken) [TaxID: 9031] 64 Sus scrofa (Pig) [TaxID: 9823] 57 Mus musculus (Mouse) [TaxID: 10090] 54 Meleagris gallopavo (Wild turkey) [TaxID: 9103] 51 Homo sapiens (Human) [TaxID: 9606]; Paguma larvata 50 Alliaria petiolata (Garlic mustard) (Arabis petiol 33 Pipistrellus abramus (Japanese pipistrelle) (Pipis 32 Tylonycteris pachypus (Lesser bamboo bat) [TaxID: 19 Canis lupus familiaris (Dog) (Canis familiaris) [T 16 Rattus norvegicus (Rat) [TaxID: 10116] 11 Rousettus leschenaultii (Leschenault's rousette) [ 11 Equus caballus (Horse) [TaxID: 9796] 9 Felidae (cat family) [TaxID: 9681] 9 Impatiens [TaxID: 35939] 9 Scotophilus kuhlii (Lesser asiatic yellow bat) [Ta 7 Rhinolophus sinicus (Chinese rufous horseshoe bat) 7","title":"Q: What are the different Viral hosts? Can you find top Virus hosts?"},{"location":"uniprot/uniprot/#3-cleaning-protein-names-synonyms-and-abbreviations","text":"def filter (line): proteins = set () line = str (line) line = line . lower() for lines without () or [] terms if ( not in line or [ not in line: proteins . add(line . strip() . replace( , _ )) for line including () terms if ( in line: start = 0 open_in = line . find( ( ) tmp = line[start:open_in] . strip() . replace( , _ ) proteins . add(tmp) while open_in = 0 : start = open_in + 1 end = line . find( ) , start) proteins . add(line[start:end] . strip() . replace( , _ )) open_in = line . find( ( , end) for lines including [] trems if [ in line: raw = line[line . find( [ ):line . find( ] )] #print( THIS IS RAW: , raw[15:-1]) raw = raw[ 15 : - 1 ] lraw = raw . split( ; ) for item in lraw: #print(item) if ( in item: start = 0 open_in = item . find( ( ) tmp = item[start:open_in] . strip() . replace( , _ ) proteins . add(tmp) else : proteins . add(item . strip() . replace( , _ )) return proteins allProteins = [] i = 0 for u,p in zip (df[ Entry ],df[ Protein names ]): print (u, | ,p) print ( ------------ ) print (u, | , filter (p)) print ( =================================================== ) i += 1 if i 4 : break A0A3R5SMJ6 | Genome polyprotein ------------ A0A3R5SMJ6 | { genome_polyprotein } =================================================== M1UFP6 | Genome polyprotein ------------ M1UFP6 | { genome_polyprotein } =================================================== P11223 | Spike glycoprotein (S glycoprotein) (E2) (Peplomer protein) [Cleaved into: Spike protein S1; Spike protein S2; Spike protein S2 ] ------------ P11223 | { peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein } =================================================== P11224 | Spike glycoprotein (S glycoprotein) (E2) (Peplomer protein) [Cleaved into: Spike protein S1; Spike protein S2; Spike protein S2 ] ------------ P11224 | { peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein } =================================================== allProteins = [] for u,p in zip (df[ Entry ],df[ Protein names ]): allProteins . append({ id :u, names : list ( filter (p))}) allProteins[ 0 : 5 ] [{ id : A0A3R5SMJ6 , names : [ genome_polyprotein ]}, { id : M1UFP6 , names : [ genome_polyprotein ]}, { id : P11223 , names : [ peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein ]}, { id : P11224 , names : [ peplomer_protein , spike_protein_s1 , e2 , spike_protein_s2 , spike_glycoprotein , s_glycoprotein ]}, { id : P0C6X9 , names : [ m-pro , nsp16 , nsp7 , exon , nsp14 , nsp10 , guanine-n7_methyltransferase , ec_3.4.22.- , nendou , non-structural_protein_3 , non-structural_protein_2 , ec_2.1.1.- , 2 -o-methyltransferase , hel , pol , ec_3.6.4.13 , growth_factor-like_peptide , non-structural_protein_7 , gfl , ec_3.4.22.69 , p22 , p27 , non-structural_protein_9 , orf1ab_polyprotein , nsp4 , uridylate-specific_endoribonuclease , 3cl-pro , pp1ab , p65 , host_translation_inhibitor_nsp1 , p15 , nsp1 , ec_3.6.4.12 , nsp2 , ec_2.7.7.48 , p67 , nsp12 , peptide_hd2 , nsp5 , p210 , rdrp , nsp9 , p100 , nsp3 , nsp8 , non-structural_protein_6 , rna-directed_rna_polymerase , p35 , 3c-like_proteinase , papain-like_proteinase , ec_3.1.13.- , non-structural_protein_4 , pl-pro , 3clp , p12 , non-structural_protein_10 , p28 , non-structural_protein_8 , helicase , nsp6 , nsp13 , nsp15 , p44 , ec_3.1.-.- , replicase_polyprotein_1ab , ec_3.4.19.12 , p10 ]}] import json with open ( virus-proteins.json , w ) as fn: json . dump(allProteins,fn)","title":"3. Cleaning Protein Names, Synonyms and abbreviations"}]}