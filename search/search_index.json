{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CaseOLAP Platform We are greatly excited with the new enthusiasm to introduce the Context-aware Semantic Online Analytical Processing pipeline (CaseOLAP), developed in 2016. The rapidly accumulating quantity of biomedical textual data has far exceeded the human capacity of manual curation and analysis, necessitating novel text-mining tools to extract biological insights from large volumes of scientific reports.CaseOLAP successfully quantifies user-defined phrase-category relationships through analysis of textual data. This online documents has been prepared for caseOLAP platform for coding tools development and implementation. We have developed a protocol for the complete CaseOLAP platform, including data preprocessing (i.e., downloading and parsing text documents), indexing and searching with Elasticsearch, creating a functional document structure called Text-Cube and quantifying phrase-category relationships using the core CaseOLAP algorithm. Data preprocessing generates key-value pairs for all documents involved. As an example, a key may refer to the document PMID, while a value may refer to different document metadata. Preprocessed data is rearranged by indexing and searching for an entity count, which further facilitates the CaseOLAP score calculation. Obtained raw CaseOLAP results can be taken to integrative analysis including dimensionality reduction, clustering, temporal and geographical analysis, as well as the creation of a graphical database which enables semantic mapping of the documents . CaseOLAP defines phrase-category relationships in an accurate (pinpoints relationships), consistent (highly reproducible), and efficient manner (processes 100,000 words/sec). Following our protocol, one can build up a cloud-computing environment supporting CaseOLAP which offers enhanced accessibility and affords grand opportunities to empower the biomedical community with phrase-mining tools for widespread research applications Read more . image source: www.jove.com Figure : Counting number of decendent nodes in each categories of ICD 11 code trees.","title":"Home"},{"location":"#caseolap-platform","text":"We are greatly excited with the new enthusiasm to introduce the Context-aware Semantic Online Analytical Processing pipeline (CaseOLAP), developed in 2016. The rapidly accumulating quantity of biomedical textual data has far exceeded the human capacity of manual curation and analysis, necessitating novel text-mining tools to extract biological insights from large volumes of scientific reports.CaseOLAP successfully quantifies user-defined phrase-category relationships through analysis of textual data. This online documents has been prepared for caseOLAP platform for coding tools development and implementation. We have developed a protocol for the complete CaseOLAP platform, including data preprocessing (i.e., downloading and parsing text documents), indexing and searching with Elasticsearch, creating a functional document structure called Text-Cube and quantifying phrase-category relationships using the core CaseOLAP algorithm. Data preprocessing generates key-value pairs for all documents involved. As an example, a key may refer to the document PMID, while a value may refer to different document metadata. Preprocessed data is rearranged by indexing and searching for an entity count, which further facilitates the CaseOLAP score calculation. Obtained raw CaseOLAP results can be taken to integrative analysis including dimensionality reduction, clustering, temporal and geographical analysis, as well as the creation of a graphical database which enables semantic mapping of the documents . CaseOLAP defines phrase-category relationships in an accurate (pinpoints relationships), consistent (highly reproducible), and efficient manner (processes 100,000 words/sec). Following our protocol, one can build up a cloud-computing environment supporting CaseOLAP which offers enhanced accessibility and affords grand opportunities to empower the biomedical community with phrase-mining tools for widespread research applications Read more . image source: www.jove.com Figure : Counting number of decendent nodes in each categories of ICD 11 code trees.","title":"CaseOLAP Platform"},{"location":"CaseOLAP/","text":"CaseOLAP score calculation CaseOLAP score calculation : CaseOLAP score are the quantification of user defined entity-category association. It start with the text-cube document structure and finds the entity in each document in each cell of the text-cube and by implementing updated text-cube metadata, it calculates the CaseOLAP score with following steps: Integrity : Integrity of user defined phrase is taken to be 1. (Autophrase, Segphrase) Popularity : It depends on how frequently a protein name is mentioned within one category, and it is calculated only using the statistics from the cells of documents pertaining to that individual category. Rare protein names in a cell are ranked low, while an increase in their frequency of mention has a diminishing return. - In each of the cell c in text-cube, term frequency tf(p,c) of each entity is calculated. - Using individual term frequency for each entity(protein), total sum of the term frequency cntP(c) is calculated. - A normalized popularity of phrase p in cell c , pop(p,c) is calculated by using tf(p,c) and ntfP(c) calculated in 6.2.1 and 6.2.2. [eq ref]. Distinctiveness : It is based on the relevance of a entity name to a specific category by comparing the occurrence of the protein name in the target data set, i.e., the cell documents describing one cell, to the contrastive data set, i.e., the cells of documents describing the remaining cells. - Normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] at 5.4.3 are used to calculated the relevance score rel(p,c) [eq. ref] of protein p in cell c . - Normalized distinctiveness disti(p,c) [eq ref] is calculated by using relevance score of protein p in cell c and and relevance across neighbouring cells c\u2019 (c\u2019 K(p,c) : neighbourhoods of cell c ). CaseOLAP score : It is the product of Integrity, Popularity and Distinctiveness calculated in 6.1,6.2 and 6.3. Import required libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline import json Data with open ( 'input/cat2pmids.json' , 'r' ) as f: cvd2pmids = json . load(f) print ( 'total cat2pmids:' , len (cat2pmids)) with open ( 'input/pmid2pcount.json' , 'r' ) as f: pmid2pcount = json . load(f) print ( 'total pmid2pcount:' , len (pmid2pcount)) Caseolap Score Calculation class Caseolap ( object ): def __init__ ( self ,cvd2pmids,pmid2pcount): self . cellnames = [] self . cvd2pmids = cvd2pmids self . pmid2pcount = pmid2pcount self . cell_pmids = {} self . cell_pmid2pcount = {} self . all_proteins = [] self . cell_uniqp = {} self . cell_p2tf = {} self . cell_tf = {} self . cell_cntp = {} self . cell_pop = {} self . cell_p2pmid = {} self . cell_ntf = {} self . cell_ndf = {} self . cell_rel = {} self . cell_dist = {} self . cell_caseolap = {} def df_builder ( self ,cell_quant,fname): flatdata = [] for p in self . all_proteins: d = { 'protein' :p} for name in self . cellnames: d . update({name:cell_quant[name][p]}) flatdata . append(d) df = pd . DataFrame(flatdata) df = df . set_index( 'protein' ) df . to_csv( 'data/' + fname + '.csv' ) return df def dump_json ( self ,data,fname): with open ( 'data/' + fname + '.json' , 'w' ) as dl: json . dump(data, dl) def cell_pmids_collector ( self , dump = False ,verbose = False ): for key,value in self . cvd2pmids . items(): cell_name = key cell_pmids = value self . cellnames . append(cell_name) self . cell_pmids . update({cell_name:cell_pmids}) if verbose: print ( 'total pmids collected for cell - ' ,cell_name, len (cell_pmids)) if dump: self . dump_json( self . cell_pmids,fname = 'cellpmids' ) def cell_pmid2pcount_collector ( self ): for key,value in self . cell_pmids . items(): cell_name = key cell_pmids = value ipmid2pcount = {} for pmid in cell_pmids: pmid_pcount = self . pmid2pcount[pmid] ipmid2pcount . update({pmid:pmid_pcount}) self . cell_pmid2pcount . update({cell_name:ipmid2pcount}) def all_protein_finder ( self ,dump = False ,verbose = False ): allproteins = [] for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value cellproteins = [] for key, value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key,value in pmid_pcount . items(): allproteins . append(key) cellproteins . append(key) uprotein = list ( set (cellproteins)) self . cell_uniqp . update({cell_name:uprotein}) if verbose: print ( 'total proteins collected for cell - ' ,cell_name, len (uprotein)) self . all_proteins = list ( set (allproteins)) if verbose: print ( 'total proteins collected: ' , len ( self . all_proteins)) if dump: self . dump_json( self . all_proteins,fname = 'allproteins' ) self . dump_json( self . cell_uniqp,fname = 'unique_proteins' ) def cell_map ( self ,cellpmid2pcount,select): map_dict = [] for key,value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key, value in pmid_pcount . items(): if select == 'tf' : map_dict . append({ 'protein' : key, 'tf' : int (value)}) elif select == 'pmid' : map_dict . append({ 'protein' : key, 'pmid' :pmid}) return map_dict def cell_reduce ( self ,Dict,col,operation): df = pd . DataFrame(Dict) df = df . set_index(col[ 0 ]) if operation == 'sum' : gdf = df . groupby(col[ 0 ]) . sum() elif operation == 'count' : gdf = df . groupby(col[ 0 ]) . count() index_name = list (gdf . index) csum = list (gdf[col[ 1 ]]) ucount = {} for x,y in zip (index_name,csum): ucount . update({x:y}) return ucount def cell_p2tf_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value '''map-reduce''' CellP2tf = self . cell_map(cellpmid2pcount,select = 'tf' ) cellp2tf = self . cell_reduce(CellP2tf,[ 'protein' , 'tf' ],operation = 'sum' ) self . cell_p2tf . update({cell_name:cellp2tf}) def cell_tf_finder ( self ): for key, value in self . cell_p2tf . items(): cell_name = key cellp2tf = value celltf = {} for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: celltf . update({p:cellp2tf[p]}) else : celltf . update({p: 0 }) self . cell_tf . update({cell_name:celltf}) def cell_pop_finder ( self ,dump = False ): for key,value in self . cell_tf . items(): cell_name = key cell_tf = value cellpop = {} cntp = 0 #---------------------------- for key,value in cell_tf . items(): cntp = cntp + int (value) self . cell_cntp . update({cell_name:cntp}) #------------------------------ for key,value in cell_tf . items(): pop = np . log(value + 1 ) / np . log(cntp) cellpop . update({key:pop}) self . cell_pop . update({cell_name:cellpop}) if dump: self . df_builder( self . cell_pop,fname = 'pop' ) def cell_p2pmid_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value '''map-reduce''' CellP2pmid = self . cell_map(cellpmid2pcount,select = 'pmid' ) cellp2pmid = self . cell_reduce(CellP2pmid,[ 'protein' , 'pmid' ],operation = 'count' ) self . cell_p2pmid . update({cell_name:cellp2pmid}) def cell_ntf_finder ( self ): k1 = 1.2 b = 0.75 for key,value in self . cell_tf . items(): cell_name = key celltf = value #---------------------------- nonzero_celltf = [] for key,value in celltf . items(): if int (value) > 0 : nonzero_celltf . append( int (value)) #------------------------------------------- av_cntp = self . cell_cntp[cell_name] / float ( len (nonzero_celltf)) cellntf = {} for key,value in celltf . items(): p = key tf = value ntf = (tf * (k1 + 1 )) / float (tf + (k1 * ( 1 - b + (b * ( self . cell_cntp[cell_name] / float (av_cntp)))))) cellntf . update({p:ntf}) self . cell_ntf . update({cell_name:cellntf}) def cell_ndf_finder ( self ): for key,value in self . cell_p2pmid . items(): cell_name = key cellp2pmid = value all_pmid_counts = [] cellndf = {} #-------------------------------------------- for key,value in cellp2pmid . items(): all_pmid_counts . append(value) maxv = max (all_pmid_counts) #----------------------------------------- for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: c = cellp2pmid[p] ndf = np . log( 1 + c) / np . log( 1 + maxv) else : ndf = 0 cellndf . update({p:ndf}) self . cell_ndf . update({cell_name:cellndf}) def cell_rel_finder ( self ): for key,value in self . cell_ntf . items(): cell_name = key cellntf = value cellrel = {} for p in self . all_proteins: rel = cellntf[p] * self . cell_ndf[cell_name][p] cellrel . update({p:rel}) self . cell_rel . update({cell_name:cellrel}) def cell_dist_finder ( self ,dump = False ): cell_exprel = {} for key,value in self . cell_rel . items(): cell_name = key cellrel = value cellexprel = {} for key,value in cellrel . items(): cellexprel . update({key:np . exp(value)}) cell_exprel . update({cell_name:cellexprel}) #----------------------------------------------------- p2din = {} for p in self . all_proteins: din = 1.0 for cellname in self . cellnames: din = din + cell_exprel[cellname][p] p2din . update({p:din}) #-------------------------------------------------------- for key,value in cell_exprel . items(): cell_name = key cellexprel = value celldist = {} for key,value in cellexprel . items(): celldist . update({key:value / p2din[key]}) self . cell_dist . update({cell_name:celldist}) if dump: self . df_builder( self . cell_dist,fname = 'dist' ) def cell_cseolap_finder ( self ,dump = False ): for key,value in self . cell_dist . items(): cell_name = key celldist = value cellcaseolap = {} for key,value in celldist . items(): cellcaseolap . update({key:(value * self . cell_pop[cell_name][key])}) self . cell_caseolap . update({cell_name:cellcaseolap}) if dump: self . df_builder( self . cell_caseolap,fname = 'caseolap' ) self . dump_json( self . cell_caseolap,fname = 'caseolap' ) Test Run C = Caseolap(cvd2pmids,pmid2pcount) C . cell_pmids_collector(dump = True ,verbose = True ) #C.cell_pmids C . cell_pmid2pcount_collector() #C.cell_pmid2pcount C . all_protein_finder(dump = True ,verbose = True ) #C.all_proteins C . all_protein_finder() #C.all_proteins #C.cell_uniqp C . cell_p2tf_finder() #C.cell_p2tf C . cell_tf_finder() #C.cell_tf C . cell_pop_finder(dump = True ) #C.cell_pop C . cell_p2pmid_finder() #C.cell_p2pmid C . cell_ntf_finder() #C.cell_ntf C . cell_ndf_finder() #C.cell_ndf C . cell_rel_finder() #C.cell_rel C . cell_dist_finder(dump = True ) #C.cell_dist C . cell_cseolap_finder(dump = True ) #C.cell_caseolap","title":"CaseOLAP"},{"location":"CaseOLAP/#caseolap-score-calculation","text":"CaseOLAP score calculation : CaseOLAP score are the quantification of user defined entity-category association. It start with the text-cube document structure and finds the entity in each document in each cell of the text-cube and by implementing updated text-cube metadata, it calculates the CaseOLAP score with following steps: Integrity : Integrity of user defined phrase is taken to be 1. (Autophrase, Segphrase) Popularity : It depends on how frequently a protein name is mentioned within one category, and it is calculated only using the statistics from the cells of documents pertaining to that individual category. Rare protein names in a cell are ranked low, while an increase in their frequency of mention has a diminishing return. - In each of the cell c in text-cube, term frequency tf(p,c) of each entity is calculated. - Using individual term frequency for each entity(protein), total sum of the term frequency cntP(c) is calculated. - A normalized popularity of phrase p in cell c , pop(p,c) is calculated by using tf(p,c) and ntfP(c) calculated in 6.2.1 and 6.2.2. [eq ref]. Distinctiveness : It is based on the relevance of a entity name to a specific category by comparing the occurrence of the protein name in the target data set, i.e., the cell documents describing one cell, to the contrastive data set, i.e., the cells of documents describing the remaining cells. - Normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] at 5.4.3 are used to calculated the relevance score rel(p,c) [eq. ref] of protein p in cell c . - Normalized distinctiveness disti(p,c) [eq ref] is calculated by using relevance score of protein p in cell c and and relevance across neighbouring cells c\u2019 (c\u2019 K(p,c) : neighbourhoods of cell c ). CaseOLAP score : It is the product of Integrity, Popularity and Distinctiveness calculated in 6.1,6.2 and 6.3.","title":"CaseOLAP score calculation"},{"location":"CaseOLAP/#import-required-libraries","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline import json","title":"Import required libraries"},{"location":"CaseOLAP/#data","text":"with open ( 'input/cat2pmids.json' , 'r' ) as f: cvd2pmids = json . load(f) print ( 'total cat2pmids:' , len (cat2pmids)) with open ( 'input/pmid2pcount.json' , 'r' ) as f: pmid2pcount = json . load(f) print ( 'total pmid2pcount:' , len (pmid2pcount))","title":"Data"},{"location":"CaseOLAP/#caseolap-score-calculation_1","text":"class Caseolap ( object ): def __init__ ( self ,cvd2pmids,pmid2pcount): self . cellnames = [] self . cvd2pmids = cvd2pmids self . pmid2pcount = pmid2pcount self . cell_pmids = {} self . cell_pmid2pcount = {} self . all_proteins = [] self . cell_uniqp = {} self . cell_p2tf = {} self . cell_tf = {} self . cell_cntp = {} self . cell_pop = {} self . cell_p2pmid = {} self . cell_ntf = {} self . cell_ndf = {} self . cell_rel = {} self . cell_dist = {} self . cell_caseolap = {} def df_builder ( self ,cell_quant,fname): flatdata = [] for p in self . all_proteins: d = { 'protein' :p} for name in self . cellnames: d . update({name:cell_quant[name][p]}) flatdata . append(d) df = pd . DataFrame(flatdata) df = df . set_index( 'protein' ) df . to_csv( 'data/' + fname + '.csv' ) return df def dump_json ( self ,data,fname): with open ( 'data/' + fname + '.json' , 'w' ) as dl: json . dump(data, dl) def cell_pmids_collector ( self , dump = False ,verbose = False ): for key,value in self . cvd2pmids . items(): cell_name = key cell_pmids = value self . cellnames . append(cell_name) self . cell_pmids . update({cell_name:cell_pmids}) if verbose: print ( 'total pmids collected for cell - ' ,cell_name, len (cell_pmids)) if dump: self . dump_json( self . cell_pmids,fname = 'cellpmids' ) def cell_pmid2pcount_collector ( self ): for key,value in self . cell_pmids . items(): cell_name = key cell_pmids = value ipmid2pcount = {} for pmid in cell_pmids: pmid_pcount = self . pmid2pcount[pmid] ipmid2pcount . update({pmid:pmid_pcount}) self . cell_pmid2pcount . update({cell_name:ipmid2pcount}) def all_protein_finder ( self ,dump = False ,verbose = False ): allproteins = [] for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value cellproteins = [] for key, value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key,value in pmid_pcount . items(): allproteins . append(key) cellproteins . append(key) uprotein = list ( set (cellproteins)) self . cell_uniqp . update({cell_name:uprotein}) if verbose: print ( 'total proteins collected for cell - ' ,cell_name, len (uprotein)) self . all_proteins = list ( set (allproteins)) if verbose: print ( 'total proteins collected: ' , len ( self . all_proteins)) if dump: self . dump_json( self . all_proteins,fname = 'allproteins' ) self . dump_json( self . cell_uniqp,fname = 'unique_proteins' ) def cell_map ( self ,cellpmid2pcount,select): map_dict = [] for key,value in cellpmid2pcount . items(): pmid = key pmid_pcount = value for key, value in pmid_pcount . items(): if select == 'tf' : map_dict . append({ 'protein' : key, 'tf' : int (value)}) elif select == 'pmid' : map_dict . append({ 'protein' : key, 'pmid' :pmid}) return map_dict def cell_reduce ( self ,Dict,col,operation): df = pd . DataFrame(Dict) df = df . set_index(col[ 0 ]) if operation == 'sum' : gdf = df . groupby(col[ 0 ]) . sum() elif operation == 'count' : gdf = df . groupby(col[ 0 ]) . count() index_name = list (gdf . index) csum = list (gdf[col[ 1 ]]) ucount = {} for x,y in zip (index_name,csum): ucount . update({x:y}) return ucount def cell_p2tf_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value '''map-reduce''' CellP2tf = self . cell_map(cellpmid2pcount,select = 'tf' ) cellp2tf = self . cell_reduce(CellP2tf,[ 'protein' , 'tf' ],operation = 'sum' ) self . cell_p2tf . update({cell_name:cellp2tf}) def cell_tf_finder ( self ): for key, value in self . cell_p2tf . items(): cell_name = key cellp2tf = value celltf = {} for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: celltf . update({p:cellp2tf[p]}) else : celltf . update({p: 0 }) self . cell_tf . update({cell_name:celltf}) def cell_pop_finder ( self ,dump = False ): for key,value in self . cell_tf . items(): cell_name = key cell_tf = value cellpop = {} cntp = 0 #---------------------------- for key,value in cell_tf . items(): cntp = cntp + int (value) self . cell_cntp . update({cell_name:cntp}) #------------------------------ for key,value in cell_tf . items(): pop = np . log(value + 1 ) / np . log(cntp) cellpop . update({key:pop}) self . cell_pop . update({cell_name:cellpop}) if dump: self . df_builder( self . cell_pop,fname = 'pop' ) def cell_p2pmid_finder ( self ): for key,value in self . cell_pmid2pcount . items(): cell_name = key cellpmid2pcount = value '''map-reduce''' CellP2pmid = self . cell_map(cellpmid2pcount,select = 'pmid' ) cellp2pmid = self . cell_reduce(CellP2pmid,[ 'protein' , 'pmid' ],operation = 'count' ) self . cell_p2pmid . update({cell_name:cellp2pmid}) def cell_ntf_finder ( self ): k1 = 1.2 b = 0.75 for key,value in self . cell_tf . items(): cell_name = key celltf = value #---------------------------- nonzero_celltf = [] for key,value in celltf . items(): if int (value) > 0 : nonzero_celltf . append( int (value)) #------------------------------------------- av_cntp = self . cell_cntp[cell_name] / float ( len (nonzero_celltf)) cellntf = {} for key,value in celltf . items(): p = key tf = value ntf = (tf * (k1 + 1 )) / float (tf + (k1 * ( 1 - b + (b * ( self . cell_cntp[cell_name] / float (av_cntp)))))) cellntf . update({p:ntf}) self . cell_ntf . update({cell_name:cellntf}) def cell_ndf_finder ( self ): for key,value in self . cell_p2pmid . items(): cell_name = key cellp2pmid = value all_pmid_counts = [] cellndf = {} #-------------------------------------------- for key,value in cellp2pmid . items(): all_pmid_counts . append(value) maxv = max (all_pmid_counts) #----------------------------------------- for p in self . all_proteins: if p in self . cell_uniqp[cell_name]: c = cellp2pmid[p] ndf = np . log( 1 + c) / np . log( 1 + maxv) else : ndf = 0 cellndf . update({p:ndf}) self . cell_ndf . update({cell_name:cellndf}) def cell_rel_finder ( self ): for key,value in self . cell_ntf . items(): cell_name = key cellntf = value cellrel = {} for p in self . all_proteins: rel = cellntf[p] * self . cell_ndf[cell_name][p] cellrel . update({p:rel}) self . cell_rel . update({cell_name:cellrel}) def cell_dist_finder ( self ,dump = False ): cell_exprel = {} for key,value in self . cell_rel . items(): cell_name = key cellrel = value cellexprel = {} for key,value in cellrel . items(): cellexprel . update({key:np . exp(value)}) cell_exprel . update({cell_name:cellexprel}) #----------------------------------------------------- p2din = {} for p in self . all_proteins: din = 1.0 for cellname in self . cellnames: din = din + cell_exprel[cellname][p] p2din . update({p:din}) #-------------------------------------------------------- for key,value in cell_exprel . items(): cell_name = key cellexprel = value celldist = {} for key,value in cellexprel . items(): celldist . update({key:value / p2din[key]}) self . cell_dist . update({cell_name:celldist}) if dump: self . df_builder( self . cell_dist,fname = 'dist' ) def cell_cseolap_finder ( self ,dump = False ): for key,value in self . cell_dist . items(): cell_name = key celldist = value cellcaseolap = {} for key,value in celldist . items(): cellcaseolap . update({key:(value * self . cell_pop[cell_name][key])}) self . cell_caseolap . update({cell_name:cellcaseolap}) if dump: self . df_builder( self . cell_caseolap,fname = 'caseolap' ) self . dump_json( self . cell_caseolap,fname = 'caseolap' )","title":"Caseolap Score Calculation"},{"location":"CaseOLAP/#test-run","text":"C = Caseolap(cvd2pmids,pmid2pcount) C . cell_pmids_collector(dump = True ,verbose = True ) #C.cell_pmids C . cell_pmid2pcount_collector() #C.cell_pmid2pcount C . all_protein_finder(dump = True ,verbose = True ) #C.all_proteins C . all_protein_finder() #C.all_proteins #C.cell_uniqp C . cell_p2tf_finder() #C.cell_p2tf C . cell_tf_finder() #C.cell_tf C . cell_pop_finder(dump = True ) #C.cell_pop C . cell_p2pmid_finder() #C.cell_p2pmid C . cell_ntf_finder() #C.cell_ntf C . cell_ndf_finder() #C.cell_ndf C . cell_rel_finder() #C.cell_rel C . cell_dist_finder(dump = True ) #C.cell_dist C . cell_cseolap_finder(dump = True ) #C.cell_caseolap","title":"Test Run"},{"location":"Download/","text":"Download Pipeline This pipeline downloads the data from the source server. It checks the integrity of the downloaded data and extracts it to the target directory. Following are the outline the download of the pipeline. 1. Import Required Python packages import os import sys import re import time import subprocess 2. Specify source and target data directory DATA_DIR = './' '''FTP address for baseline directory of Pubmed ''' BASELINE_DIR = os . path . join(DATA_DIR,\\ 'ftp.ncbi.nlm.nih.gov/pubmed/baseline/' ) '''FTP address for baseline directory of Pubmed''' UPDATE_FILES_DIR = os . path . join(DATA_DIR,\\ 'ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/' ) 3. Controlling download with os package and wget command. os.path.join() functionality is implemented to join one or more path components intelligently. os.system (wget command) functionality is implemented to execute the wget command in a subshell. The following are optional syntax modifiers for the wget command to control the download: - flag -q turns off wget output, - flag -r turns on recursive retrieving, - directory prefix --directory-prefix=%s sets the prefix of all other files and subdirectories, - directory based limit --no-parent guarantees that you will never leave the existing hierarchy of the directory. def download_pubmed_baseline (): '''Baseline''' print ( \"Start downloading pubmed baseline files.\" , \"ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/\" ) t1 = time . time() rc = os . system( 'wget -q -r --directory-prefix= %s --no-parent ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/' % DATA_DIR) if rc != 0 : print ( \"Return code of downloading pubmed baseline files via wget is %d , not zero.\" % rc) print ( \"Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline\" ) exit (rc) t2 = time . time() print ( \"Finish downloading pubmed baseline files. %f s\" % (t2 - t1)) def download_pubmed_update (): '''Update''' print ( \"Start downloading pubmed updatefiles.\" , \"ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/\" ) rc = os . system( 'wget -q -r --directory-prefix= %s --no-parent ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/' % DATA_DIR) if rc != 0 : print ( \"Return code of downloading pubmed update files via wget is %d , not zero.\" % rc) print ( \"Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles\" ) exit (rc) t3 = time . time() print ( \"Finish downloading pubmed updatefiles. %f s\" % (t3 - t2)) def download_bioconcepts2pubtator_offsets (): '''bioconcepts2pubtator''' rc = os . system( 'wget -q --directory-prefix= %s ftp://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator/bioconcepts2pubtator_offsets.gz' % DATA_DIR) if rc != 0 : print ( \"Return code of downloading pubmed update files via wget is %d , not zero.\" % rc) print ( \"Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles\" ) exit (rc) 4. Md5-checksum An MD5-checksum is a 32-character hexadecimal number that is computed on a file. This number helps to verify the integrity of the file download. Implementing the re and subprocess packages adds MD5-checksum functionality. def check_all_md5_in_dir ( dir ): if os . system( \"which md5sum 1>/dev/null\" ) != 0 : print ( \"md5sum not found\" ) # Continue executing return count = 0 print ( \"==== Start checking md5 in %s ====\" % dir ) if os . path . isdir( dir ): for file in os . listdir( dir ): if re . search( '^medline17n\\d\\d\\d\\d.xml.gz$' , file ): count += 1 check_md5(os . path . join( dir , file )) if count % 100 == 0 : print ( \" %d files checked\" % count) print ( \"==== All md5 check succeeded ( %d files) ====\" % count) else : print ( \"Directory not found: %s (for md5 check)\" % dir ) def check_md5 ( file ): if os . path . isfile( file ) and os . path . isfile( file + \".md5\" ): # Work only on Linux, user \"md5\" for Mac stdout = subprocess . check_output( \"md5sum %s \" % file , shell = True ) . decode( 'utf-8' ) md5_calculated = re . search( '[0-9a-f]{32}' , stdout) . group( 0 ) md5 = re . search( '[0-9a-f]{32}' , open ( file + \".md5\" , 'r' ) . readline()) . group( 0 ) if md5 != md5_calculated: print ( \"Error: md5 check failed for file %s \" % file ) exit ( 1 ) 5. Data extraction : Downloaded data files are in a compressed \u2018.gz\u2019 format, which need to be extracted. A data extraction pipeline can be created with the following steps: - import regular expression (re) , and subprocess modules, - list all data files using os.listdir functionality, - with os and wget command, gunzip -fqk , extract all files in a loop. # Assume filename is *.gz def extract ( file ): rc = os . system( 'gunzip -fqk %s ' % file ) if rc != 0 : print ( \"gunzip return code for file %s is %d , not zero\" % ( file , rc)) exit (rc) return rc def extract_all_gz_in_dir ( dir ): if os . path . isdir( dir ): count = 0 print ( \"==== Start extracting in %s ====\" % dir ) t1 = time . time() for file in os . listdir( dir ): if re . search( '.*\\.gz$' , file ): extract(os . path . join( dir , file )) count += 1 if count % 50 == 0 : print ( \" %d files extracted, %f s taken so far\" % (count, time . time() - t1)) print ( \"==== All files extracted ( %d files). Total time: %f s ====\" % (count, time . time() - t1)) else : print ( \"Directory not found: %s (for extraction)\" % dir ) Run the pipeline BASELINE_DIR = os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/baseline/' ) UPDATE_FILES_DIR = os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/' ) '''Download''' download_pubmed_baseline() download_pubmed_update() '''MD5 Checksum''' check_all_md5_in_dir(BASELINE_DIR) check_all_md5_in_dir(UPDATE_FILES_DIR) '''Extraction''' extract_all_gz_in_dir(BASELINE_DIR) extract_all_gz_in_dir(UPDATE_FILES_DIR) '''bioconcepts2pubtator''' download_bioconcepts2pubtator_offsets() extract(os . path . join(DATA_DIR, 'bioconcepts2pubtator_offsets.gz' ))","title":"Download Pipeline"},{"location":"Download/#download-pipeline","text":"This pipeline downloads the data from the source server. It checks the integrity of the downloaded data and extracts it to the target directory. Following are the outline the download of the pipeline. 1. Import Required Python packages import os import sys import re import time import subprocess 2. Specify source and target data directory DATA_DIR = './' '''FTP address for baseline directory of Pubmed ''' BASELINE_DIR = os . path . join(DATA_DIR,\\ 'ftp.ncbi.nlm.nih.gov/pubmed/baseline/' ) '''FTP address for baseline directory of Pubmed''' UPDATE_FILES_DIR = os . path . join(DATA_DIR,\\ 'ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/' ) 3. Controlling download with os package and wget command. os.path.join() functionality is implemented to join one or more path components intelligently. os.system (wget command) functionality is implemented to execute the wget command in a subshell. The following are optional syntax modifiers for the wget command to control the download: - flag -q turns off wget output, - flag -r turns on recursive retrieving, - directory prefix --directory-prefix=%s sets the prefix of all other files and subdirectories, - directory based limit --no-parent guarantees that you will never leave the existing hierarchy of the directory. def download_pubmed_baseline (): '''Baseline''' print ( \"Start downloading pubmed baseline files.\" , \"ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/\" ) t1 = time . time() rc = os . system( 'wget -q -r --directory-prefix= %s --no-parent ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/' % DATA_DIR) if rc != 0 : print ( \"Return code of downloading pubmed baseline files via wget is %d , not zero.\" % rc) print ( \"Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline\" ) exit (rc) t2 = time . time() print ( \"Finish downloading pubmed baseline files. %f s\" % (t2 - t1)) def download_pubmed_update (): '''Update''' print ( \"Start downloading pubmed updatefiles.\" , \"ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/\" ) rc = os . system( 'wget -q -r --directory-prefix= %s --no-parent ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/' % DATA_DIR) if rc != 0 : print ( \"Return code of downloading pubmed update files via wget is %d , not zero.\" % rc) print ( \"Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles\" ) exit (rc) t3 = time . time() print ( \"Finish downloading pubmed updatefiles. %f s\" % (t3 - t2)) def download_bioconcepts2pubtator_offsets (): '''bioconcepts2pubtator''' rc = os . system( 'wget -q --directory-prefix= %s ftp://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator/bioconcepts2pubtator_offsets.gz' % DATA_DIR) if rc != 0 : print ( \"Return code of downloading pubmed update files via wget is %d , not zero.\" % rc) print ( \"Link: ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles\" ) exit (rc) 4. Md5-checksum An MD5-checksum is a 32-character hexadecimal number that is computed on a file. This number helps to verify the integrity of the file download. Implementing the re and subprocess packages adds MD5-checksum functionality. def check_all_md5_in_dir ( dir ): if os . system( \"which md5sum 1>/dev/null\" ) != 0 : print ( \"md5sum not found\" ) # Continue executing return count = 0 print ( \"==== Start checking md5 in %s ====\" % dir ) if os . path . isdir( dir ): for file in os . listdir( dir ): if re . search( '^medline17n\\d\\d\\d\\d.xml.gz$' , file ): count += 1 check_md5(os . path . join( dir , file )) if count % 100 == 0 : print ( \" %d files checked\" % count) print ( \"==== All md5 check succeeded ( %d files) ====\" % count) else : print ( \"Directory not found: %s (for md5 check)\" % dir ) def check_md5 ( file ): if os . path . isfile( file ) and os . path . isfile( file + \".md5\" ): # Work only on Linux, user \"md5\" for Mac stdout = subprocess . check_output( \"md5sum %s \" % file , shell = True ) . decode( 'utf-8' ) md5_calculated = re . search( '[0-9a-f]{32}' , stdout) . group( 0 ) md5 = re . search( '[0-9a-f]{32}' , open ( file + \".md5\" , 'r' ) . readline()) . group( 0 ) if md5 != md5_calculated: print ( \"Error: md5 check failed for file %s \" % file ) exit ( 1 ) 5. Data extraction : Downloaded data files are in a compressed \u2018.gz\u2019 format, which need to be extracted. A data extraction pipeline can be created with the following steps: - import regular expression (re) , and subprocess modules, - list all data files using os.listdir functionality, - with os and wget command, gunzip -fqk , extract all files in a loop. # Assume filename is *.gz def extract ( file ): rc = os . system( 'gunzip -fqk %s ' % file ) if rc != 0 : print ( \"gunzip return code for file %s is %d , not zero\" % ( file , rc)) exit (rc) return rc def extract_all_gz_in_dir ( dir ): if os . path . isdir( dir ): count = 0 print ( \"==== Start extracting in %s ====\" % dir ) t1 = time . time() for file in os . listdir( dir ): if re . search( '.*\\.gz$' , file ): extract(os . path . join( dir , file )) count += 1 if count % 50 == 0 : print ( \" %d files extracted, %f s taken so far\" % (count, time . time() - t1)) print ( \"==== All files extracted ( %d files). Total time: %f s ====\" % (count, time . time() - t1)) else : print ( \"Directory not found: %s (for extraction)\" % dir ) Run the pipeline BASELINE_DIR = os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/baseline/' ) UPDATE_FILES_DIR = os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/' ) '''Download''' download_pubmed_baseline() download_pubmed_update() '''MD5 Checksum''' check_all_md5_in_dir(BASELINE_DIR) check_all_md5_in_dir(UPDATE_FILES_DIR) '''Extraction''' extract_all_gz_in_dir(BASELINE_DIR) extract_all_gz_in_dir(UPDATE_FILES_DIR) '''bioconcepts2pubtator''' download_bioconcepts2pubtator_offsets() extract(os . path . join(DATA_DIR, 'bioconcepts2pubtator_offsets.gz' ))","title":"Download Pipeline"},{"location":"EntityCount/","text":"Entity Count CaseOLAP score calculation requires the entity count per document. This step provides the data which connects metadata for text-cube structure and entity-count per document. Selection of entity : Entity(phrase) are defined by user to be studied under the text-cube document structure. User defined entity could be protein names, chemicals, disease or signs and symptoms etc. Search and listing of entity based document: The search functionality in Elasticsearch DSL package uses index name, parameters and query to list the document from indexed database. The query includes all representatives of the specific entity e.g. synonyms, abbreviations. Entity count : Then each document from the list is analysed one by one to count the entity also called term frequency, tf(p,c) . With the help of text-cube metadata, for each of the document in cell of the text-cube, the entity count is recorded as PMID to entity count mapping. Text-cube metadata update : Once the entity count is completed, text cube metadata is updated by adding PMID to entity count mapping . Following are the additional metadata prepared: - count the total occurance of each entity cntP(c) within each cell, - count the total number of documents df(p,c) within a cell in which entity appears. - calculate normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] using quantities obtained above. Import required libearies import sys import json from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q from collections import Counter year_constraints = None Setting up input output directories input_index_dir = \"../../elasticsearch-6.2.2/data\" input_file_pmid_and_cat = \"input/pmid_and_cat.json\" # a file containing cell to pmid mapping input_file_entity_list = \"../data/entities.txt\" # a file containing all entities output_file_paper_entity_count = \"output/paper_entity_count.txt\" # an output file for entity count per PMID output_file_paper_category = \"output/paper_category.txt\" # an output file containing PMID to cell mapping Read in paper info and entity info \"\"\" Read in paper info and entity info \"\"\" with open (input_file_pmid_and_cat, \"r\" ) as f_in: pmid_and_cat = json . load(f_in) concerned_pmid_set = set ( map ( lambda x: x[ 0 ], pmid_and_cat)) entity_count_per_pmid = {pmid: Counter() for pmid in concerned_pmid_set} entity_dict = {} with open (input_file_entity_list, \"r\" ) as f_in: for line in f_in: # synonums seperated by \"|\" and represented by the first one on each line line_split = line . strip() . split( \"|\" ) entity_dict[line_split[ 0 ]] = line_split Search and count entities: to optimize and find count from indexer \"\"\" Search and count entities: to optimize and find count from indexer \"\"\" es = Elasticsearch(timeout = 300 ) k = 0 for entity_rep in entity_dict: for entity in entity_dict[entity_rep]: #entity_space_sep = \"\".join(map(lambda x: \" \" if x == \"_\" else x, entity)) entity_space_sep = entity . replace( \"_\" , \" \" ) # s = Search(using=es, index=\"pmc_all_index\").query(\"match\", abstract=entity_space_sep) s = Search(using = es, index = \"pubmed\" )\\ . params(request_timeout = 300 )\\ . query( \"match_phrase\" , abstract = entity_space_sep) num_hits = 0 num_valid_hits = 0 num_counts = 0 for hit in s . scan(): num_hits += 1 cur_pmid = str (hit . pmid) if cur_pmid not in concerned_pmid_set: continue #if hit.PMCflag != 0: # continue if year_constraints is not None : # to-do print ( \"To add year constraint handler.\" ) abs_lower = hit . abstract . lower() . replace( \"-\" , \" \" ) entity_lower = entity_space_sep . lower() . replace( \"-\" , \" \" ) entity_cnt = abs_lower . count(entity_lower) if entity_cnt == 0 : #print \"----------\", entity_space_sep, \"----------\" #print abs_lower continue entity_count_per_pmid[cur_pmid][entity_rep] += entity_cnt num_valid_hits += 1 num_counts += entity_cnt #print(entity, \"# hits:\", num_hits, \"# valid hits:\", num_valid_hits, \"# counts:\", num_counts) k = k + 1 if k % 1000 == 0 : print (k, 'entity counted!' ) Entity count metadata update in each Cell \"\"\" Output \"\"\" ## paper entity count & paper category with open (output_file_paper_entity_count, \"w\" ) as f_out_entity_count,\\ open (output_file_paper_category, \"w\" ) as f_out_category: f_out_category . write( \"doc_id \\t label_id \\n \" ) paper_new_id = 1 for cur_pmid, cur_cat in pmid_and_cat: if len (entity_count_per_pmid[cur_pmid]) == 0 : continue # print paper category f_out_category . write( str (cur_pmid) + \" \\t \" + str (cur_cat) + \" \\n \" ) # print paper entity count f_out_entity_count . write( str (cur_pmid)) for entity in entity_count_per_pmid[cur_pmid]: f_out_entity_count . write( \" \" + entity + \"|\" + str (entity_count_per_pmid[cur_pmid][entity])) f_out_entity_count . write( \" \\n \" ) paper_new_id += 1 PMID to Entity count Dictionary with open ( 'paper_entity_count.txt' ) as ff: pmid2pcount = {} PMID2PCOUNT = [] for line in ff: item = line . split() pmid = item[ 0 ] if len (item) > 1 : prot_freq = {} for pf in item[ 1 :]: pfs = pf . split( '|' ) prot_freq . update({pfs[ 0 ]:pfs[ 1 ]}) pmid2pcount . update({pmid:prot_freq}) with open ( 'pmid2pcount.json' , 'w' ) as fp: json . dump(pmid2pcount, fp) PMID to Category Dictionary with open ( \"paper_category.txt\" ) as f: allpmids = [] PMID2CVD = [] cvd2pmids = {} dis0 = [] dis1 = [] for line in f: item = line . split() if item[ 0 ] != 'doc_id' : if item[ 0 ] in pmid2pcount: allpmids . append(item[ 0 ]) PMID2CVD . append({ 'pmid' :item[ 0 ], 'cat' :item[ 1 ]}) if item[ 1 ] == '0' : dis0 . append(item[ 0 ]) elif item[ 1 ] == '1' : dis1 . append(item[ 0 ]) cvd2pmids . update({dis[ 0 ]:dis0,dis[ 1 ]:dis1}) with open ( 'cat2pmids.json' , 'w' ) as fp: json . dump(cvd2pmids, fp)","title":"EntityCount"},{"location":"EntityCount/#entity-count","text":"CaseOLAP score calculation requires the entity count per document. This step provides the data which connects metadata for text-cube structure and entity-count per document. Selection of entity : Entity(phrase) are defined by user to be studied under the text-cube document structure. User defined entity could be protein names, chemicals, disease or signs and symptoms etc. Search and listing of entity based document: The search functionality in Elasticsearch DSL package uses index name, parameters and query to list the document from indexed database. The query includes all representatives of the specific entity e.g. synonyms, abbreviations. Entity count : Then each document from the list is analysed one by one to count the entity also called term frequency, tf(p,c) . With the help of text-cube metadata, for each of the document in cell of the text-cube, the entity count is recorded as PMID to entity count mapping. Text-cube metadata update : Once the entity count is completed, text cube metadata is updated by adding PMID to entity count mapping . Following are the additional metadata prepared: - count the total occurance of each entity cntP(c) within each cell, - count the total number of documents df(p,c) within a cell in which entity appears. - calculate normalized term frequency ntf(p,c) [eq ref] and normalized document frequency ndf(p,c) [eq ref] using quantities obtained above.","title":"Entity Count"},{"location":"EntityCount/#import-required-libearies","text":"import sys import json from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q from collections import Counter year_constraints = None","title":"Import required libearies"},{"location":"EntityCount/#setting-up-input-output-directories","text":"input_index_dir = \"../../elasticsearch-6.2.2/data\" input_file_pmid_and_cat = \"input/pmid_and_cat.json\" # a file containing cell to pmid mapping input_file_entity_list = \"../data/entities.txt\" # a file containing all entities output_file_paper_entity_count = \"output/paper_entity_count.txt\" # an output file for entity count per PMID output_file_paper_category = \"output/paper_category.txt\" # an output file containing PMID to cell mapping","title":"Setting up input output directories"},{"location":"EntityCount/#read-in-paper-info-and-entity-info","text":"\"\"\" Read in paper info and entity info \"\"\" with open (input_file_pmid_and_cat, \"r\" ) as f_in: pmid_and_cat = json . load(f_in) concerned_pmid_set = set ( map ( lambda x: x[ 0 ], pmid_and_cat)) entity_count_per_pmid = {pmid: Counter() for pmid in concerned_pmid_set} entity_dict = {} with open (input_file_entity_list, \"r\" ) as f_in: for line in f_in: # synonums seperated by \"|\" and represented by the first one on each line line_split = line . strip() . split( \"|\" ) entity_dict[line_split[ 0 ]] = line_split","title":"Read in paper info and entity info"},{"location":"EntityCount/#search-and-count-entities-to-optimize-and-find-count-from-indexer","text":"\"\"\" Search and count entities: to optimize and find count from indexer \"\"\" es = Elasticsearch(timeout = 300 ) k = 0 for entity_rep in entity_dict: for entity in entity_dict[entity_rep]: #entity_space_sep = \"\".join(map(lambda x: \" \" if x == \"_\" else x, entity)) entity_space_sep = entity . replace( \"_\" , \" \" ) # s = Search(using=es, index=\"pmc_all_index\").query(\"match\", abstract=entity_space_sep) s = Search(using = es, index = \"pubmed\" )\\ . params(request_timeout = 300 )\\ . query( \"match_phrase\" , abstract = entity_space_sep) num_hits = 0 num_valid_hits = 0 num_counts = 0 for hit in s . scan(): num_hits += 1 cur_pmid = str (hit . pmid) if cur_pmid not in concerned_pmid_set: continue #if hit.PMCflag != 0: # continue if year_constraints is not None : # to-do print ( \"To add year constraint handler.\" ) abs_lower = hit . abstract . lower() . replace( \"-\" , \" \" ) entity_lower = entity_space_sep . lower() . replace( \"-\" , \" \" ) entity_cnt = abs_lower . count(entity_lower) if entity_cnt == 0 : #print \"----------\", entity_space_sep, \"----------\" #print abs_lower continue entity_count_per_pmid[cur_pmid][entity_rep] += entity_cnt num_valid_hits += 1 num_counts += entity_cnt #print(entity, \"# hits:\", num_hits, \"# valid hits:\", num_valid_hits, \"# counts:\", num_counts) k = k + 1 if k % 1000 == 0 : print (k, 'entity counted!' )","title":"Search and count entities: to optimize and find count from indexer"},{"location":"EntityCount/#entity-count-metadata-update-in-each-cell","text":"\"\"\" Output \"\"\" ## paper entity count & paper category with open (output_file_paper_entity_count, \"w\" ) as f_out_entity_count,\\ open (output_file_paper_category, \"w\" ) as f_out_category: f_out_category . write( \"doc_id \\t label_id \\n \" ) paper_new_id = 1 for cur_pmid, cur_cat in pmid_and_cat: if len (entity_count_per_pmid[cur_pmid]) == 0 : continue # print paper category f_out_category . write( str (cur_pmid) + \" \\t \" + str (cur_cat) + \" \\n \" ) # print paper entity count f_out_entity_count . write( str (cur_pmid)) for entity in entity_count_per_pmid[cur_pmid]: f_out_entity_count . write( \" \" + entity + \"|\" + str (entity_count_per_pmid[cur_pmid][entity])) f_out_entity_count . write( \" \\n \" ) paper_new_id += 1","title":"Entity count metadata update in each Cell"},{"location":"EntityCount/#pmid-to-entity-count-dictionary","text":"with open ( 'paper_entity_count.txt' ) as ff: pmid2pcount = {} PMID2PCOUNT = [] for line in ff: item = line . split() pmid = item[ 0 ] if len (item) > 1 : prot_freq = {} for pf in item[ 1 :]: pfs = pf . split( '|' ) prot_freq . update({pfs[ 0 ]:pfs[ 1 ]}) pmid2pcount . update({pmid:prot_freq}) with open ( 'pmid2pcount.json' , 'w' ) as fp: json . dump(pmid2pcount, fp)","title":"PMID to Entity count Dictionary"},{"location":"EntityCount/#pmid-to-category-dictionary","text":"with open ( \"paper_category.txt\" ) as f: allpmids = [] PMID2CVD = [] cvd2pmids = {} dis0 = [] dis1 = [] for line in f: item = line . split() if item[ 0 ] != 'doc_id' : if item[ 0 ] in pmid2pcount: allpmids . append(item[ 0 ]) PMID2CVD . append({ 'pmid' :item[ 0 ], 'cat' :item[ 1 ]}) if item[ 1 ] == '0' : dis0 . append(item[ 0 ]) elif item[ 1 ] == '1' : dis1 . append(item[ 0 ]) cvd2pmids . update({dis[ 0 ]:dis0,dis[ 1 ]:dis1}) with open ( 'cat2pmids.json' , 'w' ) as fp: json . dump(cvd2pmids, fp)","title":"PMID to Category Dictionary"},{"location":"Indexing/","text":"Indexing and Search Indexing and search : This step prepares an indexed database which facilitates the entity search and counting operation . First, initialize the ElasticSearch index object. Then the index is populated in batches with bulk indexing functionality available in Elasticsearch package. Installation of required python package : Install and import elasticsearch and elasticsearch_dsl library to the current python environment. import time import re import sys import os from collections import defaultdict from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q Index initialization : Index initialization is done with the index information which includes index name, type name, number of shards, number of replicas. INDEX_NAME = \"pubmed\" TYPE_NAME = \"pubmed_meta\" NUMBER_SHARDS = 1 # keep this as one if no clusterNUMBER_REPLICAS = 0 ''' following is the defined schema totally 5 fields: pmid, date, author list, journal name, mesh heading list ''' request_body = { \"settings\" : { \"number_of_shards\" : NUMBER_SHARDS, \"number_of_replicas\" : NUMBER_REPLICAS }, \"mappings\" : { TYPE_NAME: { \"properties\" : { \"pmid\" : { \"type\" : \"keyword\" }, # \"date\": { # \"type\": \"long\" # }, # \"author_list\": { # \"type\": \"keyword\" # }, # \"journal_name\": { # \"type\": \"keyword\" # }, \"mesh_heading\" : { \"type\" : \"text\" , \"similarity\" : \"BM25\" }, \"abstract\" :{ \"type\" : \"text\" } } } } } es = Elasticsearch() if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( \"Deleting index %s , Response: %s \" % (INDEX_NAME, res)) res = es . indices . create(index = INDEX_NAME, body = request_body) print ( \"Create index %s , Response: %s \" % (INDEX_NAME, res)) Bulk indexing : In the first step of bulk indexing, data bulk is created with two components. - First component is a dictionary with metadata information of index name, type name and bluck id which is \u2018pmid\u2019 key. - Prepare the second component which is data dictionary with all information of \u2018title\u2019,\u2019abstract\u2019,\u2019MeSH\u2019 etc. inputFilePath = \"./pubmed.json\" logFilePath = \"./index_pubmed_log_20171001.txt\" INDEX_NAME = \"pubmed\" TYPE_NAME = \"pubmed_meta\" es = Elasticsearch() mesh2pmid = dict () ic = 0 ir = 0 with open (inputFilePath, \"r\" ) as fin, open (logFilePath, \"w\" ) as fout: start = time . time() bulk_size = 5000 # number of document processed in each bulk index bulk_data = [] # data in bulk index cnt = 0 for line in fin: ## each line is single document try : cnt += 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ \"pmid\" ] = paperInfo . get( \"PMID\" , \"-1\" ) #update MeSH Heading <---------------------- data_dict[ \"mesh_heading\" ] = \" \" . join(paperInfo[ \"MeshHeadingList\" ]) # update Abstract<------------------ data_dict[ \"abstract\" ] = paperInfo . get( \"Abstract\" , \"\" ) . lower() . replace( '-' , ' ' ) ## Put current data into the bulk <--------- op_dict = { \"index\" : { \"_index\" : INDEX_NAME, \"_type\" : TYPE_NAME, \"_id\" : data_dict[ \"pmid\" ] } } bulk_data . append(op_dict) bulk_data . append(data_dict) ## Start Bulk indexing if cnt % bulk_size == 0 and cnt != 0 : ic += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 ) fout . write( \"bulk indexing... %s , escaped time %s (seconds) \\n \" \\ % ( cnt, tmp - start ) ) if ic % 100 == 0 : print ( \" i bulk indexing... %s , escaped time %s (seconds) \" \\ % ( cnt, tmp - start ) ) bulk_data = [] except : cnt -= 1 print ( \"XXXX Unexpected Error happened at line: XXXX\" ) #print(line) ## indexing those left papers if bulk_data: ir += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 ) fout . write( \"bulk indexing... %s , escaped time %s (seconds) \\n \" \\ % ( cnt, tmp - start ) ) if ir % 100 == 0 : print ( \" r bulk indexing... %s , escaped time %s (seconds) \" \\ % ( cnt, tmp - start ) ) bulk_data = [] end = time . time() fout . write( \"Finish PubMed meta-data indexing. Total escaped time %s (seconds) \\n \" \\ % (end - start) ) print ( \"Finish PubMed meta-data indexing. Total escaped time %s (seconds) \" \\ % (end - start) ) Search functionality : One can perform search operation over data index created by Elasticsearch application. Once a search operation is initiated for user defined entity, it gathers information of that entity as a ranked list. Following are the steps for search operation: - start the Elasticsearch server - implement Elasticsearch DSL search functionality with index name, parameters and query - iterate over all hits obtained in search result to find desired entity","title":"Indexing"},{"location":"Indexing/#indexing-and-search","text":"Indexing and search : This step prepares an indexed database which facilitates the entity search and counting operation . First, initialize the ElasticSearch index object. Then the index is populated in batches with bulk indexing functionality available in Elasticsearch package. Installation of required python package : Install and import elasticsearch and elasticsearch_dsl library to the current python environment. import time import re import sys import os from collections import defaultdict from elasticsearch import Elasticsearch from elasticsearch_dsl import Search, Q Index initialization : Index initialization is done with the index information which includes index name, type name, number of shards, number of replicas. INDEX_NAME = \"pubmed\" TYPE_NAME = \"pubmed_meta\" NUMBER_SHARDS = 1 # keep this as one if no clusterNUMBER_REPLICAS = 0 ''' following is the defined schema totally 5 fields: pmid, date, author list, journal name, mesh heading list ''' request_body = { \"settings\" : { \"number_of_shards\" : NUMBER_SHARDS, \"number_of_replicas\" : NUMBER_REPLICAS }, \"mappings\" : { TYPE_NAME: { \"properties\" : { \"pmid\" : { \"type\" : \"keyword\" }, # \"date\": { # \"type\": \"long\" # }, # \"author_list\": { # \"type\": \"keyword\" # }, # \"journal_name\": { # \"type\": \"keyword\" # }, \"mesh_heading\" : { \"type\" : \"text\" , \"similarity\" : \"BM25\" }, \"abstract\" :{ \"type\" : \"text\" } } } } } es = Elasticsearch() if es . indices . exists(INDEX_NAME): res = es . indices . delete(index = INDEX_NAME) print ( \"Deleting index %s , Response: %s \" % (INDEX_NAME, res)) res = es . indices . create(index = INDEX_NAME, body = request_body) print ( \"Create index %s , Response: %s \" % (INDEX_NAME, res)) Bulk indexing : In the first step of bulk indexing, data bulk is created with two components. - First component is a dictionary with metadata information of index name, type name and bluck id which is \u2018pmid\u2019 key. - Prepare the second component which is data dictionary with all information of \u2018title\u2019,\u2019abstract\u2019,\u2019MeSH\u2019 etc. inputFilePath = \"./pubmed.json\" logFilePath = \"./index_pubmed_log_20171001.txt\" INDEX_NAME = \"pubmed\" TYPE_NAME = \"pubmed_meta\" es = Elasticsearch() mesh2pmid = dict () ic = 0 ir = 0 with open (inputFilePath, \"r\" ) as fin, open (logFilePath, \"w\" ) as fout: start = time . time() bulk_size = 5000 # number of document processed in each bulk index bulk_data = [] # data in bulk index cnt = 0 for line in fin: ## each line is single document try : cnt += 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ \"pmid\" ] = paperInfo . get( \"PMID\" , \"-1\" ) #update MeSH Heading <---------------------- data_dict[ \"mesh_heading\" ] = \" \" . join(paperInfo[ \"MeshHeadingList\" ]) # update Abstract<------------------ data_dict[ \"abstract\" ] = paperInfo . get( \"Abstract\" , \"\" ) . lower() . replace( '-' , ' ' ) ## Put current data into the bulk <--------- op_dict = { \"index\" : { \"_index\" : INDEX_NAME, \"_type\" : TYPE_NAME, \"_id\" : data_dict[ \"pmid\" ] } } bulk_data . append(op_dict) bulk_data . append(data_dict) ## Start Bulk indexing if cnt % bulk_size == 0 and cnt != 0 : ic += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 ) fout . write( \"bulk indexing... %s , escaped time %s (seconds) \\n \" \\ % ( cnt, tmp - start ) ) if ic % 100 == 0 : print ( \" i bulk indexing... %s , escaped time %s (seconds) \" \\ % ( cnt, tmp - start ) ) bulk_data = [] except : cnt -= 1 print ( \"XXXX Unexpected Error happened at line: XXXX\" ) #print(line) ## indexing those left papers if bulk_data: ir += 1 tmp = time . time() es . bulk(index = INDEX_NAME, body = bulk_data, request_timeout = 500 ) fout . write( \"bulk indexing... %s , escaped time %s (seconds) \\n \" \\ % ( cnt, tmp - start ) ) if ir % 100 == 0 : print ( \" r bulk indexing... %s , escaped time %s (seconds) \" \\ % ( cnt, tmp - start ) ) bulk_data = [] end = time . time() fout . write( \"Finish PubMed meta-data indexing. Total escaped time %s (seconds) \\n \" \\ % (end - start) ) print ( \"Finish PubMed meta-data indexing. Total escaped time %s (seconds) \" \\ % (end - start) ) Search functionality : One can perform search operation over data index created by Elasticsearch application. Once a search operation is initiated for user defined entity, it gathers information of that entity as a ranked list. Following are the steps for search operation: - start the Elasticsearch server - implement Elasticsearch DSL search functionality with index name, parameters and query - iterate over all hits obtained in search result to find desired entity","title":"Indexing and Search"},{"location":"Parsing/","text":"Data Parsingn Pipeline This pipeline will parse the extracted data and convert it into data structures compatible with the CaseOLAP pipeline. Installation of required python package : Install and import lxml, itertools and json libraries into the current python environment. import re import itertools import json import sys import os import time import traceback from lxml import etree Set up output data dir DATA_DIR = './' MeSH statistics dictionary mesh_statistics = {} Data parsing strategy : The extracted data is an XML file, and text data is embedded in the tree structure of XML document. The following are steps for parsing data: - Implement the etree functionality in lxml module to dig into the tree structure of XML document. - The separate components e.g. PMID, authors, abstract , MeSH etc. of the data is obtained by using tags representing these components. - Implement the chain functionality in itertools to creates an iterator that returns elements from the iterables which was obtained by implementing etree functionality in lxml . # Search the tag in the xml element # Return tag's text if tag exists, return empty string if doesn't def get_text (element, tag): e = element . find(tag) if e is not None : return e . text else : return '' # <!ELEMENT AuthorList (Author+) > # <!ELEMENT Author (((LastName, ForeName?, Initials?, Suffix?)\\ # | CollectiveName), Identifier*, AffiliationInfo*) > def parse_author (authors): result = [] for author in authors: item = {} item[ 'LastName' ] = get_text(author, 'LastName' ) item[ 'ForeName' ] = get_text(author, 'ForeName' ) item[ 'Initials' ] = get_text(author, 'Initials' ) item[ 'Suffix' ] = get_text(author, 'Suffix' ) item[ 'CollectiveName' ] = get_text(author, 'CollectiveName' ) result . append(item) return result Creation of dictionary of parsed data : A python dictionary is created with all the components as key-value pair. This JSON-like data structure makes it compatible for indexing and searching in Elasticsearch which is described in step 3 of protocol. Creation of MeSH to PMID mapping : During the creation of dictionary of parsed data, MeSH to PMID mapping table can also be created. This mapping is used to create Text-cube(in step 4) document structure as an requirement of CaseOLAP algorithm. def parse_pubmed_file ( file , pubmed_output_file, pmid2mesh_output_file): print ( 'Start parsing %s ' % file ) sys . stdout . flush() t1 = time . time() f = open ( file , 'r' ) tree = etree . parse(f) articles = itertools . chain(tree . findall( 'PubmedArticle' ), tree . findall( 'BookDocument' )) count = 0 noabs = 0 for article in articles: count += 1 result = {} pmid2mesh = {} # PMID - Exactly One Occurrance result[ 'PMID' ] = get_text(article, './/PMID' ) pmid2mesh[ 'PMID' ] = get_text(article, './/PMID' ) # # Article title - Zero or One Occurrences # result['ArticleTitle'] = get_text(article, './/ArticleTitle') # Abstract - Zero or One Occurrences abstractList = article . find( './/Abstract' ) if abstractList != None : try : abstract = ' \\n ' . join([line . text for line in abstractList . \\ findall( 'AbstractText' )]) result[ 'Abstract' ] = abstract except : result[ 'Abstract' ] = '' noabs += 1 else : result[ 'Abstract' ] = '' noabs += 1 # # Author List - Zero or More Occurrences # authors = article.findall('.//Author') # result['AuthorList'] = parse_author(authors) # # Journal - Exactly One Occurrance # journal = article.find('.//Journal') # result['Journal'] = get_text(journal, 'Title') result[ 'PubDate' ] = {} result[ 'PubDate' ][ 'Year' ] = get_text(journal, 'JournalIssue/PubDate/Year' ) # result['PubDate']['Month'] = get_text(journal, 'JournalIssue/PubDate/Month') # result['PubDate']['Day'] = get_text(journal, 'JournalIssue/PubDate/Day') # result['PubDate']['Season'] = get_text(journal, 'JournalIssue/PubDate/Season') # result['PubDate']['MedlineDate'] = get_text(journal,\\ # 'JournalIssue/PubDate/MedlineDate') # MeshHeading - Zero or More Occurrences headings = article . findall( './/MeshHeading' ) result[ 'MeshHeadingList' ] = [] pmid2mesh[ 'MeshHeadingList' ] = [] if headings: for heading in headings: descriptor_names = heading . findall( 'DescriptorName' ) qualifier_names = heading . findall( 'QualifierName' ) if descriptor_names: for descriptor_name in descriptor_names: result[ 'MeshHeadingList' ] . append(descriptor_name . text) pmid2mesh[ 'MeshHeadingList' ] . append(descriptor_name . text) if qualifier_names: for qualifier_name in qualifier_names: result[ 'MeshHeadingList' ] . append(qualifier_name . text) pmid2mesh[ 'MeshHeadingList' ] . append(qualifier_name . text) mesh_count = len (result[ 'MeshHeadingList' ]) if mesh_count in mesh_statistics: mesh_statistics[mesh_count] += 1 else : mesh_statistics[mesh_count] = 1 # Dump to pubmed json file <---------------------------- json . dump(result, pubmed_output_file) pubmed_output_file . write( ' \\n ' ) # Dump to pmid2mesh json file <------------------------- json . dump(pmid2mesh, pmid2mesh_output_file) pmid2mesh_output_file . write( ' \\n ' ) print ( 'Finish parsing %s , totally %d articles parsed. Total time: %f s' \\ % ( file , count, time . time() - t1)) print ( ' %d acticles no abstracts' % (noabs)) sys . stdout . flush() f . close() Setting up directories in parsing loop def parse_dir (source_dir, pubmed_output_file,pmid2mesh_output_file): if os . path . isdir(source_dir): for file in os . listdir(source_dir): if re . search( r'^pubmed18n\\d\\d\\d\\d.xml$' , file ) is not None : try : parse_pubmed_file(os . path . join(source_dir, file ),\\ pubmed_output_file, pmid2mesh_output_file) except : print ( \"XXXX Unexpected error happended when parsing %s XXXX\" % file ) print (traceback . print_exc()) sys . stdout . flush() Run the Parsing Pipeline t1 = time . time() pubmed_output_file_path = os . path . join(DATA_DIR, 'data/pubmed.json' ) pmid2mesh_output_file_path = os . path . join(DATA_DIR, 'pmid2mesh/pmid2mesh_from_parsing.json' ) pubmed_output_file = open (pubmed_output_file_path, 'w' ) pmid2mesh_output_file = open (pmid2mesh_output_file_path, 'w' ) parse_dir(os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/baseline' ),\\ pubmed_output_file, pmid2mesh_output_file) parse_dir(os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/updatefiles' ),\\ pubmed_output_file, pmid2mesh_output_file) pubmed_output_file . close() pmid2mesh_output_file . close() mesh_file = open (os . path . join(DATA_DIR, 'data/mesh_statistics.json' ), 'w' ) json . dump(mesh_statistics, mesh_file) mesh_file . close() print ( \"==== Parsing finished, results dumped to %s ====\" % pubmed_output_file_path) print ( \"==== TOTAL TIME: %f s ====\" % (time . time() - t1)) MeSH to PMID Mapping inputFilePath = \"data/pubmed.json\" meshFilePath = \"mesh2pmid/\" mesh2pmid_output_file = open (meshFilePath + 'mesh2pmid.json' , \"w\" ) mesh2pmid = dict () with open (inputFilePath, \"r\" ) as fin: start = time . time() k = 0 for line in fin: ## each line is single document try : k = k + 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ \"pmid\" ] = paperInfo . get( \"PMID\" , \"-1\" ) #update MeSH Heading <---------------------- data_dict[ \"mesh_heading\" ] = \" \" . join(paperInfo[ \"MeshHeadingList\" ]) # collect Mesh2PMID <------------------- if data_dict[ \"pmid\" ] != \"-1\" : for mesh in paperInfo[ \"MeshHeadingList\" ]: if mesh not in mesh2pmid: mesh2pmid[mesh] = [] mesh2pmid[mesh] . append(data_dict[ \"pmid\" ]) if k % 500000 == 0 : print (k, 'done!' ) #break except : print ( \"XXXX Unexpected Error happened at line: XXXX\" ) # Dumping rest papers for key,value in mesh2pmid . items(): json . dump({key:value}, mesh2pmid_output_file) mesh2pmid_output_file . write( ' \\n ' ) mesh2pmid = dict () end = time . time() print ( \"Finish Total escaped time %s (seconds) \" % (end - start) )","title":"Parsing Pipeline"},{"location":"Parsing/#data-parsingn-pipeline","text":"This pipeline will parse the extracted data and convert it into data structures compatible with the CaseOLAP pipeline. Installation of required python package : Install and import lxml, itertools and json libraries into the current python environment. import re import itertools import json import sys import os import time import traceback from lxml import etree","title":"Data Parsingn Pipeline"},{"location":"Parsing/#set-up-output-data-dir","text":"DATA_DIR = './' MeSH statistics dictionary mesh_statistics = {} Data parsing strategy : The extracted data is an XML file, and text data is embedded in the tree structure of XML document. The following are steps for parsing data: - Implement the etree functionality in lxml module to dig into the tree structure of XML document. - The separate components e.g. PMID, authors, abstract , MeSH etc. of the data is obtained by using tags representing these components. - Implement the chain functionality in itertools to creates an iterator that returns elements from the iterables which was obtained by implementing etree functionality in lxml . # Search the tag in the xml element # Return tag's text if tag exists, return empty string if doesn't def get_text (element, tag): e = element . find(tag) if e is not None : return e . text else : return '' # <!ELEMENT AuthorList (Author+) > # <!ELEMENT Author (((LastName, ForeName?, Initials?, Suffix?)\\ # | CollectiveName), Identifier*, AffiliationInfo*) > def parse_author (authors): result = [] for author in authors: item = {} item[ 'LastName' ] = get_text(author, 'LastName' ) item[ 'ForeName' ] = get_text(author, 'ForeName' ) item[ 'Initials' ] = get_text(author, 'Initials' ) item[ 'Suffix' ] = get_text(author, 'Suffix' ) item[ 'CollectiveName' ] = get_text(author, 'CollectiveName' ) result . append(item) return result Creation of dictionary of parsed data : A python dictionary is created with all the components as key-value pair. This JSON-like data structure makes it compatible for indexing and searching in Elasticsearch which is described in step 3 of protocol. Creation of MeSH to PMID mapping : During the creation of dictionary of parsed data, MeSH to PMID mapping table can also be created. This mapping is used to create Text-cube(in step 4) document structure as an requirement of CaseOLAP algorithm. def parse_pubmed_file ( file , pubmed_output_file, pmid2mesh_output_file): print ( 'Start parsing %s ' % file ) sys . stdout . flush() t1 = time . time() f = open ( file , 'r' ) tree = etree . parse(f) articles = itertools . chain(tree . findall( 'PubmedArticle' ), tree . findall( 'BookDocument' )) count = 0 noabs = 0 for article in articles: count += 1 result = {} pmid2mesh = {} # PMID - Exactly One Occurrance result[ 'PMID' ] = get_text(article, './/PMID' ) pmid2mesh[ 'PMID' ] = get_text(article, './/PMID' ) # # Article title - Zero or One Occurrences # result['ArticleTitle'] = get_text(article, './/ArticleTitle') # Abstract - Zero or One Occurrences abstractList = article . find( './/Abstract' ) if abstractList != None : try : abstract = ' \\n ' . join([line . text for line in abstractList . \\ findall( 'AbstractText' )]) result[ 'Abstract' ] = abstract except : result[ 'Abstract' ] = '' noabs += 1 else : result[ 'Abstract' ] = '' noabs += 1 # # Author List - Zero or More Occurrences # authors = article.findall('.//Author') # result['AuthorList'] = parse_author(authors) # # Journal - Exactly One Occurrance # journal = article.find('.//Journal') # result['Journal'] = get_text(journal, 'Title') result[ 'PubDate' ] = {} result[ 'PubDate' ][ 'Year' ] = get_text(journal, 'JournalIssue/PubDate/Year' ) # result['PubDate']['Month'] = get_text(journal, 'JournalIssue/PubDate/Month') # result['PubDate']['Day'] = get_text(journal, 'JournalIssue/PubDate/Day') # result['PubDate']['Season'] = get_text(journal, 'JournalIssue/PubDate/Season') # result['PubDate']['MedlineDate'] = get_text(journal,\\ # 'JournalIssue/PubDate/MedlineDate') # MeshHeading - Zero or More Occurrences headings = article . findall( './/MeshHeading' ) result[ 'MeshHeadingList' ] = [] pmid2mesh[ 'MeshHeadingList' ] = [] if headings: for heading in headings: descriptor_names = heading . findall( 'DescriptorName' ) qualifier_names = heading . findall( 'QualifierName' ) if descriptor_names: for descriptor_name in descriptor_names: result[ 'MeshHeadingList' ] . append(descriptor_name . text) pmid2mesh[ 'MeshHeadingList' ] . append(descriptor_name . text) if qualifier_names: for qualifier_name in qualifier_names: result[ 'MeshHeadingList' ] . append(qualifier_name . text) pmid2mesh[ 'MeshHeadingList' ] . append(qualifier_name . text) mesh_count = len (result[ 'MeshHeadingList' ]) if mesh_count in mesh_statistics: mesh_statistics[mesh_count] += 1 else : mesh_statistics[mesh_count] = 1 # Dump to pubmed json file <---------------------------- json . dump(result, pubmed_output_file) pubmed_output_file . write( ' \\n ' ) # Dump to pmid2mesh json file <------------------------- json . dump(pmid2mesh, pmid2mesh_output_file) pmid2mesh_output_file . write( ' \\n ' ) print ( 'Finish parsing %s , totally %d articles parsed. Total time: %f s' \\ % ( file , count, time . time() - t1)) print ( ' %d acticles no abstracts' % (noabs)) sys . stdout . flush() f . close() Setting up directories in parsing loop def parse_dir (source_dir, pubmed_output_file,pmid2mesh_output_file): if os . path . isdir(source_dir): for file in os . listdir(source_dir): if re . search( r'^pubmed18n\\d\\d\\d\\d.xml$' , file ) is not None : try : parse_pubmed_file(os . path . join(source_dir, file ),\\ pubmed_output_file, pmid2mesh_output_file) except : print ( \"XXXX Unexpected error happended when parsing %s XXXX\" % file ) print (traceback . print_exc()) sys . stdout . flush() Run the Parsing Pipeline t1 = time . time() pubmed_output_file_path = os . path . join(DATA_DIR, 'data/pubmed.json' ) pmid2mesh_output_file_path = os . path . join(DATA_DIR, 'pmid2mesh/pmid2mesh_from_parsing.json' ) pubmed_output_file = open (pubmed_output_file_path, 'w' ) pmid2mesh_output_file = open (pmid2mesh_output_file_path, 'w' ) parse_dir(os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/baseline' ),\\ pubmed_output_file, pmid2mesh_output_file) parse_dir(os . path . join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/updatefiles' ),\\ pubmed_output_file, pmid2mesh_output_file) pubmed_output_file . close() pmid2mesh_output_file . close() mesh_file = open (os . path . join(DATA_DIR, 'data/mesh_statistics.json' ), 'w' ) json . dump(mesh_statistics, mesh_file) mesh_file . close() print ( \"==== Parsing finished, results dumped to %s ====\" % pubmed_output_file_path) print ( \"==== TOTAL TIME: %f s ====\" % (time . time() - t1))","title":"Set up output data dir"},{"location":"Parsing/#mesh-to-pmid-mapping","text":"inputFilePath = \"data/pubmed.json\" meshFilePath = \"mesh2pmid/\" mesh2pmid_output_file = open (meshFilePath + 'mesh2pmid.json' , \"w\" ) mesh2pmid = dict () with open (inputFilePath, \"r\" ) as fin: start = time . time() k = 0 for line in fin: ## each line is single document try : k = k + 1 paperInfo = json . loads(line . strip()) data_dict = {} # update PMID data_dict[ \"pmid\" ] = paperInfo . get( \"PMID\" , \"-1\" ) #update MeSH Heading <---------------------- data_dict[ \"mesh_heading\" ] = \" \" . join(paperInfo[ \"MeshHeadingList\" ]) # collect Mesh2PMID <------------------- if data_dict[ \"pmid\" ] != \"-1\" : for mesh in paperInfo[ \"MeshHeadingList\" ]: if mesh not in mesh2pmid: mesh2pmid[mesh] = [] mesh2pmid[mesh] . append(data_dict[ \"pmid\" ]) if k % 500000 == 0 : print (k, 'done!' ) #break except : print ( \"XXXX Unexpected Error happened at line: XXXX\" ) # Dumping rest papers for key,value in mesh2pmid . items(): json . dump({key:value}, mesh2pmid_output_file) mesh2pmid_output_file . write( ' \\n ' ) mesh2pmid = dict () end = time . time() print ( \"Finish Total escaped time %s (seconds) \" % (end - start) )","title":"MeSH to PMID Mapping"},{"location":"TextCube/","text":"Text-cube creation Text-cube creation is an intelligent data engineering step in CaseOLAP which outputs a functional document structure with dimensions and cells informed by user provided document metadata. Each cell within the text-cube corresponds to a subset of documents. Following are the steps to create a text-cube. Selection of user defined categories : User selects the MeSH descriptors associated with the defined categories. Using those MeSH descriptors, cells of the documents are prepared. MeSH to PMID mapping prepared at step 2.4 is used to populate user defined cells in text-cube. Implementation of MeSH descriptors : U.S. National Library of Medicine provides the Medical Subject Heading (MeSH) in the hierarchical tree(data structure) with node ids. This permits searching for publications at varying levels of specificity. With given set of root node ids, one can select all documents for a specific cell by collecting all descendant nodes. Text-cube metadata preparation : A collection of user provided metadata(cell name, associated MeSH, PMID etc) representing each text document in the cell is prepared. There could be the significant number of documents falling under two or more cells. A dimensional hierarchy is implemented to organize the text-cube, providing each cell with a specific cell context (e.g., a parent cell, child cell, or sibling cell). Following are the steps to prepare cell-document metadata preparation: - provide the name of the cell, - make a list of document id (PMID) within each cell, - count the number of documents in each cell. Import required libraries import json import sys import time Set up input and out file address input_file_meshtree = \"input/mtrees2018.bin\" # MeSH Tree input_file_mesh2pmid = \"mesh2pmid.json\" # MeSH to PMID mapping input_file_input_cat = 'input/categories.txt' # file containing MeSH root nodes for each category output_file_pmid_and_cat = \"pmid_and_cat.json\" # file containing pmid to cell mapping concerned_cat = [] Collection of all decendent MeSH nods in MeSH tree \"\"\" Find corresponding MeSH Terms for each category. \"\"\" with open (input_file_input_cat, \"r\" ) as f_in_input_cat: for line in f_in_input_cat: concerned_cat . append(line . strip() . split()) num_cat = len (concerned_cat) term_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_meshtree, \"r\" ) as f_in_meshtree: for line in f_in_meshtree: term_tree = line . strip() . split( \";\" ) cur_term = term_tree[ 0 ] cur_tree = term_tree[ 1 ] for i in range (num_cat): for cur_cat_tree in concerned_cat[i]: if cur_cat_tree in cur_tree: term_set_per_cat[i] . add(cur_term) Application of Mesh to PMID mapping to find documents for each cell \"\"\" Find corresponding papers for each category. \"\"\" pmid_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_mesh2pmid, \"r\" ) as f_in: start = time . time() k = 0 for line in f_in: mesh2pmid = {} Info = json . loads(line . strip()) for key,value in Info . items(): mesh2pmid . update({key:value}) k = k + 1 if k % 1000 == 0 : print (k, 'done!' ) #break for i in range (num_cat): for cur_term in term_set_per_cat[i]: if cur_term == key: pmid_set_per_cat[i] = pmid_set_per_cat[i] | set (mesh2pmid[cur_term]) Creation of PMID to Cell mapping \"\"\" Serialize papers \"\"\" pmid_and_cat = [] for i in range (num_cat): for cur_pmid in pmid_set_per_cat[i]: pmid_and_cat . append([cur_pmid, i]) with open (output_file_pmid_and_cat, \"w\" ) as f_out: json . dump(pmid_and_cat, f_out)","title":"Text-Cube Creation"},{"location":"TextCube/#text-cube-creation","text":"Text-cube creation is an intelligent data engineering step in CaseOLAP which outputs a functional document structure with dimensions and cells informed by user provided document metadata. Each cell within the text-cube corresponds to a subset of documents. Following are the steps to create a text-cube. Selection of user defined categories : User selects the MeSH descriptors associated with the defined categories. Using those MeSH descriptors, cells of the documents are prepared. MeSH to PMID mapping prepared at step 2.4 is used to populate user defined cells in text-cube. Implementation of MeSH descriptors : U.S. National Library of Medicine provides the Medical Subject Heading (MeSH) in the hierarchical tree(data structure) with node ids. This permits searching for publications at varying levels of specificity. With given set of root node ids, one can select all documents for a specific cell by collecting all descendant nodes. Text-cube metadata preparation : A collection of user provided metadata(cell name, associated MeSH, PMID etc) representing each text document in the cell is prepared. There could be the significant number of documents falling under two or more cells. A dimensional hierarchy is implemented to organize the text-cube, providing each cell with a specific cell context (e.g., a parent cell, child cell, or sibling cell). Following are the steps to prepare cell-document metadata preparation: - provide the name of the cell, - make a list of document id (PMID) within each cell, - count the number of documents in each cell.","title":"Text-cube creation"},{"location":"TextCube/#import-required-libraries","text":"import json import sys import time Set up input and out file address input_file_meshtree = \"input/mtrees2018.bin\" # MeSH Tree input_file_mesh2pmid = \"mesh2pmid.json\" # MeSH to PMID mapping input_file_input_cat = 'input/categories.txt' # file containing MeSH root nodes for each category output_file_pmid_and_cat = \"pmid_and_cat.json\" # file containing pmid to cell mapping concerned_cat = []","title":"Import required libraries"},{"location":"TextCube/#collection-of-all-decendent-mesh-nods-in-mesh-tree","text":"\"\"\" Find corresponding MeSH Terms for each category. \"\"\" with open (input_file_input_cat, \"r\" ) as f_in_input_cat: for line in f_in_input_cat: concerned_cat . append(line . strip() . split()) num_cat = len (concerned_cat) term_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_meshtree, \"r\" ) as f_in_meshtree: for line in f_in_meshtree: term_tree = line . strip() . split( \";\" ) cur_term = term_tree[ 0 ] cur_tree = term_tree[ 1 ] for i in range (num_cat): for cur_cat_tree in concerned_cat[i]: if cur_cat_tree in cur_tree: term_set_per_cat[i] . add(cur_term)","title":"Collection of all decendent MeSH nods in MeSH tree"},{"location":"TextCube/#application-of-mesh-to-pmid-mapping-to-find-documents-for-each-cell","text":"\"\"\" Find corresponding papers for each category. \"\"\" pmid_set_per_cat = [ set () for _ in range (num_cat)] with open (input_file_mesh2pmid, \"r\" ) as f_in: start = time . time() k = 0 for line in f_in: mesh2pmid = {} Info = json . loads(line . strip()) for key,value in Info . items(): mesh2pmid . update({key:value}) k = k + 1 if k % 1000 == 0 : print (k, 'done!' ) #break for i in range (num_cat): for cur_term in term_set_per_cat[i]: if cur_term == key: pmid_set_per_cat[i] = pmid_set_per_cat[i] | set (mesh2pmid[cur_term])","title":"Application of Mesh to PMID mapping to find documents for each cell"},{"location":"TextCube/#creation-of-pmid-to-cell-mapping","text":"\"\"\" Serialize papers \"\"\" pmid_and_cat = [] for i in range (num_cat): for cur_pmid in pmid_set_per_cat[i]: pmid_and_cat . append([cur_pmid, i]) with open (output_file_pmid_and_cat, \"w\" ) as f_out: json . dump(pmid_and_cat, f_out)","title":"Creation of PMID to Cell mapping"},{"location":"workflow/","text":"Cloud Articture and Workflow The detail of the workflow is shown in the figure below: Image source: www.jove.com Figure : Platform architecture and workflow","title":"Workflow"},{"location":"workflow/#cloud-articture-and-workflow","text":"The detail of the workflow is shown in the figure below: Image source: www.jove.com Figure : Platform architecture and workflow","title":"Cloud Articture and Workflow"},{"location":"setup/anaconda/","text":"Installing Python To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python. Note- Linux: For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook Note - Cloud For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Setting up Python"},{"location":"setup/anaconda/#installing-python","text":"To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python.","title":"Installing Python"},{"location":"setup/anaconda/#note-linux","text":"For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook","title":"Note- Linux:"},{"location":"setup/anaconda/#note-cloud","text":"For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Note - Cloud"},{"location":"setup/elastic/","text":"Setting UP ElasticSearch Elasticsearch provides the functionality for Indexing and Search of the text documents. More information can be obtained from official website for Elasticsearch . Downloading and Installing Make sure that you have proper version of Java installed in your device Download extract the elasticsearch and kibana from Elasticsearch website Go to the bin folder and run ./elasticsearch Elasticsearch functionality can be used from Python environment which need to use package elasticsearch and elasticsearch_dsl Create the indexing of the documents Run kibana by running ./kibana from bin folder of extracted kibana. If index is successfully created, the new index can be seen at `localhost/IP:5601``` To run kibana reotely setup 0.0.0.0 in the configuration file. Sample Application with Python Install packages in environment pip install elasticsearch pip install elasticsearch_dsl Setting up Indexing request_body = { \"settings\" : { \"number_of_shards\" : 1 , \"number_of_replicas\" : 0 }, \"mappings\" : { pubmed: { \"properties\" : { \"pmid\" : { \"type\" : \"keyword\" }, \"mesh_heading\" : { \"type\" : \"text\" , \"similarity\" : \"BM25\" }, \"abstract\" :{ \"type\" : \"text\" } } } } } es = Elasticsearch() res = es . indices . create(index = pubmed, body = request_body) Setup search entity = \"insulin\" s = Search(using = es, index = \"pubmed\" )\\ . params(request_timeout = 300 )\\ . query( \"match_phrase\" , abstract = entity) for hit in s . scan(): print (hit . abstract) Read more Elasticsearch Documentation","title":"Setting up Elasticsearch"},{"location":"setup/elastic/#setting-up-elasticsearch","text":"Elasticsearch provides the functionality for Indexing and Search of the text documents. More information can be obtained from official website for Elasticsearch .","title":"Setting UP ElasticSearch"},{"location":"setup/elastic/#downloading-and-installing","text":"Make sure that you have proper version of Java installed in your device Download extract the elasticsearch and kibana from Elasticsearch website Go to the bin folder and run ./elasticsearch Elasticsearch functionality can be used from Python environment which need to use package elasticsearch and elasticsearch_dsl Create the indexing of the documents Run kibana by running ./kibana from bin folder of extracted kibana. If index is successfully created, the new index can be seen at `localhost/IP:5601``` To run kibana reotely setup 0.0.0.0 in the configuration file.","title":"Downloading and Installing"},{"location":"setup/elastic/#sample-application-with-python","text":"Install packages in environment pip install elasticsearch pip install elasticsearch_dsl Setting up Indexing request_body = { \"settings\" : { \"number_of_shards\" : 1 , \"number_of_replicas\" : 0 }, \"mappings\" : { pubmed: { \"properties\" : { \"pmid\" : { \"type\" : \"keyword\" }, \"mesh_heading\" : { \"type\" : \"text\" , \"similarity\" : \"BM25\" }, \"abstract\" :{ \"type\" : \"text\" } } } } } es = Elasticsearch() res = es . indices . create(index = pubmed, body = request_body) Setup search entity = \"insulin\" s = Search(using = es, index = \"pubmed\" )\\ . params(request_timeout = 300 )\\ . query( \"match_phrase\" , abstract = entity) for hit in s . scan(): print (hit . abstract)","title":"Sample Application with Python"},{"location":"setup/elastic/#read-more","text":"Elasticsearch Documentation","title":"Read more"},{"location":"setup/env/","text":"Python Environment Basics To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = \"/Users/username/anaconda/bin: $PATH \" or update above command line to your .zsh_config file. Install Package :Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Remove Package : Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name Update Package : If you want to update all packages in an environment, which is often useful, use conda update --all List Package : to list installed packages, it's conda list Search Package Name : If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup Environments Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Name : Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy Version : When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 Different Python versions : I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 Activate Environment : for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate Saving and loading environments Export the environment : A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml Create environment from exported file : writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml . Listing environments List the environments : If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root . Removing environments If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ). Using environments One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican . Sharing environments When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda. More to learn To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"Setting up Environment"},{"location":"setup/env/#python-environment","text":"","title":"Python Environment"},{"location":"setup/env/#basics","text":"To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = \"/Users/username/anaconda/bin: $PATH \" or update above command line to your .zsh_config file. Install Package :Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Remove Package : Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name Update Package : If you want to update all packages in an environment, which is often useful, use conda update --all List Package : to list installed packages, it's conda list Search Package Name : If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup","title":"Basics"},{"location":"setup/env/#environments","text":"Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Name : Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy Version : When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 Different Python versions : I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 Activate Environment : for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate","title":"Environments"},{"location":"setup/env/#saving-and-loading-environments","text":"Export the environment : A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml Create environment from exported file : writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml .","title":"Saving and loading environments"},{"location":"setup/env/#listing-environments","text":"List the environments : If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root .","title":"Listing environments"},{"location":"setup/env/#removing-environments","text":"If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ).","title":"Removing environments"},{"location":"setup/env/#using-environments","text":"One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican .","title":"Using environments"},{"location":"setup/env/#sharing-environments","text":"When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda.","title":"Sharing environments"},{"location":"setup/env/#more-to-learn","text":"To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"More to learn"},{"location":"setup/git/","text":"How to git Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m \"First commit\" Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"Setting up Git"},{"location":"setup/git/#how-to-git","text":"Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m \"First commit\" Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"How to git"},{"location":"setup/mongo/","text":"Setting Up MongoDB MongoDB is NOSQL database. THe detail of the installation can be founs at official website of MongoDB . Following is th installation steps obtained at 2019-Oct 15. Downloading and Installing 1. Import the public key used by the package management system. From a terminal, issue the following command to import the MongoDB public GPG Key from https://www.mongodb.org/static/pgp/server-4.2.asc: wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add - The operation should respond with an OK. 2. Create a list file for MongoDB. Create the list file /etc/apt/sources.list.d/mongodb-org-4.2.list for your version of Ubuntu. Click on the appropriate tab for your version of Ubuntu. If you are unsure of what Ubuntu version the host is running, open a terminal or shell on the host and execute lsb_release -dc . Ubuntu 18.04 (Bionic): The following instruction is for Ubuntu 18.04 (Bionic). For Ubuntu 16.04 (Xenial), click on the appropriate tab. Create the /etc/apt/sources.list.d/mongodb-org-4.2.list file for Ubuntu 18.04 (Bionic): echo \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.list 3. Reload local package database. Issue the following command to reload the local package database: sudo apt-get update 4. Install the MongoDB packages. You can install either the latest stable version of MongoDB or a specific version of MongoDB. Install the latest version of MongoDB. Install a specific release of MongoDB. To install the latest stable version, issue the following sudo apt-get install -y mongodb-org Sample application with Python: Install the python package pymongo in the environment pip install pymongo Create Database and Collection import pymongo client = pymongo . MongoClient( \"mongodb://localhost:27017/\" ) db = client[ \"test\" ] collection = db[ \"test_collection\" ] Next, Populate the collection x = collection . insert_many(DATA) Print out one sample data: x = collection . find_one() print (x) Read More MongoDB Documents MongoDB Tutorials -I MongoDB Tutorials Point","title":"Setting up MongoDB"},{"location":"setup/mongo/#setting-up-mongodb","text":"MongoDB is NOSQL database. THe detail of the installation can be founs at official website of MongoDB . Following is th installation steps obtained at 2019-Oct 15.","title":"Setting Up MongoDB"},{"location":"setup/mongo/#downloading-and-installing","text":"1. Import the public key used by the package management system. From a terminal, issue the following command to import the MongoDB public GPG Key from https://www.mongodb.org/static/pgp/server-4.2.asc: wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add - The operation should respond with an OK. 2. Create a list file for MongoDB. Create the list file /etc/apt/sources.list.d/mongodb-org-4.2.list for your version of Ubuntu. Click on the appropriate tab for your version of Ubuntu. If you are unsure of what Ubuntu version the host is running, open a terminal or shell on the host and execute lsb_release -dc . Ubuntu 18.04 (Bionic): The following instruction is for Ubuntu 18.04 (Bionic). For Ubuntu 16.04 (Xenial), click on the appropriate tab. Create the /etc/apt/sources.list.d/mongodb-org-4.2.list file for Ubuntu 18.04 (Bionic): echo \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.list 3. Reload local package database. Issue the following command to reload the local package database: sudo apt-get update 4. Install the MongoDB packages. You can install either the latest stable version of MongoDB or a specific version of MongoDB. Install the latest version of MongoDB. Install a specific release of MongoDB. To install the latest stable version, issue the following sudo apt-get install -y mongodb-org","title":"Downloading and Installing"},{"location":"setup/mongo/#sample-application-with-python","text":"Install the python package pymongo in the environment pip install pymongo Create Database and Collection import pymongo client = pymongo . MongoClient( \"mongodb://localhost:27017/\" ) db = client[ \"test\" ] collection = db[ \"test_collection\" ] Next, Populate the collection x = collection . insert_many(DATA) Print out one sample data: x = collection . find_one() print (x)","title":"Sample application with Python:"},{"location":"setup/mongo/#read-more","text":"MongoDB Documents MongoDB Tutorials -I MongoDB Tutorials Point","title":"Read More"},{"location":"setup/neo4j/","text":"Setting Up Neo4j The detail instruction for installing Neo4j(latest version) can be found at official website of Neo4j . Here are the simple steps created the current time (2019 -Oct 15). Before you install, make sure the correct version of java is installed in your device Install Neo4j To install Neo4j Community Edition: sudo apt-get install neo4j = <version> To install Neo4j Enterprise Edition: sudo apt-get install neo4j-enterprise = <version> Startting Stopping or Restarting Service To start the neo4j service : sudo service neo4j start Go to IP/localhost:7474 to have neo4j browser. To stop the neo4j service : sudo service neo4j start To restart the neo4j service : sudo service neo4j start For Forgotten Password Stop the neo4j service remove the auth file inside /var/lib/neo4j/data/dbms restart the service Follow server connect command to recreate password For remote access Stop the neo4j service Setup 0.0.0.0 instead of localhost in the configuration file Start the neo4j service How to use new graph database Stop the neo4j service Find out neo4j.config file and repoint the new graph database. Start the neo4j service","title":"Setting up Neo4J"},{"location":"setup/neo4j/#setting-up-neo4j","text":"The detail instruction for installing Neo4j(latest version) can be found at official website of Neo4j . Here are the simple steps created the current time (2019 -Oct 15). Before you install, make sure the correct version of java is installed in your device","title":"Setting Up Neo4j"},{"location":"setup/neo4j/#install-neo4j","text":"To install Neo4j Community Edition: sudo apt-get install neo4j = <version> To install Neo4j Enterprise Edition: sudo apt-get install neo4j-enterprise = <version>","title":"Install Neo4j"},{"location":"setup/neo4j/#startting-stopping-or-restarting-service","text":"To start the neo4j service : sudo service neo4j start Go to IP/localhost:7474 to have neo4j browser. To stop the neo4j service : sudo service neo4j start To restart the neo4j service : sudo service neo4j start","title":"Startting Stopping or Restarting Service"},{"location":"setup/neo4j/#for-forgotten-password","text":"Stop the neo4j service remove the auth file inside /var/lib/neo4j/data/dbms restart the service Follow server connect command to recreate password","title":"For Forgotten Password"},{"location":"setup/neo4j/#for-remote-access","text":"Stop the neo4j service Setup 0.0.0.0 instead of localhost in the configuration file Start the neo4j service","title":"For remote access"},{"location":"setup/neo4j/#how-to-use-new-graph-database","text":"Stop the neo4j service Find out neo4j.config file and repoint the new graph database. Start the neo4j service","title":"How to use new graph database"}]}